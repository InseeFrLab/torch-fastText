{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import pandas as pd\n",
    "# import pyarrow.parquet as pq\n",
    "# import s3fs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torchFastText import torchFastText\n",
    "from torchFastText.preprocess import (\n",
    "    clean_and_tokenize_df,\n",
    "    clean_text_feature,\n",
    "    stratified_split_rare_labels,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorize_surface(\n",
    "    df: pd.DataFrame, surface_feature_name: int, like_sirene_3: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Categorize the surface of the activity.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to categorize.\n",
    "        surface_feature_name (str): Name of the surface feature.\n",
    "        like_sirene_3 (bool): If True, categorize like Sirene 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a new column \"surf_cat\".\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].replace(\"nan\", np.nan)\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(float)\n",
    "    # Check surface feature exists\n",
    "    if surface_feature_name not in df.columns:\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} not found in DataFrame.\")\n",
    "    # Check surface feature is a float variable\n",
    "    if not (pd.api.types.is_float_dtype(df_copy[surface_feature_name])):\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} must be a float variable.\")\n",
    "\n",
    "    if like_sirene_3:\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy[surface_feature_name],\n",
    "            bins=[0, 120, 400, 2500, np.inf],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "    else:\n",
    "        # Log transform the surface\n",
    "        df_copy[\"surf_log\"] = np.log(df[surface_feature_name])\n",
    "\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy.surf_log,\n",
    "            bins=[0, 3, 4, 5, 12],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "\n",
    "    df_copy[surface_feature_name] = df_copy[\"surf_cat\"].replace(\"nan\", \"0\")\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(int)\n",
    "    df_copy = df_copy.drop(columns=[\"surf_log\", \"surf_cat\"], errors=\"ignore\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def clean_and_tokenize_df(\n",
    "    df,\n",
    "    categorical_features=[\"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\"],\n",
    "    text_feature=\"libelle_processed\",\n",
    "    label_col=\"apet_finale\",\n",
    "):\n",
    "    df.fillna(\"nan\", inplace=True)\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"evenement_type\": \"EVT\",\n",
    "            \"cj\": \"CJ\",\n",
    "            \"activ_nat_et\": \"NAT\",\n",
    "            \"liasse_type\": \"TYP\",\n",
    "            \"activ_surf_et\": \"SRF\",\n",
    "            \"activ_perm_et\": \"CRT\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    les = []\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        les.append(le)\n",
    "\n",
    "    df = categorize_surface(df, \"SRF\", like_sirene_3=True)\n",
    "    df = df[[text_feature, \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"SRF\", \"CRT\", label_col]]\n",
    "\n",
    "    return df, les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show -f torchFastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/inseefrlab/torch-fasttext@package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "    key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    ")\n",
    "df = (\n",
    "    pq.ParquetDataset(\n",
    "        \"projet-ape/extractions/20241027_sirene4.parquet\",\n",
    "        filesystem=fs,\n",
    "    )\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ").sample(frac=0.01).fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(\"projet-ape/data/naf2008.csv\") as file:\n",
    "    naf2008 = pd.read_csv(file, sep=\";\")\n",
    "naf2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_libelles(\n",
    "    df: pd.DataFrame,\n",
    "    df_naf: pd.DataFrame,\n",
    "    y: str,\n",
    "    text_feature: str,\n",
    "    textual_features: list,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    missing_codes = set(df_naf[\"code\"])\n",
    "    fake_obs = df_naf[df_naf[\"code\"].isin(missing_codes)]\n",
    "    fake_obs[y] = fake_obs[\"code\"]\n",
    "    fake_obs[text_feature] = fake_obs[[text_feature]].apply(\n",
    "        lambda row: \" \".join(f\"[{col}] {val}\" for col, val in row.items() if val != \"\"), axis=1\n",
    "    )\n",
    "    df = pd.concat([df, fake_obs[[col for col in fake_obs.columns if col in df.columns]]])\n",
    "\n",
    "    if textual_features is not None:\n",
    "        for feature in textual_features:\n",
    "            df[feature] = df[feature].fillna(value=\"\")\n",
    "    if categorical_features is not None:\n",
    "        for feature in categorical_features:\n",
    "            df[feature] = df[feature].fillna(value=\"NaN\")\n",
    "\n",
    "    print(f\"\\t*** {len(missing_codes)} missing codes have been added in the database...\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"evenement_type\", \"cj\",  \"activ_nat_et\", \"liasse_type\", \"activ_surf_et\", \"activ_perm_et\"]\n",
    "text_feature = \"libelle\"\n",
    "y = \"apet_finale\"\n",
    "textual_features = None\n",
    "\n",
    "df = add_libelles(df, naf2008, y, text_feature, textual_features, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make available our processing function clean_text_feature for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"libelle_processed\"] = clean_text_feature(df[\"libelle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the columns in the right format:\n",
    " - First column contains the processed text (str)\n",
    " - Next ones contain the \"tokenized\" categorical variables in int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle_processed\") # NE PAS OUBLIER DE REMETYTRE PROCESSEd\n",
    "X = df[[\"libelle_processed\", \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
    "y = df[\"apet_finale\"].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test sets. We especially take care that:  \n",
    "- classes with only one instance appear in the train set (instead of the test set)\n",
    "- all classes are represented in the train set\n",
    "\n",
    "The `stratified_split_rare_labels` function from the `preprocess` subpackage is used to carefully split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = stratified_split_rare_labels(X, y)\n",
    "assert set(range(len(naf2008[\"code\"]))) == set(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Build the torch-fastText model (without training it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model building\n",
    "\n",
    "NUM_BUCKETS = int(1e5) # Number of rows in the embedding matrix\n",
    "EMBED_DIM = 50 # Dimension of the embedding = number of columns in the embedding matrix\n",
    "MIN_COUNT = 1 # Minimum number of occurrences of a word in the corpus to be included in the vocabulary\n",
    "MIN_N = 3 # Minimum length of char n-grams\n",
    "MAX_N = 6 # Maximum length of char n-grams\n",
    "LEN_WORD_NGRAMS = 3 # Length of word n-grams\n",
    "SPARSE = False # Whether to use sparse Embedding layer for fast computation (see PyTorch documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchFastText(\n",
    "    num_buckets=NUM_BUCKETS,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    min_count=MIN_COUNT,\n",
    "    min_n=MIN_N,\n",
    "    max_n=MAX_N,\n",
    "    len_word_ngrams=LEN_WORD_NGRAMS,\n",
    "    sparse = SPARSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model using the training data. We have now access to the PyTorch model and a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 4e-3\n",
    "model.build(X_train, y_train, lightning=True, lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)\n",
    "print(model.lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is useful to initialize the full torchFastText model without training it, if needed for some reason. But if it is not necessary, and we could have directly launched the training (building is then handled automatically if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a torchFastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience_scheduler=PATIENCE,\n",
    "    patience_train=PATIENCE,\n",
    "    lr=LR,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained model from a Lightning checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model.best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

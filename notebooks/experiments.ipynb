{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "from config.preprocess import clean_and_tokenize_df, clean_text_feature\n",
    "from torchFastText import torchFastText\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "    key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    ")\n",
    "df = (\n",
    "    pq.ParquetDataset(\n",
    "        \"projet-ape/extractions/20241027_sirene4.parquet\",\n",
    "        filesystem=fs,\n",
    "    )\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ").fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(\"projet-ape/data/naf2008.csv\") as file:\n",
    "    naf2008 = pd.read_csv(file, sep=\";\")\n",
    "naf2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_codes(\n",
    "    df: pd.DataFrame,\n",
    "    df_naf: pd.DataFrame,\n",
    "    y: str,\n",
    "    text_feature: str,\n",
    "    textual_features: list,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    missing_codes = set(df_naf[\"code\"])\n",
    "    fake_obs = df_naf[df_naf[\"code\"].isin(missing_codes)]\n",
    "    fake_obs[y] = fake_obs[\"code\"]\n",
    "    fake_obs[text_feature] = fake_obs[[text_feature]].apply(\n",
    "        lambda row: \" \".join(f\"[{col}] {val}\" for col, val in row.items() if val != \"\"), axis=1\n",
    "    )\n",
    "    df = pd.concat([df, fake_obs[[col for col in fake_obs.columns if col in df.columns]]])\n",
    "\n",
    "    if textual_features is not None:\n",
    "        for feature in textual_features:\n",
    "            df[feature] = df[feature].fillna(value=\"\")\n",
    "    if categorical_features is not None:\n",
    "        for feature in categorical_features:\n",
    "            df[feature] = df[feature].fillna(value=\"NaN\")\n",
    "\n",
    "    print(f\"\\t*** {len(missing_codes)} missing codes have been added in the database...\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"evenement_type\", \"cj\",  \"activ_nat_et\", \"liasse_type\", \"activ_surf_et\", \"activ_perm_et\"]\n",
    "text_feature = \"libelle\"\n",
    "y = \"apet_finale\"\n",
    "textual_features = None\n",
    "\n",
    "df= add_missing_codes(df, naf2008, y, text_feature, textual_features, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make available our processing function clean_text_feature for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"libelle_processed\"] = clean_text_feature(df[\"libelle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the columns in the right format:\n",
    " - First column contains the processed text (str)\n",
    " - Next ones contain the \"tokenized\" categorical variables in int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle\") # NE PAS OUBLIER DE REMETYTRE PROCESSEd\n",
    "X = df[[\"libelle\", \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
    "y = df[\"apet_finale\"].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test sets. We especially take care that classes with only one instance appear in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "single_instance_classes = {cls for cls, count in class_counts.items() if count == 1}\n",
    "is_single_instance = np.isin(y, list(single_instance_classes))\n",
    "is_remaining = ~is_single_instance\n",
    "\n",
    "X_remaining, X_test, y_remaining, y_test = train_test_split(\n",
    "    X[is_remaining], y[is_remaining], test_size=0.2, random_state=42, shuffle = True\n",
    ")\n",
    "\n",
    "# Combine single-instance samples with the training set\n",
    "X_train = np.vstack([X_remaining, X[is_single_instance]])\n",
    "y_train = np.concatenate([y_remaining, y[is_single_instance]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Build the torch-fastText model (without training it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model building\n",
    "\n",
    "NUM_BUCKETS = int(2e6) # Number of rows in the embedding matrix\n",
    "EMBED_DIM = 180 # Dimension of the embedding = number of columns in the embedding matrix\n",
    "MIN_COUNT = 1 # Minimum number of occurrences of a word in the corpus to be included in the vocabulary\n",
    "MIN_N = 3 # Minimum length of char n-grams\n",
    "MAX_N = 6 # Maximum length of char n-grams\n",
    "LEN_WORD_NGRAMS = 3 # Length of word n-grams\n",
    "SPARSE = False # Whether to use sparse Embedding layer for fast computation (see PyTorch documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchFastText(\n",
    "    num_buckets=NUM_BUCKETS,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    min_count=MIN_COUNT,\n",
    "    min_n=MIN_N,\n",
    "    max_n=MAX_N,\n",
    "    len_word_ngrams=LEN_WORD_NGRAMS,\n",
    "    sparse = SPARSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model using the training data. We have now access to the PyTorch model and a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is useful to initialize the full torchFastText model without training it, if needed for some reason. But if it is not necessary, and we could have directly launched the training (building is then handled automatically if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a torchFastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 3\n",
    "LR = 4e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience_scheduler=PATIENCE,\n",
    "    patience_train=PATIENCE,\n",
    "    lr=LR,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = X_train[text_feature].to_list()\n",
    "tokenizer = NGramTokenizer(min_count, min_n, max_n, buckets, word_ngrams, training_text)\n",
    "\n",
    "train_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_train[column].astype(int).to_list() for column in X_train[categorical_features]\n",
    "    ],\n",
    "    texts=training_text,\n",
    "    outputs=y_train.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataset = FastTextModelDataset(\n",
    "    categorical_variables=[\n",
    "        X_val[column].astype(int).to_list() for column in X_val[categorical_features]\n",
    "    ],\n",
    "    texts=X_val[text_feature].to_list(),\n",
    "    outputs=y_val.to_list(),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = train_dataset.create_dataloader(batch_size=batch_size, num_workers=4)\n",
    "val_dataloader = val_dataset.create_dataloader(batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = df[y].nunique()\n",
    "categorical_vocabulary_sizes = [np.max(X_train[feature]) + 1 for feature in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vocabulary_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CJ.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastTextModel(\n",
    "    tokenizer=tokenizer,\n",
    "    nace_encoder=encoder,\n",
    "    embedding_dim=embedding_dim,\n",
    "    vocab_size=buckets + tokenizer.get_nwords() + 1,\n",
    "    num_classes=num_classes,\n",
    "    categorical_vocabulary_sizes=categorical_vocabulary_sizes,\n",
    "    padding_idx=buckets + tokenizer.get_nwords(),\n",
    "    sparse=sparse,\n",
    "    direct_bagging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x[0], x[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer & scheduler\n",
    "if sparse:\n",
    "    optimizer = SGD\n",
    "else:\n",
    "    optimizer = Adam\n",
    "optimizer_params = {\"lr\": lr}\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler_params = {\n",
    "    \"mode\": \"min\",\n",
    "    \"patience\": patience,\n",
    "}\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# Lightning module\n",
    "module = FastTextModule(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_params=scheduler_params,\n",
    "    scheduler_interval=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.training_step(x, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

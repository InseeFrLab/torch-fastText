{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exemple d’utilisation de la librairie `TorchFastText`\n",
        "\n",
        "*Warning*\n",
        "\n",
        "*`TorchFastText` library is still under active development. Have a\n",
        "regular look to <https://github.com/inseefrlab/torch-fastText> for\n",
        "latest information.*\n",
        "\n",
        "To install package, you can run the following snippet"
      ],
      "id": "a01b1526-51df-4bf9-9fd4-11ef22ffcc79"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stable version\n",
        "pip install torchFastText \n",
        "# Development version\n",
        "# pip install !https://github.com/InseeFrLab/torch-fastText.git"
      ],
      "id": "a00a2856"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and preprocess data\n",
        "\n",
        "In that guide, we propose to illustrate main package functionalities\n",
        "using that `DataFrame`:"
      ],
      "id": "b292ea76-57a1-4d4e-9bde-dcc9656dc447"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet(\"https://minio.lab.sspcloud.fr/projet-ape/extractions/20241027_sirene4.parquet\")\n",
        "df = df.sample(10000)"
      ],
      "id": "37c042fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal will be to build multilabel classification for the `code`\n",
        "variable using `libelle` as feature.\n",
        "\n",
        "## Enriching our test dataset\n",
        "\n",
        "Unlike `Fasttext`, this package offers the possibility of having several\n",
        "feature columns of different types (string for the text column and\n",
        "additional variables in numeric form, for example). To illustrate that,\n",
        "we propose the following enrichment of the example dataset:"
      ],
      "id": "c399b4b0-a9cb-450e-9a5e-480e0e657b8e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def categorize_surface(\n",
        "    df: pd.DataFrame, surface_feature_name: int, like_sirene_3: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Categorize the surface of the activity.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame to categorize.\n",
        "        surface_feature_name (str): Name of the surface feature.\n",
        "        like_sirene_3 (bool): If True, categorize like Sirene 3.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with a new column \"surf_cat\".\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    df_copy[surface_feature_name] = df_copy[surface_feature_name].replace(\"nan\", np.nan)\n",
        "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(float)\n",
        "    # Check surface feature exists\n",
        "    if surface_feature_name not in df.columns:\n",
        "        raise ValueError(f\"Surface feature {surface_feature_name} not found in DataFrame.\")\n",
        "    # Check surface feature is a float variable\n",
        "    if not (pd.api.types.is_float_dtype(df_copy[surface_feature_name])):\n",
        "        raise ValueError(f\"Surface feature {surface_feature_name} must be a float variable.\")\n",
        "\n",
        "    if like_sirene_3:\n",
        "        # Categorize the surface\n",
        "        df_copy[\"surf_cat\"] = pd.cut(\n",
        "            df_copy[surface_feature_name],\n",
        "            bins=[0, 120, 400, 2500, np.inf],\n",
        "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
        "        ).astype(str)\n",
        "    else:\n",
        "        # Log transform the surface\n",
        "        df_copy[\"surf_log\"] = np.log(df[surface_feature_name])\n",
        "\n",
        "        # Categorize the surface\n",
        "        df_copy[\"surf_cat\"] = pd.cut(\n",
        "            df_copy.surf_log,\n",
        "            bins=[0, 3, 4, 5, 12],\n",
        "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
        "        ).astype(str)\n",
        "\n",
        "    df_copy[surface_feature_name] = df_copy[\"surf_cat\"].replace(\"nan\", \"0\")\n",
        "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(int)\n",
        "    df_copy = df_copy.drop(columns=[\"surf_log\", \"surf_cat\"], errors=\"ignore\")\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "def clean_and_tokenize_df(\n",
        "    df,\n",
        "    categorical_features=[\"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\"],\n",
        "    text_feature=\"libelle_processed\",\n",
        "    label_col=\"apet_finale\",\n",
        "):\n",
        "    df.fillna(\"nan\", inplace=True)\n",
        "\n",
        "    df = df.rename(\n",
        "        columns={\n",
        "            \"evenement_type\": \"EVT\",\n",
        "            \"cj\": \"CJ\",\n",
        "            \"activ_nat_et\": \"NAT\",\n",
        "            \"liasse_type\": \"TYP\",\n",
        "            \"activ_surf_et\": \"SRF\",\n",
        "            \"activ_perm_et\": \"CRT\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    les = []\n",
        "    for col in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        les.append(le)\n",
        "\n",
        "    df = categorize_surface(df, \"SRF\", like_sirene_3=True)\n",
        "    df = df[[text_feature, \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"SRF\", \"CRT\", label_col]]\n",
        "\n",
        "    return df, les\n",
        "\n",
        "\n",
        "def stratified_split_rare_labels(X, y, test_size=0.2, min_train_samples=1):\n",
        "    # Get unique labels and their frequencies\n",
        "    unique_labels, label_counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    # Separate rare and common labels\n",
        "    rare_labels = unique_labels[label_counts == 1]\n",
        "\n",
        "    # Create initial mask for rare labels to go into training set\n",
        "    rare_label_mask = np.isin(y, rare_labels)\n",
        "\n",
        "    # Separate data into rare and common label datasets\n",
        "    X_rare = X[rare_label_mask]\n",
        "    y_rare = y[rare_label_mask]\n",
        "    X_common = X[~rare_label_mask]\n",
        "    y_common = y[~rare_label_mask]\n",
        "\n",
        "    # Split common labels stratified\n",
        "    X_common_train, X_common_test, y_common_train, y_common_test = train_test_split(\n",
        "        X_common, y_common, test_size=test_size, stratify=y_common\n",
        "    )\n",
        "\n",
        "    # Combine rare labels with common labels split\n",
        "    X_train = np.concatenate([X_rare, X_common_train])\n",
        "    y_train = np.concatenate([y_rare, y_common_train])\n",
        "    X_test = X_common_test\n",
        "    y_test = y_common_test\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def add_libelles(\n",
        "    df: pd.DataFrame,\n",
        "    df_naf: pd.DataFrame,\n",
        "    y: str,\n",
        "    text_feature: str,\n",
        "    textual_features: list,\n",
        "    categorical_features: list,\n",
        "):\n",
        "    missing_codes = set(df_naf[\"code\"])\n",
        "    fake_obs = df_naf[df_naf[\"code\"].isin(missing_codes)]\n",
        "    fake_obs[y] = fake_obs[\"code\"]\n",
        "    fake_obs[text_feature] = fake_obs[[text_feature]].apply(\n",
        "        lambda row: \" \".join(f\"[{col}] {val}\" for col, val in row.items() if val != \"\"), axis=1\n",
        "    )\n",
        "    df = pd.concat([df, fake_obs[[col for col in fake_obs.columns if col in df.columns]]])\n",
        "\n",
        "    if textual_features is not None:\n",
        "        for feature in textual_features:\n",
        "            df[feature] = df[feature].fillna(value=\"\")\n",
        "    if categorical_features is not None:\n",
        "        for feature in categorical_features:\n",
        "            df[feature] = df[feature].fillna(value=\"NaN\")\n",
        "\n",
        "    print(f\"\\t*** {len(missing_codes)} codes have been added in the database...\\n\")\n",
        "    return df"
      ],
      "id": "92402df7"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    *** 732 codes have been added in the database...\n"
          ]
        }
      ],
      "source": [
        "categorical_features = [\"evenement_type\", \"cj\",  \"activ_nat_et\", \"liasse_type\", \"activ_surf_et\", \"activ_perm_et\"]\n",
        "text_feature = \"libelle\"\n",
        "y = \"apet_finale\"\n",
        "textual_features = None\n",
        "\n",
        "naf2008 = pd.read_csv(\"https://minio.lab.sspcloud.fr/projet-ape/data/naf2008.csv\", sep=\";\")\n",
        "df = add_libelles(df, naf2008, y, text_feature, textual_features, categorical_features)"
      ],
      "id": "1fd02895"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "\n",
        "To reduce noise in text fields, we recommend pre-processing before\n",
        "training a model with our package. We assume this preprocessing is\n",
        "handled by the package user : this gives him the opportunity to control\n",
        "data cleansing.\n",
        "\n",
        "Here’s an example of the type of preprocessing that can be carried out\n",
        "before moving on to the modeling phase"
      ],
      "id": "67f4160d-0c98-4700-80f4-1ba454e6a2df"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchFastText.preprocess import clean_text_feature\n",
        "df[\"libelle_processed\"] = clean_text_feature(df[\"libelle\"])"
      ],
      "id": "61b0252e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Right now, the model requires the label (variable y) to be a numerical\n",
        "variable. If the label variable is a text variable, we recommend using\n",
        "Scikit Learn’s\n",
        "[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
        "to convert into a numeric variable. Using that function will give user\n",
        "the possibility to get back labels from the encoder after running\n",
        "predictions."
      ],
      "id": "acde2929-fe92-4107-8066-a5c8ac5d6428"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
      ],
      "id": "8c02a833"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function `clean_and_tokenize_df` requires special `DataFrame`\n",
        "formatting:\n",
        "\n",
        "-   First column contains the processed text (str)\n",
        "-   Next ones contain the “encoded” categorical (discrete) variables in\n",
        "    int format"
      ],
      "id": "25593e1a-1661-49e3-9734-272ec4745de1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_90631/2075507147.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  df.fillna(\"nan\", inplace=True)"
          ]
        }
      ],
      "source": [
        "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle_processed\")\n",
        "X = df[[\"libelle_processed\", \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
        "y = df[\"apet_finale\"].values"
      ],
      "id": "5fb5b0c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting in train-test sets\n",
        "\n",
        "As usual in a learning approach, you need to break down your data into\n",
        "learning and test/validation samples to obtain robust performance\n",
        "statistics.\n",
        "\n",
        "This work is the responsibility of the package’s users. Here’s an\n",
        "example of how to do it, using the\n",
        "[`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "function in `Scikit`."
      ],
      "id": "e70de831-dbc9-49be-b0c4-d70dd6479d03"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "id": "b593fd75"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build the torch-fastText model (without training it)\n",
        "\n",
        "There are several ways to define and train a pytorch.fasttext model in\n",
        "this package.\n",
        "\n",
        "We first show how to initialize the model and then afterwars build it.\n",
        "\n",
        "`torchFastText` function accepts the following parameters:\n",
        "\n",
        "| Parameter | Meaning | Example Value |\n",
        "|---------------------|------------------------------------------|----------|\n",
        "| `num_tokens` | Number of rows in the embedding matrix (size of the vocabulary) | 100000 |\n",
        "| `embedding_dim` | Dimension of the embedding (number of columns in the matrix) | 50 |\n",
        "| `sparse` | Use sparse embedding for fast computation (PyTorch) | False |\n",
        "| `categorical_embedding_dims` | Dimension of the embedding for categorical features | 10 |\n",
        "| `min_count` | Minimum occurrences of a word in the corpus to be included | 1 |\n",
        "| `min_n` | Minimum length of character n-grams | 3 |\n",
        "| `max_n` | Maximum length of character n-grams | 6 |\n",
        "| `len_word_ngrams` | Length of word n-grams | 3 |"
      ],
      "id": "8729c5f4-9038-4437-929b-fc500dc0db7a"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchFastText import torchFastText\n",
        "\n",
        "parameters = {\n",
        "    \"num_tokens\": 100000,\n",
        "    \"embedding_dim\": 50,\n",
        "    \"sparse\": False,\n",
        "    \"categorical_embedding_dims\": 10,\n",
        "    \"min_count\": 1,\n",
        "    \"min_n\": 3,\n",
        "    \"max_n\": 6,\n",
        "    \"len_word_ngrams\": 3,\n",
        "}\n",
        "\n",
        "parameters_train = {\n",
        "    \"lr\": 0.004,\n",
        "    \"num_epochs\": 1,\n",
        "    \"batch_size\": 256,\n",
        "    \"patience\": 3   \n",
        "}\n",
        "\n",
        "model = torchFastText(**parameters)"
      ],
      "id": "5879ca88"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`model` is then a special `torchFastText` object:"
      ],
      "id": "05f9d26b-f08f-41be-93e4-b55a2c86690c"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "torchFastText.torchFastText.torchFastText"
            ]
          }
        }
      ],
      "source": [
        "type(model)"
      ],
      "id": "ebf5608b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As any `PyTorch` model, it accepts being save as a JSON for later on\n",
        "use:"
      ],
      "id": "dcbe8289-f506-48f9-b854-96f25974368f"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to_json('torchFastText_config.json')\n",
        "# model = torchFastText.from_json('torchFastText_config.json')"
      ],
      "id": "6c3b2b85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can apply `build` to finally train our model. These are the\n",
        "parameters accepted by the `build` method\n",
        "\n",
        "| Parameter | Meaning | Example Value |\n",
        "|---------------------|------------------------------------------|----------|\n",
        "| `lr` | Learning rate | 0.004 |\n",
        "| `num_epochs` | Number of training epochs | 1 |\n",
        "| `batch_size` | Batch size for training | 256 |\n",
        "| `patience` | Early stopping patience (number of epochs without improvement) | 3 |\n",
        "\n",
        "We build the model using the training data. We have now access to the\n",
        "tokenizer, the PyTorch model as well as a PyTorch Lightning module ready\n",
        "to be trained. Note that Lightning is high-level framework for PyTorch\n",
        "that simplifies the process of training, validating, and deploying\n",
        "machine learning models."
      ],
      "id": "5f8b017f-66a1-413d-85e8-1981adf64823"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-05 16:27:41 - torchFastText.model.pytorch_model - num_rows is different from the number of tokens in the tokenizer. Using provided num_rows.\n",
            "2025-03-05 16:27:41 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau)."
          ]
        }
      ],
      "source": [
        "model.build(X_train, y_train, lightning=True, lr=parameters_train.get(\"lr\"))"
      ],
      "id": "e2e43d0e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can retrieve different objects from `model` instance:\n",
        "\n",
        "-   `model.pytorch_model`\n",
        "-   `model.tokenizer`\n",
        "-   `model.lightning_module`"
      ],
      "id": "b5a7d5fa-596a-470b-892e-e8fafdb8221a"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "FastTextModel(\n",
              "  (embeddings): EmbeddingBag(107992, 50, mode='mean', padding_idx=107991)\n",
              "  (emb_0): Embedding(24, 10)\n",
              "  (emb_1): Embedding(40, 10)\n",
              "  (emb_2): Embedding(8, 10)\n",
              "  (emb_3): Embedding(13, 10)\n",
              "  (emb_4): Embedding(3, 10)\n",
              "  (emb_5): Embedding(4, 10)\n",
              "  (fc): Linear(in_features=60, out_features=646, bias=True)\n",
              ")"
            ]
          }
        }
      ],
      "source": [
        "model.pytorch_model"
      ],
      "id": "091024e6"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "<torchFastText.datasets.tokenizer.NGramTokenizer at 0x7f8e2b55dfa0>"
            ]
          }
        }
      ],
      "source": [
        "model.tokenizer"
      ],
      "id": "d983b113"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "FastTextModule(\n",
              "  (model): FastTextModel(\n",
              "    (embeddings): EmbeddingBag(107992, 50, mode='mean', padding_idx=107991)\n",
              "    (emb_0): Embedding(24, 10)\n",
              "    (emb_1): Embedding(40, 10)\n",
              "    (emb_2): Embedding(8, 10)\n",
              "    (emb_3): Embedding(13, 10)\n",
              "    (emb_4): Embedding(3, 10)\n",
              "    (emb_5): Embedding(4, 10)\n",
              "    (fc): Linear(in_features=60, out_features=646, bias=True)\n",
              "  )\n",
              "  (loss): CrossEntropyLoss()\n",
              "  (accuracy_fn): MulticlassAccuracy()\n",
              ")"
            ]
          }
        }
      ],
      "source": [
        "model.lightning_module"
      ],
      "id": "9b23f1ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can also retrieve more precise information regarding the tokenizer.\n",
        "This can be useful to know how text is parsed before being given to the\n",
        "neural network:"
      ],
      "id": "b804391a-979a-4a74-a5f7-d8e27550e20e"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '</s>',\n",
            " 8097: 'lorem ipsum dolor',\n",
            " 8172: '<sit',\n",
            " 8653: 'sit amet </s>',\n",
            " 8949: '<dol',\n",
            " 9297: '<amet>',\n",
            " 15121: 'lorem>',\n",
            " 17369: 'ame',\n",
            " 18928: '<am',\n",
            " 19903: 'dol',\n",
            " 20651: 'amet>',\n",
            " 21355: '<lor',\n",
            " 22388: 'ips',\n",
            " 23844: 'lore',\n",
            " 26780: 'dolo',\n",
            " 27738: '<si',\n",
            " 29478: 'lorem ipsum',\n",
            " 31065: 'amet',\n",
            " 31980: 'met>',\n",
            " 33381: 'or>',\n",
            " 35841: 'ipsum dolor',\n",
            " 37380: '<ipsum',\n",
            " 37409: '<lorem',\n",
            " 40410: '<lore',\n",
            " 41835: 'dolor sit amet',\n",
            " 42838: 'sit',\n",
            " 43102: 'sit amet',\n",
            " 43464: 'dolor>',\n",
            " 44394: '<ips',\n",
            " 45021: 'et>',\n",
            " 45738: 'sit>',\n",
            " 45871: 'ipsu',\n",
            " 48778: 'psu',\n",
            " 48931: 'orem>',\n",
            " 49786: '<dolor',\n",
            " 50327: 'lorem',\n",
            " 53682: 'um>',\n",
            " 57345: 'it>',\n",
            " 57990: 'olor>',\n",
            " 60515: 'lor>',\n",
            " 60522: 'ore',\n",
            " 62809: 'sum>',\n",
            " 65472: 'met',\n",
            " 65559: '<amet',\n",
            " 67426: 'em>',\n",
            " 67778: 'olor',\n",
            " 67985: 'orem',\n",
            " 68529: 'psum',\n",
            " 69320: '<sit>',\n",
            " 72158: 'rem',\n",
            " 73818: 'ipsum>',\n",
            " 74637: 'dolor sit',\n",
            " 76593: 'lor',\n",
            " 77594: '<dolo',\n",
            " 78022: 'ipsum',\n",
            " 82418: 'rem>',\n",
            " 87627: '<ame',\n",
            " 89926: 'amet </s>',\n",
            " 92771: 'psum>',\n",
            " 92809: '<ip',\n",
            " 94948: 'ipsum dolor sit',\n",
            " 97065: '<ipsu',\n",
            " 99551: 'sum',\n",
            " 102333: '<do',\n",
            " 102636: 'dolor',\n",
            " 103090: 'olo',\n",
            " 106213: '<lo'}"
          ]
        }
      ],
      "source": [
        "from pprint import pprint \n",
        "sentence = [\"lorem ipsum dolor sit amet\"]\n",
        "pprint(model.tokenizer.tokenize(sentence)[2][0])"
      ],
      "id": "00c077b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving parameters to JSON can also be done after building, but the model\n",
        "needs to be rebuilt after loading."
      ],
      "id": "c1ed6afa-6b7d-4d51-a4f0-3e8845a38704"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.to_json('torchFastText_config.json')"
      ],
      "id": "bab40010"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative way to build torchFastText\n",
        "\n",
        "The training data is only useful to initialize the tokenizer, but\n",
        "X_train and y_train are not needed to initialize the PyTorch model,\n",
        "provided we give the right parameters to construct layer.\n",
        "\n",
        "To highlight this, we provide a lower-level process to build the model\n",
        "where one can first build the tokenizer, and then build the model with\n",
        "custom architecture parameters.\n",
        "\n",
        "The tokenizer can be loaded **from the same JSON file** as the model\n",
        "parameters, or initialized using the right arguments."
      ],
      "id": "017f8d12-0be8-45df-a0e4-80919c89db2d"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model"
      ],
      "id": "3b13330d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s decompose our features in two group:\n",
        "\n",
        "-   We have our textual feature stored in the first column of the\n",
        "    features matrix\n",
        "-   All other columns are categorical variables"
      ],
      "id": "c6d7a335-bd31-455c-9184-48d2f2d60fbd"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_text = X_train[:, 0].tolist()\n",
        "categorical_variables = X_train[:, 1:]"
      ],
      "id": "5f75f055"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to create a few variables that will be useful afterwards"
      ],
      "id": "adc8da37-5b6f-4c8a-8198-6c4080ffc7be"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "CAT_VOCAB_SIZE = (np.max(categorical_variables, axis=0) + 1).astype(int).tolist()\n",
        "NUM_CLASSES = len(np.unique(y_train))\n",
        "NUM_CAT_VAR = categorical_variables.shape[1]"
      ],
      "id": "931103ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s come to the nitty gritty. There are several ways to create an\n",
        "instance of the tokenizer.\n",
        "\n",
        "First, we can create the tokenizer from :\n",
        "\n",
        "-   model definition in the JSON file created beforehand\n",
        "-   textual data in training dataset"
      ],
      "id": "0d3ad544-d0f6-4d46-b322-af979c48bb43"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchFastText.datasets import NGramTokenizer\n",
        "tokenizer = NGramTokenizer.from_json('torchFastText_config.json', training_text)"
      ],
      "id": "0357c85f"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "([['<H>', '</s>', 'H </s>'],\n",
              "  ['<e>', '</s>', 'e </s>'],\n",
              "  ['<l>', '</s>', 'l </s>'],\n",
              "  ['<l>', '</s>', 'l </s>'],\n",
              "  ['<o>', '</s>', 'o </s>'],\n",
              "  ['</s>'],\n",
              "  ['<w>', '</s>', 'w </s>'],\n",
              "  ['<o>', '</s>', 'o </s>'],\n",
              "  ['<r>', '</s>', 'r </s>'],\n",
              "  ['<l>', '</s>', 'l </s>'],\n",
              "  ['<d>', '</s>', 'd </s>']],\n",
              " [tensor([40876,     0, 51965]),\n",
              "  tensor([51907,     0, 77296]),\n",
              "  tensor([74312,     0, 26137]),\n",
              "  tensor([74312,     0, 26137]),\n",
              "  tensor([ 9853,     0, 53786]),\n",
              "  tensor([0]),\n",
              "  tensor([29925,     0, 74978]),\n",
              "  tensor([ 9853,     0, 53786]),\n",
              "  tensor([ 8646,     0, 13223]),\n",
              "  tensor([74312,     0, 26137]),\n",
              "  tensor([ 89472,      0, 104945])],\n",
              " [{40876: '<H>', 0: '</s>', 51965: 'H </s>'},\n",
              "  {51907: '<e>', 0: '</s>', 77296: 'e </s>'},\n",
              "  {74312: '<l>', 0: '</s>', 26137: 'l </s>'},\n",
              "  {74312: '<l>', 0: '</s>', 26137: 'l </s>'},\n",
              "  {9853: '<o>', 0: '</s>', 53786: 'o </s>'},\n",
              "  {0: '</s>'},\n",
              "  {29925: '<w>', 0: '</s>', 74978: 'w </s>'},\n",
              "  {9853: '<o>', 0: '</s>', 53786: 'o </s>'},\n",
              "  {8646: '<r>', 0: '</s>', 13223: 'r </s>'},\n",
              "  {74312: '<l>', 0: '</s>', 26137: 'l </s>'},\n",
              "  {89472: '<d>', 0: '</s>', 104945: 'd </s>'}],\n",
              " [{'<H>': 40876, '</s>': 0, 'H </s>': 51965},\n",
              "  {'<e>': 51907, '</s>': 0, 'e </s>': 77296},\n",
              "  {'<l>': 74312, '</s>': 0, 'l </s>': 26137},\n",
              "  {'<l>': 74312, '</s>': 0, 'l </s>': 26137},\n",
              "  {'<o>': 9853, '</s>': 0, 'o </s>': 53786},\n",
              "  {'</s>': 0},\n",
              "  {'<w>': 29925, '</s>': 0, 'w </s>': 74978},\n",
              "  {'<o>': 9853, '</s>': 0, 'o </s>': 53786},\n",
              "  {'<r>': 8646, '</s>': 0, 'r </s>': 13223},\n",
              "  {'<l>': 74312, '</s>': 0, 'l </s>': 26137},\n",
              "  {'<d>': 89472, '</s>': 0, 'd </s>': 104945}])"
            ]
          }
        }
      ],
      "source": [
        "tokenizer.tokenize(\"Hello world\")"
      ],
      "id": "0b4964f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, there is a more straightforward way to do: creating directly\n",
        "the `NGramTokenizer` instance:"
      ],
      "id": "fd5b6899-7831-40a6-9841-bbc1b0804956"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = NGramTokenizer(\n",
        "    **parameters,\n",
        "    training_text=training_text\n",
        "    )"
      ],
      "id": "8a6ee96b"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "([['<H>', '</s>', 'H </s>'],\n",
              "  ['<e>', '</s>', 'e </s>'],\n",
              "  ['<l>', '</s>', 'l </s>'],\n",
              "  ['<l>', '</s>', 'l </s>'],\n",
              "  ['<o>', '</s>', 'o </s>'],\n",
              "  ['</s>'],\n",
              "  ['<w>', '</s>', 'w </s>'],\n",
              "  ['<o>', '</s>', 'o </s>'],\n",
              "  ['<r>', '</s>', 'r </s>'],\n",
              "  ['<l>', '</s>', 'l </s>'],\n",
              "  ['<d>', '</s>', 'd </s>']],\n",
              " [tensor([40876,     0, 51965]),\n",
              "  tensor([51907,     0, 77296]),\n",
              "  tensor([74312,     0, 26137]),\n",
              "  tensor([74312,     0, 26137]),\n",
              "  tensor([ 9853,     0, 53786]),\n",
              "  tensor([0]),\n",
              "  tensor([29925,     0, 74978]),\n",
              "  tensor([ 9853,     0, 53786]),\n",
              "  tensor([ 8646,     0, 13223]),\n",
              "  tensor([74312,     0, 26137]),\n",
              "  tensor([ 89472,      0, 104945])],\n",
              " [{40876: '<H>', 0: '</s>', 51965: 'H </s>'},\n",
              "  {51907: '<e>', 0: '</s>', 77296: 'e </s>'},\n",
              "  {74312: '<l>', 0: '</s>', 26137: 'l </s>'},\n",
              "  {74312: '<l>', 0: '</s>', 26137: 'l </s>'},\n",
              "  {9853: '<o>', 0: '</s>', 53786: 'o </s>'},\n",
              "  {0: '</s>'},\n",
              "  {29925: '<w>', 0: '</s>', 74978: 'w </s>'},\n",
              "  {9853: '<o>', 0: '</s>', 53786: 'o </s>'},\n",
              "  {8646: '<r>', 0: '</s>', 13223: 'r </s>'},\n",
              "  {74312: '<l>', 0: '</s>', 26137: 'l </s>'},\n",
              "  {89472: '<d>', 0: '</s>', 104945: 'd </s>'}],\n",
              " [{'<H>': 40876, '</s>': 0, 'H </s>': 51965},\n",
              "  {'<e>': 51907, '</s>': 0, 'e </s>': 77296},\n",
              "  {'<l>': 74312, '</s>': 0, 'l </s>': 26137},\n",
              "  {'<l>': 74312, '</s>': 0, 'l </s>': 26137},\n",
              "  {'<o>': 9853, '</s>': 0, 'o </s>': 53786},\n",
              "  {'</s>': 0},\n",
              "  {'<w>': 29925, '</s>': 0, 'w </s>': 74978},\n",
              "  {'<o>': 9853, '</s>': 0, 'o </s>': 53786},\n",
              "  {'<r>': 8646, '</s>': 0, 'r </s>': 13223},\n",
              "  {'<l>': 74312, '</s>': 0, 'l </s>': 26137},\n",
              "  {'<d>': 89472, '</s>': 0, 'd </s>': 104945}])"
            ]
          }
        }
      ],
      "source": [
        "tokenizer.tokenize(\"Hello world\")"
      ],
      "id": "776636e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why creating a `NGramTokenizer` separately ? Because model constructor\n",
        "is now independent from training data:"
      ],
      "id": "6b0fd6c0-9740-4a32-9bb2-4a3cfe174ea8"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-05 16:27:41 - torchFastText.model.pytorch_model - num_rows is different from the number of tokens in the tokenizer. Using provided num_rows.\n",
            "2025-03-05 16:27:42 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau)."
          ]
        }
      ],
      "source": [
        "model = torchFastText.build_from_tokenizer(\n",
        "    tokenizer, \n",
        "    embedding_dim=parameters[\"embedding_dim\"], \n",
        "    categorical_embedding_dims=parameters[\"categorical_embedding_dims\"], \n",
        "    sparse=parameters[\"sparse\"], \n",
        "    lr=parameters_train[\"lr\"], \n",
        "    num_classes=NUM_CLASSES, \n",
        "    num_categorical_features=NUM_CAT_VAR, \n",
        "    categorical_vocabulary_sizes=CAT_VOCAB_SIZE\n",
        ")"
      ],
      "id": "ee5dbe0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Warning**:\n",
        "\n",
        "If the PyTorch model building did not use the training data, please keep\n",
        "in mind that its architecture (that you customize here) should match the\n",
        "vocabulary size of the categorical variables and the total number of\n",
        "class, otherwise the model will raise an error during training.\n",
        "\n",
        "# Train a torchFastText model directly\n",
        "\n",
        "If no advanced customization or PyTorch tuning is necessary, there is a\n",
        "direct way of training model."
      ],
      "id": "f53080e9-9d78-479f-a446-2feb4a92b1de"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.train(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    num_epochs=parameters_train['num_epochs'],\n",
        "    batch_size=parameters_train['batch_size'],\n",
        "    patience_scheduler=parameters_train['patience'],\n",
        "    patience_train=parameters_train['patience'],\n",
        "    lr=parameters_train['lr'],\n",
        "    verbose = True\n",
        ")"
      ],
      "id": "ce5dc4a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load a trained model from a Lightning checkpoint\n",
        "\n",
        "/! TOCOMPLETE"
      ],
      "id": "919b67ed-4a65-4c26-92a9-771a4be3cd15"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_from_checkpoint(model.best_model_path) # or any other checkpoint path (string)"
      ],
      "id": "f560047b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting from new labels"
      ],
      "id": "e521a23b-77c4-4b0c-9940-a17b19b8111d"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = [\"coiffeur, boulangerie, pâtisserie\"] # one text description\n",
        "X= np.array([[text[0], 0, 0, 0, 0, 0, 0]]) # our new entry\n",
        "TOP_K = 5\n",
        "\n",
        "pred, conf = model.predict(X, top_k=TOP_K)\n",
        "pred_naf = encoder.inverse_transform(pred.reshape(-1))\n",
        "subset = naf2008.set_index(\"code\").loc[np.flip(pred_naf)]\n",
        "\n",
        "for i in range(TOP_K-1, -1, -1):\n",
        "    print(f\"Prediction: {pred_naf[i]}, confidence:  {conf[0, i]}, description: {subset['libelle'][pred_naf[i]]}\")"
      ],
      "id": "dbbad77d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explainability"
      ],
      "id": "f84e6bff-8fa7-4896-b60a-005ae5f1d3eb"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchFastText.explainability.visualisation import (\n",
        "    visualize_letter_scores,\n",
        "    visualize_word_scores,\n",
        ")\n",
        "\n",
        "pred, conf, all_scores, all_scores_letters = model.predict_and_explain(X)\n",
        "visualize_word_scores(all_scores, text, pred_naf.reshape(1, -1))\n",
        "visualize_letter_scores(all_scores_letters, text, pred_naf.reshape(1, -1))"
      ],
      "id": "58c46021"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  }
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from torchFastText import torchFastText\n",
    "from torchFastText.preprocess import clean_text_feature\n",
    "from torchFastText.datasets import NGramTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some utils functions that will help us format our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_surface(\n",
    "    df: pd.DataFrame, surface_feature_name: int, like_sirene_3: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Categorize the surface of the activity.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to categorize.\n",
    "        surface_feature_name (str): Name of the surface feature.\n",
    "        like_sirene_3 (bool): If True, categorize like Sirene 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a new column \"surf_cat\".\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].replace(\"nan\", np.nan)\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(float)\n",
    "    # Check surface feature exists\n",
    "    if surface_feature_name not in df.columns:\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} not found in DataFrame.\")\n",
    "    # Check surface feature is a float variable\n",
    "    if not (pd.api.types.is_float_dtype(df_copy[surface_feature_name])):\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} must be a float variable.\")\n",
    "\n",
    "    if like_sirene_3:\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy[surface_feature_name],\n",
    "            bins=[0, 120, 400, 2500, np.inf],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "    else:\n",
    "        # Log transform the surface\n",
    "        df_copy[\"surf_log\"] = np.log(df[surface_feature_name])\n",
    "\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy.surf_log,\n",
    "            bins=[0, 3, 4, 5, 12],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "\n",
    "    df_copy[surface_feature_name] = df_copy[\"surf_cat\"].replace(\"nan\", \"0\")\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(int)\n",
    "    df_copy = df_copy.drop(columns=[\"surf_log\", \"surf_cat\"], errors=\"ignore\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def clean_and_tokenize_df(\n",
    "    df,\n",
    "    categorical_features=[\"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\"],\n",
    "    text_feature=\"libelle_processed\",\n",
    "    label_col=\"apet_finale\",\n",
    "):\n",
    "    df.fillna(\"nan\", inplace=True)\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"evenement_type\": \"EVT\",\n",
    "            \"cj\": \"CJ\",\n",
    "            \"activ_nat_et\": \"NAT\",\n",
    "            \"liasse_type\": \"TYP\",\n",
    "            \"activ_surf_et\": \"SRF\",\n",
    "            \"activ_perm_et\": \"CRT\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    les = []\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        les.append(le)\n",
    "\n",
    "    df = categorize_surface(df, \"SRF\", like_sirene_3=True)\n",
    "    df = df[[text_feature, \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"SRF\", \"CRT\", label_col]]\n",
    "\n",
    "    return df, les\n",
    "\n",
    "\n",
    "def stratified_split_rare_labels(X, y, test_size=0.2, min_train_samples=1):\n",
    "    # Get unique labels and their frequencies\n",
    "    unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Separate rare and common labels\n",
    "    rare_labels = unique_labels[label_counts == 1]\n",
    "\n",
    "    # Create initial mask for rare labels to go into training set\n",
    "    rare_label_mask = np.isin(y, rare_labels)\n",
    "\n",
    "    # Separate data into rare and common label datasets\n",
    "    X_rare = X[rare_label_mask]\n",
    "    y_rare = y[rare_label_mask]\n",
    "    X_common = X[~rare_label_mask]\n",
    "    y_common = y[~rare_label_mask]\n",
    "\n",
    "    # Split common labels stratified\n",
    "    X_common_train, X_common_test, y_common_train, y_common_test = train_test_split(\n",
    "        X_common, y_common, test_size=test_size, stratify=y_common\n",
    "    )\n",
    "\n",
    "    # Combine rare labels with common labels split\n",
    "    X_train = np.concatenate([X_rare, X_common_train])\n",
    "    y_train = np.concatenate([y_rare, y_common_train])\n",
    "    X_test = X_common_test\n",
    "    y_test = y_common_test\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:21:41 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-24 18:21:41 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-24 18:21:42 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-24 18:21:48 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-24 18:21:48 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    }
   ],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "    anon=True,\n",
    ")\n",
    "df = (\n",
    "    (\n",
    "        pq.ParquetDataset(\n",
    "            \"projet-ape/extractions/20241027_sirene4.parquet\",\n",
    "            filesystem=fs,\n",
    "        )\n",
    "        .read_pandas()\n",
    "        .to_pandas()\n",
    "    )\n",
    "    .sample(frac=0.001)\n",
    "    .fillna(np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:21:54 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>libelle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0111Z</td>\n",
       "      <td>Culture de céréales (à l'exception du riz), de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112Z</td>\n",
       "      <td>Culture du riz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0113Z</td>\n",
       "      <td>Culture de légumes, de melons, de racines et d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0114Z</td>\n",
       "      <td>Culture de la canne à sucre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0115Z</td>\n",
       "      <td>Culture du tabac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>9609Z</td>\n",
       "      <td>Autres services personnels n.c.a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>9700Z</td>\n",
       "      <td>Activités des ménages en tant qu'employeurs de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>9810Z</td>\n",
       "      <td>Activités indifférenciées des ménages en tant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>9820Z</td>\n",
       "      <td>Activités indifférenciées des ménages en tant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>9900Z</td>\n",
       "      <td>Activités des organisations et organismes extr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      code                                            libelle\n",
       "0    0111Z  Culture de céréales (à l'exception du riz), de...\n",
       "1    0112Z                                     Culture du riz\n",
       "2    0113Z  Culture de légumes, de melons, de racines et d...\n",
       "3    0114Z                        Culture de la canne à sucre\n",
       "4    0115Z                                   Culture du tabac\n",
       "..     ...                                                ...\n",
       "727  9609Z                  Autres services personnels n.c.a.\n",
       "728  9700Z  Activités des ménages en tant qu'employeurs de...\n",
       "729  9810Z  Activités indifférenciées des ménages en tant ...\n",
       "730  9820Z  Activités indifférenciées des ménages en tant ...\n",
       "731  9900Z  Activités des organisations et organismes extr...\n",
       "\n",
       "[732 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with fs.open(\"projet-ape/data/naf2008.csv\") as file:\n",
    "    naf2008 = pd.read_csv(file, sep=\";\")\n",
    "naf2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_libelles(\n",
    "    df: pd.DataFrame,\n",
    "    df_naf: pd.DataFrame,\n",
    "    y: str,\n",
    "    text_feature: str,\n",
    "    textual_features: list,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    missing_codes = set(df_naf[\"code\"])\n",
    "    fake_obs = df_naf[df_naf[\"code\"].isin(missing_codes)]\n",
    "    fake_obs[y] = fake_obs[\"code\"]\n",
    "    fake_obs[text_feature] = fake_obs[[text_feature]].apply(\n",
    "        lambda row: \" \".join(f\"[{col}] {val}\" for col, val in row.items() if val != \"\"), axis=1\n",
    "    )\n",
    "    df = pd.concat([df, fake_obs[[col for col in fake_obs.columns if col in df.columns]]])\n",
    "\n",
    "    if textual_features is not None:\n",
    "        for feature in textual_features:\n",
    "            df[feature] = df[feature].fillna(value=\"\")\n",
    "    if categorical_features is not None:\n",
    "        for feature in categorical_features:\n",
    "            df[feature] = df[feature].fillna(value=\"NaN\")\n",
    "\n",
    "    print(f\"\\t*** {len(missing_codes)} codes have been added in the database...\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** 732 codes have been added in the database...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\n",
    "    \"evenement_type\",\n",
    "    \"cj\",\n",
    "    \"activ_nat_et\",\n",
    "    \"liasse_type\",\n",
    "    \"activ_surf_et\",\n",
    "    \"activ_perm_et\",\n",
    "]\n",
    "text_feature = \"libelle\"\n",
    "y = \"apet_finale\"\n",
    "textual_features = None\n",
    "\n",
    "df = add_libelles(df, naf2008, y, text_feature, textual_features, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make available our processing function clean_text_feature for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"libelle_processed\"] = clean_text_feature(df[\"libelle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the columns in the right format:\n",
    " - First column contains the processed text (str)\n",
    " - Next ones contain the \"tokenized\" categorical (discrete) variables in int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['format' 1 13 ... 1 1 0]\n",
      " [\"l'acquisition, gestion, l'administration, mis valeur, transformation, construction, location, cession tous immeubl\"\n",
      "  1 19 ... 3 0 0]\n",
      " [\"distribu materiel medical, l'assist administr operationnel aupr professionnel sante, conseil managem\"\n",
      "  1 13 ... 1 1 0]\n",
      " ...\n",
      " ['[libelle] activit indifferencie menag tant producteur bien usag propr'\n",
      "  22 26 ... 7 0 0]\n",
      " ['[libelle] activit indifferencie menag tant producteur servic usag propr'\n",
      "  22 26 ... 7 0 0]\n",
      " ['[libelle] activit organis organ extraterritorial' 22 26 ... 7 0 0]]\n",
      "[660 579 409 ... 729 730 731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41489/1529226646.py:55: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"nan\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle_processed\")\n",
    "X = df[[\"libelle_processed\", \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
    "y = df[\"apet_finale\"].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test sets. We especially take care that:  \n",
    "- classes with only one instance appear in the train set (instead of the test set)\n",
    "- all classes are represented in the train set\n",
    "\n",
    "The `stratified_split_rare_labels` function from the `preprocess` subpackage is used to carefully split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = stratified_split_rare_labels(X, y)\n",
    "assert set(range(len(naf2008[\"code\"]))) == set(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the torch-fastText model (without training it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model (without building it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model building\n",
    "NUM_TOKENS = int(1e5)  # Number of rows in the embedding matrix\n",
    "EMBED_DIM = 50  # Dimension of the embedding = number of columns in the embedding matrix\n",
    "SPARSE = (\n",
    "    False  # Whether to use sparse Embedding layer for fast computation (see PyTorch documentation)\n",
    ")\n",
    "CAT_EMBED_DIM = 10  # Dimension of the embedding for categorical features\n",
    "\n",
    "# Parameters for tokenizer\n",
    "MIN_COUNT = (\n",
    "    1  # Minimum number of occurrences of a word in the corpus to be included in the vocabulary\n",
    ")\n",
    "MIN_N = 3  # Minimum length of char n-grams\n",
    "MAX_N = 6  # Maximum length of char n-grams\n",
    "LEN_WORD_NGRAMS = 3  # Length of word n-grams\n",
    "\n",
    "# Parameters for training - not useful immediately\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchFastText(\n",
    "    num_tokens=NUM_TOKENS,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    categorical_embedding_dims=CAT_EMBED_DIM,\n",
    "    min_count=MIN_COUNT,\n",
    "    min_n=MIN_N,\n",
    "    max_n=MAX_N,\n",
    "    len_word_ngrams=LEN_WORD_NGRAMS,\n",
    "    sparse=SPARSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save these parameters to a JSON file. Initialization can also be done providing a JSON file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_json(\"torchFastText_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchFastText.from_json(\"torchFastText_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model using the training data. We have now access to the tokenizer, the PyTorch model as well as a PyTorch Lightning module ready to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:21:56 - torchFastText.model.pytorch_model - num_rows is different from the number of tokens in the tokenizer. Using provided num_rows.\n",
      "2025-02-24 18:21:56 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau).\n"
     ]
    }
   ],
   "source": [
    "LR = 4e-3\n",
    "model.build(X_train, y_train, lightning=True, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextModel(\n",
      "  (embeddings): EmbeddingBag(103910, 50, mode='mean', padding_idx=103909)\n",
      "  (emb_0): Embedding(23, 10)\n",
      "  (emb_1): Embedding(27, 10)\n",
      "  (emb_2): Embedding(8, 10)\n",
      "  (emb_3): Embedding(11, 10)\n",
      "  (emb_4): Embedding(3, 10)\n",
      "  (emb_5): Embedding(4, 10)\n",
      "  (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      ")\n",
      "<NGramTokenizer(min_n=3, max_n=6, num_tokens=100000, word_ngrams=3, nwords=3909)>\n",
      "FastTextModule(\n",
      "  (model): FastTextModel(\n",
      "    (embeddings): EmbeddingBag(103910, 50, mode='mean', padding_idx=103909)\n",
      "    (emb_0): Embedding(23, 10)\n",
      "    (emb_1): Embedding(27, 10)\n",
      "    (emb_2): Embedding(8, 10)\n",
      "    (emb_3): Embedding(11, 10)\n",
      "    (emb_4): Embedding(3, 10)\n",
      "    (emb_5): Embedding(4, 10)\n",
      "    (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (accuracy_fn): MulticlassAccuracy()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)\n",
    "print(model.lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is useful to initialize the full torchFastText model without training it, if needed for some reason. But if it is not necessary, and we could have directly launched the training (building is then handled automatically if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{102131: '<lo', 72511: 'lor', 56440: 'ore', 68076: 'rem', 63344: 'em>', 17273: '<lor', 19762: 'lore', 63903: 'orem', 78336: 'rem>', 36328: '<lore', 46245: 'lorem', 44849: 'orem>', 33327: '<lorem', 11039: 'lorem>', 88727: '<ip', 18306: 'ips', 44696: 'psu', 95469: 'sum', 49600: 'um>', 40312: '<ips', 41789: 'ipsu', 64447: 'psum', 58727: 'sum>', 92983: '<ipsu', 73940: 'ipsum', 88689: 'psum>', 33298: '<ipsum', 69736: 'ipsum>', 98251: '<do', 15821: 'dol', 99008: 'olo', 29299: 'or>', 4867: '<dol', 22698: 'dolo', 63696: 'olor', 56433: 'lor>', 73512: '<dolo', 98554: 'dolor', 53908: 'olor>', 45704: '<dolor', 39382: 'dolor>', 23656: '<si', 38756: 'sit', 53263: 'it>', 4090: '<sit', 41656: 'sit>', 65238: '<sit>', 14846: '<am', 13287: 'ame', 61390: 'met', 40939: 'et>', 83545: '<ame', 26983: 'amet', 27898: 'met>', 61477: '<amet', 16569: 'amet>', 5215: '<amet>', 0: '</s>', 25396: 'lorem ipsum', 31759: 'ipsum dolor', 70555: 'dolor sit', 39020: 'sit amet', 85844: 'amet </s>', 4015: 'lorem ipsum dolor', 90866: 'ipsum dolor sit', 37753: 'dolor sit amet', 4571: 'sit amet </s>'}]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"lorem ipsum dolor sit amet\"]\n",
    "print(model.tokenizer.tokenize(sentence)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving parameters to JSON can also be done after building, but the model needs to be rebuilt after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:21:57 - torchFastText.model.pytorch_model - num_rows is different from the number of tokens in the tokenizer. Using provided num_rows.\n",
      "2025-02-24 18:21:57 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau).\n"
     ]
    }
   ],
   "source": [
    "model.to_json(\"torchFastText_config.json\")\n",
    "model = torchFastText.from_json(\"torchFastText_config.json\")\n",
    "model.build(X_train, y_train, lightning=True, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way to build torchFastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is only useful to initialize the tokenizer, but X_train and y_train are not needed to initialize the PyTorch model, provided we give the right parameters to construct layer. \n",
    "\n",
    "To highlight this, we provide a lower-level process to build the model where one can first build the tokenizer, and then build the model with custom architecture parameters. \n",
    "\n",
    "The tokenizer can be loaded **from the same JSON file** as the model parameters, or initialized using the right arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:23:48 - torchFastText.model.pytorch_model - num_rows is different from the number of tokens in the tokenizer. Using provided num_rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:23:48 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau).\n"
     ]
    }
   ],
   "source": [
    "training_text = X_train[:, 0].tolist()\n",
    "categorical_variables = X_train[:, 1:]\n",
    "\n",
    "# Before: this was inferred during the build method ; now required\n",
    "CAT_VOCAB_SIZE = (np.max(categorical_variables, axis=0) + 1).astype(int).tolist()\n",
    "NUM_CLASSES = len(np.unique(y_train))\n",
    "NUM_CAT_VAR = categorical_variables.shape[1]\n",
    "\n",
    "# Tokenizer needs training text to build the vocabulary\n",
    "tokenizer = NGramTokenizer.from_json(\n",
    "    \"torchFastText_config.json\", training_text\n",
    ")  # alternative 1 - see that it is the same JSON file as before\n",
    "tokenizer = NGramTokenizer(\n",
    "    min_n=MIN_N,\n",
    "    max_n=MAX_N,\n",
    "    num_tokens=NUM_TOKENS,\n",
    "    len_word_ngrams=LEN_WORD_NGRAMS,\n",
    "    min_count=MIN_COUNT,\n",
    "    training_text=training_text,\n",
    ")  # alternative 2\n",
    "\n",
    "# This model constructor is now independent from training data\n",
    "model = torchFastText.build_from_tokenizer(\n",
    "    tokenizer,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    categorical_embedding_dims=CAT_EMBED_DIM,\n",
    "    sparse=SPARSE,\n",
    "    lr=LR,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_categorical_features=NUM_CAT_VAR,\n",
    "    categorical_vocabulary_sizes=CAT_VOCAB_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the PyTorch model and the Lightning module are now directly built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextModel(\n",
      "  (embeddings): EmbeddingBag(103910, 50, mode='mean', padding_idx=103909)\n",
      "  (emb_0): Embedding(23, 10)\n",
      "  (emb_1): Embedding(27, 10)\n",
      "  (emb_2): Embedding(8, 10)\n",
      "  (emb_3): Embedding(11, 10)\n",
      "  (emb_4): Embedding(3, 10)\n",
      "  (emb_5): Embedding(4, 10)\n",
      "  (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      ")\n",
      "<NGramTokenizer(min_n=3, max_n=6, num_tokens=100000, word_ngrams=3, nwords=3909)>\n",
      "FastTextModule(\n",
      "  (model): FastTextModel(\n",
      "    (embeddings): EmbeddingBag(103910, 50, mode='mean', padding_idx=103909)\n",
      "    (emb_0): Embedding(23, 10)\n",
      "    (emb_1): Embedding(27, 10)\n",
      "    (emb_2): Embedding(8, 10)\n",
      "    (emb_3): Embedding(11, 10)\n",
      "    (emb_4): Embedding(3, 10)\n",
      "    (emb_5): Embedding(4, 10)\n",
      "    (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (accuracy_fn): MulticlassAccuracy()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)\n",
    "print(model.lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the PyTorch model building did not use the training data, please keep in mind that its architecture (that you customize here) should match the vocabulary size of the categorical variables and the total number of class, otherwise the model will raise an error during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a torchFastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:24:11 - torchFastText.torchFastText - Checking inputs...\n",
      "2025-02-24 18:24:11 - torchFastText.torchFastText - Inputs successfully checked. Starting the training process..\n",
      "2025-02-24 18:24:11 - torchFastText.torchFastText - Running on: cpu\n",
      "2025-02-24 18:24:11 - torchFastText.datasets.dataset - Creating DataLoader with 12 workers.\n",
      "2025-02-24 18:24:11 - torchFastText.datasets.dataset - Creating DataLoader with 12 workers.\n",
      "2025-02-24 18:24:11 - torchFastText.torchFastText - Lightning module successfully created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "2025-02-24 18:24:11 - torchFastText.torchFastText - Launching training...\n",
      "\n",
      "  | Name        | Type               | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model       | FastTextModel      | 5.2 M  | train\n",
      "1 | loss        | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy_fn | MulticlassAccuracy | 0      | train\n",
      "-----------------------------------------------------------\n",
      "5.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 M     Total params\n",
      "20.964    Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca73c1e40c7464b91c4fa2d6e9a17c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf258c782e0b4c8dba60237184099706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20331f89ef3435082c36ac1202262c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "2025-02-24 18:24:46 - torchFastText.torchFastText - Training done in 35.16 seconds.\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience_scheduler=PATIENCE,\n",
    "    patience_train=PATIENCE,\n",
    "    lr=LR,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "## The library uses lightning library to train the model. It is possible to add some specific parameters to the training method to use it :\n",
    "##\n",
    "## trainer_params = {'profiler': 'simple', 'enable_progress_bar': False}\n",
    "##\n",
    "## model.train(\n",
    "##    X_train,\n",
    "##    y_train,\n",
    "##    X_test,\n",
    "##    y_test,\n",
    "##    num_epochs=NUM_EPOCHS,\n",
    "##    batch_size=BATCH_SIZE,\n",
    "##    patience_scheduler=PATIENCE,\n",
    "##    patience_train=PATIENCE,\n",
    "##    lr=LR,\n",
    "##    verbose = True,\n",
    "##    trainer_params = trainer_params\n",
    "##)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained model from a Lightning checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model.load_from_checkpoint(model.best_model_path)  # or any other checkpoint path (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 0,\n",
       " 'global_step': 11,\n",
       " 'pytorch-lightning_version': '2.5.0.post0',\n",
       " 'state_dict': OrderedDict([('model.embeddings.weight',\n",
       "               tensor([[-0.4155,  0.5001, -0.0511,  ..., -0.2551, -1.0067,  0.4193],\n",
       "                       [-0.6866, -0.5464,  0.3444,  ..., -0.3271,  1.8750, -1.1317],\n",
       "                       [-0.8242, -0.1431, -3.5056,  ...,  1.0447, -0.4933, -1.1999],\n",
       "                       ...,\n",
       "                       [ 2.1735, -0.8185, -0.1860,  ...,  0.8569,  0.8747, -0.8153],\n",
       "                       [ 0.1416, -0.4247,  0.0305,  ...,  1.0649, -0.9892,  0.3626],\n",
       "                       [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])),\n",
       "              ('model.emb_0.weight',\n",
       "               tensor([[ 0.2547, -2.8003,  1.4013,  1.5881, -0.7033,  0.8540, -0.5783,  0.2256,\n",
       "                         0.7499,  2.9711],\n",
       "                       [-0.2905,  0.4540,  0.7088, -2.1960, -0.0837,  0.2569,  0.0917, -0.4995,\n",
       "                        -0.7544, -0.8420],\n",
       "                       [ 0.7764,  0.2944, -1.3286,  1.0465,  0.1634, -0.2970, -1.4491,  1.4434,\n",
       "                         0.3944, -0.5385],\n",
       "                       [ 0.5577, -0.1879, -0.8057,  0.4589, -1.5929, -1.1799,  0.3509,  1.1533,\n",
       "                         0.2778, -0.6901],\n",
       "                       [ 0.3266,  0.2407, -0.1203,  0.7924,  1.5253,  0.4662,  0.2328,  0.7615,\n",
       "                        -0.1925,  0.3005],\n",
       "                       [ 0.5094,  0.4517, -0.7178,  0.3699, -1.2519, -0.6432, -0.3173,  0.0706,\n",
       "                         0.0479, -0.7739],\n",
       "                       [ 0.1394, -0.0769, -1.3504,  0.4276, -0.4167,  0.3866,  0.4063, -0.0275,\n",
       "                        -1.7125,  1.9615],\n",
       "                       [-1.1822, -0.9157, -0.9104,  0.8130,  2.2933,  0.5746, -0.3415,  0.8826,\n",
       "                         0.2425,  1.1833],\n",
       "                       [-0.0314,  0.4494,  1.7342, -1.4387, -1.1751,  1.5138,  1.0509, -0.5927,\n",
       "                        -0.4578,  0.4731],\n",
       "                       [ 0.4088,  2.0828,  0.5645, -0.8062, -0.2043,  2.0525, -1.4807,  0.1199,\n",
       "                        -0.8415, -0.7853],\n",
       "                       [-0.2648, -0.7921, -0.4095, -0.0614,  0.0308, -2.0528, -0.3520,  0.4218,\n",
       "                         0.3649,  0.7351],\n",
       "                       [-0.2440, -0.4523,  1.5584, -1.1142,  0.2379,  2.0411,  0.3199,  0.9526,\n",
       "                         1.5570,  0.3942],\n",
       "                       [ 0.2287,  0.1728, -0.2161,  0.3742,  0.4299,  0.6654,  1.2247,  0.2753,\n",
       "                        -2.4884, -0.4199],\n",
       "                       [ 0.5021, -0.8172, -0.7520, -0.0741, -0.8070,  0.3246, -1.1347, -0.3939,\n",
       "                         1.2984,  0.3272],\n",
       "                       [-0.5994,  0.8284, -1.5543, -0.0285, -0.0354, -0.5043, -1.4973, -0.2206,\n",
       "                         0.3224,  1.9807],\n",
       "                       [ 0.4554,  0.2345, -0.9494,  0.5340,  0.8510, -2.5410, -1.8590,  0.4826,\n",
       "                        -0.0376, -0.2124],\n",
       "                       [-0.0106,  1.0781, -0.7468, -0.1119,  0.8060, -0.2719, -0.6770,  0.2100,\n",
       "                        -0.3834,  2.3817],\n",
       "                       [ 0.2904,  0.9623, -0.1900, -0.8066,  1.6398, -0.2973, -0.9863, -1.2738,\n",
       "                        -0.9691,  0.4614],\n",
       "                       [-0.4607, -0.1253,  1.3512,  0.4569,  0.8332,  1.2529,  0.7226, -2.8686,\n",
       "                        -0.0835, -0.2440],\n",
       "                       [ 1.7448,  0.9602, -0.1943, -1.0752, -0.2310, -2.1472,  0.9624,  0.3308,\n",
       "                         0.9984,  0.5242],\n",
       "                       [ 0.7399, -0.5175, -0.6300, -0.7368,  0.0636, -1.0194,  0.4594, -1.1465,\n",
       "                        -0.2824,  1.3932],\n",
       "                       [-0.0758,  2.1247,  0.6957,  1.5734,  0.0259,  1.3521,  1.0868, -0.2154,\n",
       "                         0.2219,  0.3132],\n",
       "                       [-1.8286, -0.8599, -0.1836, -0.8762, -0.8216,  0.9904,  0.8690, -0.6655,\n",
       "                         0.5540, -0.1694]])),\n",
       "              ('model.emb_1.weight',\n",
       "               tensor([[ 1.4397e-01, -4.2979e-01, -9.1404e-01,  6.5630e-01,  7.7382e-02,\n",
       "                         6.2288e-01, -7.1845e-01, -5.0776e-01,  8.6037e-01,  3.7829e-01],\n",
       "                       [-5.4717e-01, -2.0323e+00, -8.5263e-01, -3.8038e-01,  7.3903e-01,\n",
       "                        -2.3590e-01, -5.7225e-01,  1.9945e+00, -2.0421e+00,  1.8104e+00],\n",
       "                       [ 1.5440e+00,  1.7571e-01, -3.2311e-02,  4.5584e-02, -1.2590e+00,\n",
       "                         1.2217e-01,  7.4025e-01,  4.5442e-01, -2.1475e-01, -1.4698e+00],\n",
       "                       [ 4.4170e-01,  8.1123e-01,  8.4055e-01, -6.5144e-01, -6.3659e-01,\n",
       "                         2.0726e+00, -2.3308e-01,  4.6460e-02, -1.6742e+00,  3.5441e-01],\n",
       "                       [-1.0068e+00, -5.2642e-01, -1.2789e+00, -5.1854e-01, -2.9618e-02,\n",
       "                         4.2921e-01, -1.3831e+00, -3.9553e-01,  3.3291e-01, -2.8876e-01],\n",
       "                       [ 7.8558e-01,  3.7186e-02, -4.3769e-01, -9.9190e-01,  1.7743e+00,\n",
       "                         1.5573e-02,  1.9233e-01, -9.9802e-01,  1.4288e+00, -9.4944e-01],\n",
       "                       [-1.1812e+00,  9.8195e-02,  2.1570e-01,  7.3852e-02,  3.8723e-01,\n",
       "                         1.1414e+00,  6.4123e-01,  2.5340e+00,  1.0972e-01,  8.9246e-02],\n",
       "                       [ 5.7557e-01, -2.2070e-01,  5.2491e-01, -6.1075e-01,  2.7321e-01,\n",
       "                        -2.1241e-01,  9.7154e-01, -1.5351e+00,  2.5628e-02, -6.0607e-01],\n",
       "                       [-7.2420e-01, -8.5457e-01, -9.0624e-01, -1.4768e+00,  1.1395e+00,\n",
       "                         9.5174e-01,  1.4272e+00,  4.3689e-01,  3.4706e-01, -1.0215e+00],\n",
       "                       [-1.1995e+00, -3.0838e+00, -3.2510e-01, -1.6109e+00, -9.8429e-01,\n",
       "                        -2.0027e-01,  1.6317e+00, -2.1273e+00,  1.3080e+00, -3.3240e-01],\n",
       "                       [ 1.0486e-01,  4.9137e-01, -9.5157e-01,  2.0058e-01,  3.5269e-01,\n",
       "                         1.3308e-01,  8.5946e-01, -8.5246e-01, -9.3709e-01, -9.1779e-01],\n",
       "                       [-4.1000e-01,  1.5636e+00,  1.0497e+00,  1.5547e+00, -2.2922e-01,\n",
       "                         1.1527e+00, -2.3647e-01, -2.4388e-01, -1.3912e+00,  6.5021e-01],\n",
       "                       [ 4.2379e-01, -5.0672e-01, -4.5399e-01, -1.1714e+00,  1.4214e+00,\n",
       "                        -9.6597e-01,  2.2283e-01,  1.0858e+00, -1.4795e-01, -1.1864e+00],\n",
       "                       [ 1.5767e+00,  2.3617e+00, -6.5473e-01,  4.5054e-01,  1.1621e+00,\n",
       "                         8.4238e-01,  1.2586e+00, -5.1689e-01, -1.7464e-01, -1.5599e+00],\n",
       "                       [-3.4253e-01,  3.8618e-01,  1.6668e+00, -8.7767e-01,  6.9914e-01,\n",
       "                         4.5098e-01,  8.7014e-01,  6.3832e-01, -1.7012e+00, -9.4910e-01],\n",
       "                       [ 1.2409e+00, -8.8732e-02, -3.2997e-01, -9.7057e-01,  1.0109e+00,\n",
       "                        -9.6095e-01,  1.3818e-01, -4.9717e-02, -2.8177e-01,  6.7266e-01],\n",
       "                       [-9.7105e-01, -1.9079e-01,  2.2991e-01, -1.4209e+00, -8.9965e-02,\n",
       "                         4.7654e-01, -6.3897e-01,  3.3143e-01,  9.4224e-01, -8.9406e-01],\n",
       "                       [-7.9481e-01, -2.0982e-02,  3.8794e-01,  7.7866e-01, -3.6428e-01,\n",
       "                        -6.2995e-01, -4.2477e-01,  3.6359e-01,  4.5133e-01, -2.3528e+00],\n",
       "                       [ 7.4306e-01, -1.0488e+00,  2.0709e-01,  1.4073e-01,  1.5824e+00,\n",
       "                         1.6737e+00, -9.0351e-01, -1.6170e-01, -7.3325e-01, -2.1943e+00],\n",
       "                       [ 1.2595e+00,  9.6244e-04,  4.9849e-01, -5.5572e-01,  2.7816e-01,\n",
       "                         2.0225e+00, -1.0698e+00, -4.3249e-01, -1.3129e+00,  1.0897e+00],\n",
       "                       [ 1.5260e+00,  1.3195e+00, -7.7095e-01, -6.0084e-01,  4.8783e-01,\n",
       "                         5.5206e-01,  3.1862e-01, -1.1230e+00,  4.9105e-01, -4.4934e-01],\n",
       "                       [-1.2490e+00,  1.4844e+00, -1.3379e+00, -1.5693e-01,  3.0984e-01,\n",
       "                        -6.5646e-02, -1.1226e+00,  9.0881e-01, -1.1782e+00,  2.4645e-01],\n",
       "                       [ 5.2935e-01,  4.1120e-01,  6.5713e-02, -1.1175e+00,  3.9157e-01,\n",
       "                        -7.0903e-01, -1.2994e+00, -1.9751e+00, -3.9794e-01,  6.4360e-01],\n",
       "                       [-1.5100e+00, -1.5514e+00, -1.7340e+00,  7.0075e-01,  9.9412e-01,\n",
       "                         2.0263e-01,  1.6028e+00, -2.1641e+00, -1.6747e-01,  4.4282e-01],\n",
       "                       [-1.5053e-01,  1.8302e-01, -1.6262e+00,  6.3542e-01,  1.0570e-01,\n",
       "                         3.8559e-01,  8.1569e-01,  1.7522e+00,  2.6856e-01,  2.9291e-01],\n",
       "                       [-7.7674e-01,  2.0520e+00, -1.2750e+00,  1.0451e+00, -4.5219e-01,\n",
       "                        -1.7911e+00, -6.8905e-01,  4.4072e-01, -3.0356e-01, -3.0727e-01],\n",
       "                       [-3.7224e-01,  1.3075e-02,  1.2209e+00, -1.1331e+00, -8.6578e-01,\n",
       "                         1.1136e+00,  5.2025e-01,  9.3417e-01, -2.2309e+00,  1.2894e-01]])),\n",
       "              ('model.emb_2.weight',\n",
       "               tensor([[-0.2292, -1.2405,  0.7206,  0.4685, -1.3196,  0.1045,  1.7887, -0.9646,\n",
       "                         1.0955,  1.0665],\n",
       "                       [-0.3328, -1.2965,  0.3389,  0.4370,  0.6019, -0.8137, -0.3134, -0.5866,\n",
       "                         0.8577,  0.4185],\n",
       "                       [-0.2736,  0.6078,  0.4794,  0.5001,  0.6630, -0.0861, -0.9853, -1.2154,\n",
       "                        -0.2877, -0.6215],\n",
       "                       [ 1.4157,  0.3197,  0.4920, -0.2347, -0.0448, -2.0249,  0.9364, -0.3045,\n",
       "                        -1.6788, -0.0329],\n",
       "                       [-0.3737, -0.9694, -0.1592, -0.1821, -1.1786,  0.4798,  0.9184,  0.4257,\n",
       "                        -0.0257, -0.2034],\n",
       "                       [ 1.6968, -1.0561, -1.0461, -1.2691,  0.2780,  1.1827, -0.0746, -0.1129,\n",
       "                         1.4875,  0.9963],\n",
       "                       [ 0.5077, -0.5061, -0.8085,  0.1307, -0.2840,  0.9193, -0.2122, -0.2110,\n",
       "                        -0.1384, -0.6152],\n",
       "                       [ 0.6810,  0.3215,  0.0517, -0.1439,  0.8692, -1.4492, -0.7668, -1.4526,\n",
       "                        -1.1909, -0.7350]])),\n",
       "              ('model.emb_3.weight',\n",
       "               tensor([[ 0.3027, -0.1921,  1.0214, -2.3896, -0.1901,  0.0185, -1.0370,  1.3489,\n",
       "                        -0.7736,  0.2090],\n",
       "                       [ 0.0903, -0.3590,  0.2626,  0.4052,  0.3324,  0.6844, -0.4409,  0.1987,\n",
       "                         0.3039, -0.6316],\n",
       "                       [ 0.9002, -1.0063, -1.4252,  0.5734,  0.2542,  0.5686, -0.6322,  1.2570,\n",
       "                         2.1044,  1.4971],\n",
       "                       [-0.9364, -0.1591,  0.4086, -0.5350,  0.5424, -0.7821,  0.0762, -0.3668,\n",
       "                         0.1522,  0.6597],\n",
       "                       [-0.4591, -0.0702, -2.0214, -1.4642, -1.3659,  1.8508, -1.2854,  0.7659,\n",
       "                        -0.4648,  0.2193],\n",
       "                       [ 1.3596, -0.1357, -0.7627,  0.5043,  0.7832,  1.1214,  0.7954, -1.0094,\n",
       "                        -1.8407,  0.3793],\n",
       "                       [ 1.2133, -3.1952,  0.8116,  0.4774, -1.2802, -0.3556, -0.9495, -0.7332,\n",
       "                         0.0131, -0.1465],\n",
       "                       [ 1.9208, -0.0780, -0.0646,  0.3431,  0.5099,  0.1701, -0.3732,  0.3198,\n",
       "                        -0.2278,  0.1619],\n",
       "                       [-0.4567,  0.9505,  1.6226,  0.9601, -0.2684, -2.5424,  1.4763, -1.2322,\n",
       "                         0.6272,  0.2531],\n",
       "                       [ 0.2902, -0.4059,  0.3441,  1.6801,  1.0544,  1.5416, -0.0341, -1.4252,\n",
       "                        -0.4251,  1.7434],\n",
       "                       [-0.0454,  0.2379, -0.9761,  0.1495,  0.7509,  0.4459, -0.4523, -0.0690,\n",
       "                        -2.1656,  0.1657]])),\n",
       "              ('model.emb_4.weight',\n",
       "               tensor([[ 0.0865, -0.2669, -1.5804, -2.0702, -0.5976, -0.8932,  0.4712,  0.5003,\n",
       "                        -0.5975,  1.3184],\n",
       "                       [ 0.0376,  0.7054, -1.2901, -0.7775, -0.0995, -1.1941, -0.4591,  0.8472,\n",
       "                         0.3243, -1.5061],\n",
       "                       [-1.1290, -1.1999,  1.8021, -0.9535, -0.4389, -0.5378, -0.5868,  0.3677,\n",
       "                        -1.1304, -2.1722]])),\n",
       "              ('model.emb_5.weight',\n",
       "               tensor([[-0.3645,  0.1450, -0.8149, -1.0316, -0.8380, -0.4015,  0.0541,  1.8142,\n",
       "                        -0.0898,  0.8941],\n",
       "                       [ 1.2941, -1.1471,  0.0606, -1.1600, -0.3592, -0.0370, -2.1427,  2.0497,\n",
       "                        -1.0545,  0.3974],\n",
       "                       [-1.5441, -1.1856, -0.8924,  0.7793, -0.4881,  1.1631,  0.6431, -0.2927,\n",
       "                         0.6836,  0.5434],\n",
       "                       [-0.7660,  0.1227, -0.7858, -0.5370,  1.0111, -2.0497,  1.0681, -1.4446,\n",
       "                        -0.2892,  0.8585]])),\n",
       "              ('model.fc.weight',\n",
       "               tensor([[ 0.0851, -0.0164,  0.0805,  ..., -0.0016,  0.0751,  0.0488],\n",
       "                       [-0.0969,  0.0262, -0.0948,  ..., -0.1455,  0.0768, -0.0383],\n",
       "                       [ 0.0728, -0.0409, -0.0241,  ..., -0.0291,  0.0513,  0.1465],\n",
       "                       ...,\n",
       "                       [-0.0400, -0.0305,  0.0582,  ..., -0.0556, -0.0212, -0.0566],\n",
       "                       [ 0.0465, -0.0167,  0.1490,  ..., -0.0604, -0.0629, -0.0239],\n",
       "                       [ 0.1132,  0.0008,  0.0244,  ..., -0.1149, -0.0008,  0.1364]])),\n",
       "              ('model.fc.bias',\n",
       "               tensor([-1.1277e-01,  2.7171e-02, -1.2266e-01, -2.0617e-02, -6.6752e-03,\n",
       "                       -8.5297e-02, -5.5133e-02, -7.6835e-02, -9.9833e-02,  7.7459e-02,\n",
       "                        4.0140e-02, -1.3551e-01,  6.1640e-02,  7.1488e-02,  2.3445e-02,\n",
       "                        1.1215e-01,  6.6877e-02, -1.3247e-01,  7.6848e-02, -1.8986e-03,\n",
       "                       -6.6139e-02, -9.6627e-02,  1.0337e-01,  8.8911e-02,  9.6463e-02,\n",
       "                       -5.4125e-02,  9.2960e-04, -1.1132e-01,  1.0890e-01, -2.2303e-03,\n",
       "                        1.9600e-02, -3.1077e-02,  1.2272e-02, -4.4723e-02,  3.4513e-03,\n",
       "                        4.0088e-02, -1.2969e-02, -8.7200e-02, -9.7451e-02, -4.0752e-02,\n",
       "                       -3.8019e-02, -2.9226e-02, -1.1478e-01, -6.8928e-03,  3.9123e-02,\n",
       "                       -4.9087e-02,  6.0232e-02,  1.0562e-01, -9.0361e-02, -3.4972e-02,\n",
       "                        3.5129e-03, -1.2240e-01, -4.9296e-02,  2.3443e-04,  1.3333e-02,\n",
       "                       -1.1988e-01, -1.2415e-02, -1.7651e-02,  5.5468e-02, -3.6294e-02,\n",
       "                       -1.0647e-01, -1.0092e-02, -1.4911e-01, -2.4268e-02,  9.0152e-02,\n",
       "                        8.2969e-03,  4.8157e-02,  9.4342e-02, -2.9427e-02,  1.6714e-02,\n",
       "                       -2.5557e-02,  9.8481e-02,  9.4332e-02, -2.1478e-02, -1.0767e-01,\n",
       "                       -1.1618e-01, -4.9889e-02,  8.7361e-02,  7.5803e-02,  9.2873e-02,\n",
       "                       -1.1094e-01,  2.6757e-02,  6.4471e-02, -1.0739e-01, -1.3442e-01,\n",
       "                       -4.2586e-03, -7.4821e-02, -1.3863e-02, -7.2372e-02, -8.2874e-02,\n",
       "                        9.2066e-02,  5.8996e-02, -1.3589e-01, -1.5310e-02, -1.3787e-01,\n",
       "                       -5.7546e-02, -1.3620e-01,  8.9549e-02, -2.0483e-02, -1.2874e-01,\n",
       "                        3.0338e-02, -2.3842e-02,  8.0629e-03, -9.8376e-02, -2.9905e-02,\n",
       "                       -4.1310e-02, -1.3675e-01,  2.8557e-02, -8.2970e-03, -1.1392e-01,\n",
       "                        3.9679e-02,  7.8456e-02, -1.3827e-01,  2.0689e-02,  7.5249e-02,\n",
       "                       -1.3616e-01,  3.9558e-02,  6.2328e-03,  3.9394e-02,  7.3795e-03,\n",
       "                        8.6094e-02,  4.8117e-03, -7.4934e-03,  4.6829e-02,  4.9645e-02,\n",
       "                        1.1058e-01, -1.2168e-01, -1.3307e-01, -1.0290e-01, -1.0767e-01,\n",
       "                       -1.0299e-01, -1.0907e-01, -1.1191e-01, -3.8664e-02, -6.7935e-02,\n",
       "                        5.2879e-03, -1.0581e-01, -6.8876e-02,  1.1944e-01,  2.6137e-02,\n",
       "                        1.0083e-02, -4.4622e-02, -2.2781e-02, -1.2062e-01, -3.6783e-02,\n",
       "                       -5.6962e-02, -5.2991e-02,  1.5249e-02, -1.1425e-01, -1.3669e-01,\n",
       "                        8.0086e-02,  2.3806e-02, -1.0923e-01, -3.6372e-02,  4.4825e-02,\n",
       "                       -1.2514e-01, -4.0168e-02,  2.1245e-02,  7.4243e-02, -3.3226e-02,\n",
       "                       -9.0933e-02, -4.1999e-02, -5.3173e-02,  2.9491e-02,  1.0394e-01,\n",
       "                        9.9183e-03, -6.2770e-02, -8.0875e-02,  7.9299e-02, -2.5652e-02,\n",
       "                        7.6966e-02,  9.9305e-02, -1.3700e-01,  3.9757e-02,  1.7518e-02,\n",
       "                       -8.6325e-02,  3.0932e-03, -9.6067e-04, -3.8483e-02, -4.6317e-02,\n",
       "                       -6.3893e-02, -7.0697e-02, -5.4372e-02,  5.8489e-03, -8.6614e-02,\n",
       "                       -1.3249e-01,  2.4405e-02,  9.5401e-03,  8.9747e-02,  2.4484e-02,\n",
       "                       -3.5514e-02,  8.9758e-02, -4.6371e-03, -3.4918e-02,  9.0549e-02,\n",
       "                        8.9768e-02,  6.1826e-02, -2.5375e-02, -9.3550e-02, -4.3371e-02,\n",
       "                        1.0677e-01, -1.2137e-01, -1.1893e-01,  7.4266e-02, -1.2872e-01,\n",
       "                       -1.0803e-01, -2.8782e-02, -6.8401e-03, -8.8886e-03, -8.9486e-02,\n",
       "                       -1.0926e-01,  1.0304e-02, -1.1603e-01, -6.3233e-02, -5.3809e-02,\n",
       "                        3.1465e-02, -5.3913e-02, -1.0402e-01,  1.0095e-01, -7.6241e-02,\n",
       "                       -7.7328e-02,  7.9152e-02, -8.2377e-03, -4.9261e-02,  1.0795e-01,\n",
       "                        4.1153e-02, -1.2556e-01,  5.3232e-03,  4.9598e-02, -2.8915e-02,\n",
       "                        3.9472e-03, -1.0770e-01,  6.4881e-02, -1.3172e-01, -2.5251e-04,\n",
       "                        7.9284e-02, -1.0280e-01,  6.1843e-02,  3.4208e-02, -8.8359e-02,\n",
       "                       -5.0062e-02,  8.2429e-02, -5.5132e-03,  9.9135e-02, -1.1501e-01,\n",
       "                        9.5477e-02, -1.3451e-01, -5.0865e-02,  1.2399e-02,  9.5578e-03,\n",
       "                        5.0161e-02,  1.3957e-02, -9.5572e-03,  7.9260e-02, -1.0800e-02,\n",
       "                       -2.7292e-02,  1.0532e-01, -1.0325e-01,  4.6399e-02, -4.0348e-02,\n",
       "                       -7.2547e-02, -1.1359e-01, -1.0073e-01,  9.8158e-02,  1.0411e-02,\n",
       "                        8.1827e-02, -1.1242e-01, -4.9779e-02, -1.1722e-01,  9.7013e-02,\n",
       "                        9.9557e-02,  9.0229e-02, -1.0874e-02,  9.2893e-02,  2.3735e-02,\n",
       "                       -1.1803e-01,  9.2625e-02, -1.3373e-01,  4.2819e-02,  3.6685e-02,\n",
       "                        7.8200e-02,  7.1406e-02, -1.1303e-01,  6.1544e-02, -2.8545e-02,\n",
       "                        6.3425e-03, -1.0204e-01,  9.7224e-02,  5.6583e-03,  1.1115e-01,\n",
       "                       -5.1875e-02, -9.7876e-04, -5.0971e-02,  5.3357e-02, -1.2931e-01,\n",
       "                        4.7164e-03, -1.2655e-01,  6.1011e-02, -9.0819e-02,  8.7618e-02,\n",
       "                        8.3311e-02, -1.3484e-01, -1.2105e-01, -3.7763e-03, -6.6061e-02,\n",
       "                        9.0144e-03, -3.5297e-02, -7.5236e-02,  3.8319e-02, -8.1816e-02,\n",
       "                       -9.5280e-02, -1.0466e-01, -1.4173e-01, -3.3683e-02,  5.0149e-02,\n",
       "                        1.0278e-01,  7.4411e-02,  3.3269e-02, -2.9852e-02,  6.6100e-02,\n",
       "                        5.7738e-03, -1.2734e-01,  6.6431e-02,  1.7783e-02,  1.0438e-01,\n",
       "                       -1.2793e-01, -1.2783e-01, -2.9819e-02,  2.2997e-02,  6.0901e-02,\n",
       "                        5.3728e-02, -3.6793e-02,  1.8821e-02, -8.7399e-02, -1.2653e-01,\n",
       "                        5.1245e-02, -4.0677e-02,  4.7560e-02,  9.7447e-02,  8.7514e-02,\n",
       "                        8.8767e-02, -1.2875e-01, -9.6140e-02, -4.8719e-02,  5.3174e-02,\n",
       "                       -8.7463e-02,  9.9750e-02, -1.0290e-01, -5.5709e-02, -2.5765e-02,\n",
       "                        1.1153e-01, -4.0192e-02,  3.6229e-02, -3.6733e-02,  1.6474e-02,\n",
       "                       -6.5902e-02,  7.3845e-02,  5.6075e-02,  2.2728e-02, -5.9524e-02,\n",
       "                       -3.6864e-02, -2.2380e-02, -1.0777e-01, -1.3328e-02,  6.0653e-02,\n",
       "                        1.2250e-02,  4.8517e-02, -1.5658e-01, -2.0152e-02, -1.0796e-01,\n",
       "                        4.4414e-02, -1.5796e-01, -8.2101e-03, -7.7726e-02, -7.7998e-02,\n",
       "                        6.5248e-02, -1.3670e-01,  8.6914e-02, -1.0402e-01,  5.3752e-02,\n",
       "                       -1.0270e-01, -3.0611e-02,  7.2811e-02, -7.2865e-03,  5.7772e-02,\n",
       "                       -1.5999e-03, -6.6598e-02,  2.6472e-02,  1.0052e-01, -6.0865e-02,\n",
       "                       -4.5653e-03,  6.4498e-02, -1.2392e-01, -6.6055e-02, -3.4099e-02,\n",
       "                       -9.0230e-02,  4.6001e-02,  4.5723e-02, -7.8010e-02,  1.5544e-02,\n",
       "                        1.0422e-01, -1.5200e-01,  7.8585e-02,  2.4394e-02,  1.0523e-01,\n",
       "                       -5.9334e-02,  3.2182e-02,  1.0093e-02, -1.1255e-02, -1.2671e-01,\n",
       "                        1.0018e-01,  7.2276e-02,  6.4927e-02, -1.1201e-01, -9.4654e-02,\n",
       "                       -7.5844e-02, -9.1299e-02, -4.8019e-02, -9.6616e-02, -1.2605e-01,\n",
       "                       -4.1883e-03, -7.2641e-02,  7.9996e-02,  9.6118e-02, -7.5296e-02,\n",
       "                        6.1701e-02, -8.7600e-02, -8.1359e-02, -8.3240e-02,  5.6965e-02,\n",
       "                        5.0844e-02, -1.0869e-01,  4.4443e-02, -9.8640e-02, -1.2468e-01,\n",
       "                       -6.7430e-02, -1.3370e-01, -8.3085e-02, -1.0745e-01,  5.2471e-02,\n",
       "                        2.4195e-02,  1.8404e-02, -3.6062e-02, -8.7037e-02,  3.7310e-02,\n",
       "                       -4.4964e-02,  5.7337e-02,  7.0613e-03,  7.1850e-02, -6.0303e-02,\n",
       "                        8.8094e-02,  7.7831e-02, -7.8938e-02, -1.1722e-01, -1.2108e-01,\n",
       "                       -9.2130e-02, -5.3982e-02,  6.4500e-02, -1.0983e-01, -4.2117e-02,\n",
       "                        2.7992e-02,  4.2620e-03,  6.2316e-02,  2.2154e-03,  9.8753e-02,\n",
       "                        2.9234e-02,  1.3905e-02,  3.5183e-02, -6.0065e-02, -6.8822e-02,\n",
       "                        3.1811e-03,  3.0517e-02, -1.5461e-02, -3.9189e-02, -6.5997e-02,\n",
       "                       -5.6084e-02, -4.2252e-02,  1.0599e-01, -1.3818e-01, -1.0478e-01,\n",
       "                        6.1175e-02,  1.1546e-01,  1.0393e-01, -4.0865e-02,  5.3413e-02,\n",
       "                       -1.1219e-01, -1.0425e-01,  7.5803e-02,  9.2989e-02, -3.6384e-02,\n",
       "                       -1.0631e-01, -6.9975e-02,  4.9467e-02, -6.9682e-02,  9.9031e-03,\n",
       "                        3.9240e-02,  6.9142e-02, -1.1913e-01, -1.4265e-01, -1.4351e-01,\n",
       "                        9.7170e-02,  7.3476e-02, -8.5298e-02, -1.3782e-01,  4.4245e-02,\n",
       "                       -3.1829e-02, -5.7331e-02,  2.6116e-02, -1.4335e-01, -1.3723e-01,\n",
       "                        3.6558e-02, -5.0567e-02, -4.6491e-02,  1.1537e-01, -1.2793e-01,\n",
       "                       -4.3254e-02,  8.8646e-02, -8.7621e-02, -8.0367e-02,  4.2365e-02,\n",
       "                       -9.8835e-02,  4.7471e-02,  3.8610e-02,  5.9726e-02, -2.3794e-02,\n",
       "                        5.6520e-02,  1.9639e-02,  2.2588e-02, -2.3321e-02, -1.0336e-01,\n",
       "                       -8.0493e-02, -1.0112e-01, -1.7830e-02, -6.0377e-02,  5.7275e-03,\n",
       "                        8.4530e-02, -1.3044e-01, -1.5700e-02, -5.2379e-02, -8.1428e-02,\n",
       "                       -9.8491e-03, -1.2616e-01,  4.1145e-03,  5.9427e-02,  4.9884e-03,\n",
       "                       -1.2050e-01, -1.8301e-02, -1.0667e-01, -4.1797e-02,  4.8863e-02,\n",
       "                       -4.3057e-02, -1.3893e-01, -7.8274e-02,  7.7752e-02, -5.1753e-03,\n",
       "                        8.4145e-02,  8.9857e-02,  2.7439e-02, -3.0701e-02,  2.0643e-02,\n",
       "                        4.8633e-02,  6.7890e-02,  8.8071e-02, -1.3016e-01, -9.5080e-02,\n",
       "                        5.6137e-02,  1.2451e-02, -2.7605e-02, -1.0807e-01, -5.5369e-02,\n",
       "                       -1.2949e-01,  7.4789e-02,  7.9809e-02,  8.9308e-02, -5.1144e-02,\n",
       "                       -7.4655e-02, -8.5080e-02, -5.6515e-02, -7.5739e-02,  5.7073e-02,\n",
       "                        2.1922e-02, -1.0845e-01, -1.2260e-01,  7.9892e-04, -7.3698e-02,\n",
       "                        2.0009e-03,  4.3684e-02,  4.8778e-02, -3.4280e-02, -9.0988e-02,\n",
       "                        1.2913e-01,  3.5722e-02,  6.3907e-02,  2.8169e-02, -7.1671e-02,\n",
       "                        2.7912e-02, -3.3465e-02, -7.5969e-02,  1.5836e-02,  1.1893e-01,\n",
       "                        2.9846e-02,  2.2644e-03,  4.8088e-02, -9.7573e-02, -1.2218e-01,\n",
       "                        9.6239e-02,  4.7747e-02,  8.5365e-02, -1.5053e-02, -5.0392e-03,\n",
       "                        1.4000e-02, -2.9775e-02, -4.8996e-02, -1.0650e-01,  7.9219e-02,\n",
       "                       -3.7638e-03, -2.4939e-02,  6.4296e-02,  3.1782e-02, -7.5279e-02,\n",
       "                        3.3402e-02, -1.3067e-01, -4.6911e-02, -5.7859e-02, -6.5498e-05,\n",
       "                       -1.8109e-02, -8.3185e-02,  5.5889e-02, -8.4462e-02, -1.2542e-02,\n",
       "                       -1.0040e-01,  8.0014e-02, -5.6245e-02, -1.4373e-02,  1.4233e-02,\n",
       "                       -1.9786e-02, -7.1531e-02, -5.0807e-02, -8.4514e-02, -8.1073e-02,\n",
       "                       -8.2052e-03,  2.9516e-03,  2.4007e-02, -4.6792e-02, -9.8473e-02,\n",
       "                       -5.1905e-02, -4.1241e-02,  1.2572e-02, -7.8165e-02, -9.6769e-02,\n",
       "                        4.3760e-02, -4.0426e-02,  8.7715e-02,  7.3010e-02, -1.0323e-01,\n",
       "                       -5.8919e-02,  6.8023e-03,  1.1414e-01,  8.7750e-02, -1.5017e-01,\n",
       "                        5.4852e-02,  1.2168e-01, -4.6578e-02, -1.0278e-01,  4.5971e-03,\n",
       "                        3.4493e-02,  8.3267e-02,  4.2862e-02,  1.1148e-01, -4.3559e-02,\n",
       "                       -5.4374e-02, -1.0644e-01,  3.4168e-03,  3.1636e-03,  1.3163e-01,\n",
       "                        6.5910e-02,  9.5106e-02,  7.4143e-02,  7.8869e-02, -1.3076e-01,\n",
       "                        8.9075e-02, -1.3835e-01, -1.0116e-01, -1.1205e-01,  6.5325e-02,\n",
       "                       -1.5124e-01,  4.9761e-03, -1.1982e-01, -1.1303e-01,  1.6892e-02,\n",
       "                       -3.2668e-02,  1.1800e-01, -1.1168e-01,  5.6410e-02,  7.0243e-02,\n",
       "                       -7.9547e-02, -4.0423e-02, -2.7654e-02, -5.8484e-02,  2.7309e-02,\n",
       "                       -4.3083e-03,  2.3490e-02,  1.7228e-02, -1.8626e-02, -9.6661e-02,\n",
       "                       -9.3752e-02, -1.0160e-02,  4.5084e-02, -2.6548e-02,  6.4399e-02,\n",
       "                        5.4618e-02, -1.1511e-01, -2.2526e-02, -1.3532e-01, -1.0534e-01,\n",
       "                        9.6836e-02, -2.2449e-02, -3.9217e-03,  4.0680e-02,  7.9053e-02,\n",
       "                       -1.8381e-02,  2.0061e-02,  9.9854e-04,  9.5956e-02,  7.5757e-02,\n",
       "                       -7.8270e-02, -1.5212e-02, -1.4213e-02, -1.3137e-01,  5.3809e-02,\n",
       "                       -1.0905e-01, -6.1404e-02]))]),\n",
       " 'loops': {'fit_loop': {'state_dict': {},\n",
       "   'epoch_loop.state_dict': {'_batches_that_stepped': 11},\n",
       "   'epoch_loop.batch_progress': {'total': {'ready': 11,\n",
       "     'completed': 11,\n",
       "     'started': 11,\n",
       "     'processed': 11},\n",
       "    'current': {'ready': 11, 'completed': 11, 'started': 11, 'processed': 11},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_loop.scheduler_progress': {'total': {'ready': 0, 'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.automatic_optimization.state_dict': {},\n",
       "   'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 11,\n",
       "       'completed': 11},\n",
       "      'current': {'ready': 11, 'completed': 11}},\n",
       "     'zero_grad': {'total': {'ready': 11, 'completed': 11, 'started': 11},\n",
       "      'current': {'ready': 11, 'completed': 11, 'started': 11}}}},\n",
       "   'epoch_loop.manual_optimization.state_dict': {},\n",
       "   'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0,\n",
       "     'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.val_loop.state_dict': {},\n",
       "   'epoch_loop.val_loop.batch_progress': {'total': {'ready': 3,\n",
       "     'completed': 3,\n",
       "     'started': 3,\n",
       "     'processed': 3},\n",
       "    'current': {'ready': 3, 'completed': 3, 'started': 3, 'processed': 3},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_progress': {'total': {'ready': 1,\n",
       "     'completed': 0,\n",
       "     'started': 1,\n",
       "     'processed': 1},\n",
       "    'current': {'ready': 1, 'completed': 0, 'started': 1, 'processed': 1}}},\n",
       "  'validate_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'test_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'predict_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}},\n",
       " 'callbacks': {\"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\": {'wait_count': 0,\n",
       "   'stopped_epoch': 0,\n",
       "   'best_score': tensor(6.4225),\n",
       "   'patience': 3},\n",
       "  \"ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': 'val_loss',\n",
       "   'best_model_score': tensor(6.4225),\n",
       "   'best_model_path': '/home/onyxia/work/codif-ape-train/src/models/torch-fastText/notebooks/lightning_logs/version_0/checkpoints/epoch=0-step=11.ckpt',\n",
       "   'current_score': tensor(6.4225),\n",
       "   'dirpath': '/home/onyxia/work/codif-ape-train/src/models/torch-fastText/notebooks/lightning_logs/version_0/checkpoints',\n",
       "   'best_k_models': {'/home/onyxia/work/codif-ape-train/src/models/torch-fastText/notebooks/lightning_logs/version_0/checkpoints/epoch=0-step=11.ckpt': tensor(6.4225)},\n",
       "   'kth_best_model_path': '/home/onyxia/work/codif-ape-train/src/models/torch-fastText/notebooks/lightning_logs/version_0/checkpoints/epoch=0-step=11.ckpt',\n",
       "   'kth_value': tensor(6.4225),\n",
       "   'last_model_path': ''}},\n",
       " 'optimizer_states': [{'state': {0: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[ 2.2338e-05, -2.3426e-04, -5.4080e-05,  ...,  5.4984e-05,\n",
       "              -2.4077e-05, -3.8664e-06],\n",
       "             [ 2.3619e-06, -4.9676e-06, -9.1367e-06,  ...,  2.2611e-06,\n",
       "              -5.5917e-06,  3.4653e-06],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "               0.0000e+00,  0.0000e+00],\n",
       "             ...,\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "               0.0000e+00,  0.0000e+00],\n",
       "             [-1.1134e-07,  3.1727e-08,  1.9715e-07,  ..., -7.5358e-08,\n",
       "               5.5239e-08, -1.3571e-07],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "               0.0000e+00,  0.0000e+00]]),\n",
       "     'exp_avg_sq': tensor([[2.7320e-11, 1.2036e-09, 1.0352e-10,  ..., 1.1960e-10, 7.4410e-11,\n",
       "              3.3887e-11],\n",
       "             [3.5885e-12, 2.4716e-12, 1.1907e-11,  ..., 3.5009e-12, 9.5038e-12,\n",
       "              3.3666e-12],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "              0.0000e+00],\n",
       "             ...,\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "              0.0000e+00],\n",
       "             [8.1847e-15, 6.6462e-16, 2.5663e-14,  ..., 3.7496e-15, 2.0147e-15,\n",
       "              1.2161e-14],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "              0.0000e+00]])},\n",
       "    1: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[ 5.5962e-06, -6.5338e-05, -4.3761e-06, -4.0499e-05, -7.0467e-05,\n",
       "              -3.4920e-05,  5.8322e-05, -5.3986e-05,  3.1063e-05,  9.5754e-05],\n",
       "             [-5.6228e-05, -8.5631e-04,  2.5687e-04, -4.0193e-04,  1.5295e-05,\n",
       "              -1.8589e-04,  7.2255e-04, -3.1430e-05,  1.2547e-04,  1.7112e-04],\n",
       "             [-1.7607e-03, -1.0856e-03,  1.0362e-03,  6.1759e-04,  6.7966e-04,\n",
       "               1.1057e-03,  1.1210e-03, -4.0084e-04,  4.1595e-04,  9.0522e-04],\n",
       "             [-9.2624e-06, -4.3336e-06,  2.6063e-06, -2.1123e-06, -8.2419e-06,\n",
       "              -1.6552e-06,  1.1582e-05,  5.3928e-06,  1.2564e-06,  1.4473e-06],\n",
       "             [ 3.2863e-07, -5.9790e-06,  2.3875e-06,  5.9604e-07, -5.9998e-07,\n",
       "              -2.8713e-06, -4.1701e-06, -1.2274e-06,  3.2713e-06, -4.3677e-06],\n",
       "             [-1.3849e-04, -1.1661e-04,  4.1847e-05,  9.2943e-05,  3.2129e-05,\n",
       "               4.8971e-05,  4.8147e-05, -4.6264e-05,  1.3657e-06,  4.0630e-05],\n",
       "             [-2.4981e-05,  5.6344e-05,  6.7941e-07,  1.1062e-05,  5.4859e-05,\n",
       "               2.3065e-05, -6.7995e-06, -4.6206e-05,  6.4509e-05,  4.3328e-05],\n",
       "             [-2.8053e-05, -2.6817e-05,  1.6409e-05, -9.2336e-06, -6.1312e-06,\n",
       "              -3.0946e-06,  1.6041e-05, -5.6873e-06, -1.9624e-05,  2.1677e-05],\n",
       "             [-2.5990e-05, -2.0638e-05,  7.1993e-06,  1.8209e-05, -1.7081e-05,\n",
       "              -3.2021e-06,  1.8695e-05,  1.9472e-05, -1.1887e-05, -5.8904e-06],\n",
       "             [ 2.2052e-06, -1.6017e-05,  7.5288e-07, -9.3965e-06, -9.8797e-06,\n",
       "               8.4431e-06,  8.0274e-06,  6.2190e-06,  4.2106e-06,  1.3287e-05],\n",
       "             [-2.2635e-06, -5.2726e-06,  1.3453e-06, -8.8171e-07, -2.7548e-06,\n",
       "              -1.1041e-06, -1.5968e-06,  3.6453e-06, -2.8654e-07,  3.3797e-06],\n",
       "             [ 4.7471e-06, -2.0863e-05, -6.3474e-06, -4.5685e-06, -2.0081e-05,\n",
       "              -1.3534e-05,  9.6088e-06, -2.2491e-05,  1.3173e-05,  2.6322e-05],\n",
       "             [-2.2987e-05, -2.1146e-04,  3.6196e-05, -3.9097e-05, -6.9272e-05,\n",
       "              -4.5193e-05,  1.0838e-04,  3.7725e-05, -2.4034e-05,  7.8473e-05],\n",
       "             [-1.8739e-04, -2.9367e-05,  1.2541e-04,  4.8332e-05, -7.4884e-05,\n",
       "               1.6010e-04,  2.1551e-04, -1.9108e-05, -9.1689e-05,  5.7520e-05],\n",
       "             [-6.3571e-06, -9.3599e-06,  1.8033e-05,  2.9790e-06, -1.4501e-05,\n",
       "              -9.5712e-06,  1.6897e-05, -2.2512e-05,  7.2492e-06,  1.2051e-05],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "               0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "             [-4.7239e-06, -1.0969e-05, -4.9611e-07,  2.7404e-06, -1.1585e-05,\n",
       "               8.4217e-06, -6.3155e-06, -1.7770e-05,  4.0278e-06,  2.6908e-05],\n",
       "             [-8.0267e-05, -3.1508e-05, -1.1458e-05,  2.3833e-05,  2.9247e-05,\n",
       "              -4.9430e-07,  4.4957e-05,  1.2933e-05, -3.4392e-05,  3.2848e-05],\n",
       "             [ 2.3716e-06,  3.9855e-06,  1.1618e-06,  1.5063e-06,  2.9188e-06,\n",
       "               3.0549e-06, -8.6730e-07,  3.5464e-06,  2.7355e-06, -5.2284e-06],\n",
       "             [-1.8229e-06, -1.6264e-05,  8.6748e-06,  4.9183e-06,  1.6720e-05,\n",
       "               1.7638e-05,  1.5124e-05,  1.3695e-05, -2.3261e-06, -1.8056e-05],\n",
       "             [-1.6650e-05, -1.7874e-05, -2.5969e-07,  1.7723e-05,  7.9909e-07,\n",
       "               2.9818e-06, -1.3339e-05,  9.6596e-06, -5.4859e-06,  1.0824e-05],\n",
       "             [-8.6803e-06, -1.5897e-05,  4.2366e-06,  8.3968e-07, -2.1151e-05,\n",
       "               5.7116e-06, -2.0784e-05,  7.1371e-06, -2.5715e-07,  9.4049e-06],\n",
       "             [-4.9221e-06, -1.7763e-04, -5.3547e-05, -1.6315e-04, -1.9006e-04,\n",
       "              -9.6233e-05,  1.6527e-04,  1.2822e-04, -1.6229e-04,  1.7470e-04]]),\n",
       "     'exp_avg_sq': tensor([[3.2437e-11, 1.6805e-10, 1.4272e-10, 6.4697e-11, 1.4919e-10, 8.8434e-11,\n",
       "              1.0872e-10, 1.9860e-10, 8.5544e-11, 2.6553e-10],\n",
       "             [2.8608e-09, 1.7706e-08, 3.5829e-09, 6.1044e-09, 8.8596e-10, 1.8795e-09,\n",
       "              1.2019e-08, 1.4103e-09, 2.6889e-09, 2.4444e-09],\n",
       "             [7.0371e-08, 2.8872e-08, 2.5404e-08, 9.7923e-09, 1.3862e-08, 2.8849e-08,\n",
       "              2.8959e-08, 9.8638e-09, 1.1205e-08, 2.1592e-08],\n",
       "             [2.1113e-11, 9.8097e-12, 3.4835e-12, 4.6506e-12, 1.0398e-11, 5.1351e-12,\n",
       "              2.0111e-11, 3.9028e-12, 6.1775e-12, 4.1602e-12],\n",
       "             [2.0261e-14, 6.7065e-12, 1.0693e-12, 6.6650e-14, 6.7533e-14, 1.5467e-12,\n",
       "              3.2624e-12, 2.8263e-13, 2.0077e-12, 3.5789e-12],\n",
       "             [6.1087e-10, 5.1657e-10, 2.6142e-10, 4.4840e-10, 2.8079e-10, 3.0140e-10,\n",
       "              2.4766e-10, 2.6061e-10, 2.1342e-10, 2.3272e-10],\n",
       "             [1.2294e-10, 1.2139e-10, 6.6734e-11, 8.4707e-11, 8.9178e-11, 4.2154e-11,\n",
       "              1.3151e-11, 9.4225e-11, 1.2867e-10, 9.0393e-11],\n",
       "             [4.0865e-11, 5.6994e-11, 2.1008e-11, 3.3288e-11, 2.1393e-11, 2.5734e-11,\n",
       "              1.9781e-11, 7.3523e-11, 4.0323e-11, 3.6503e-11],\n",
       "             [3.6371e-11, 4.3913e-11, 6.9217e-11, 3.1508e-11, 5.2419e-11, 1.9585e-11,\n",
       "              8.0307e-11, 5.0383e-11, 2.2885e-11, 7.1160e-11],\n",
       "             [5.3443e-12, 2.3515e-11, 5.8558e-12, 3.8529e-12, 5.4725e-12, 1.4177e-11,\n",
       "              8.5991e-12, 7.4953e-12, 1.9386e-12, 1.0364e-11],\n",
       "             [1.4621e-12, 7.9332e-12, 5.1648e-13, 2.2184e-13, 2.1656e-12, 3.4787e-13,\n",
       "              7.2758e-13, 3.7919e-12, 2.3430e-14, 3.2596e-12],\n",
       "             [7.1834e-12, 3.3556e-11, 3.0380e-11, 3.9920e-12, 2.9859e-11, 1.8731e-11,\n",
       "              5.2374e-12, 4.4782e-11, 1.8561e-11, 4.0762e-11],\n",
       "             [3.6355e-10, 1.2247e-09, 1.4839e-10, 2.0720e-10, 4.6638e-10, 2.9892e-10,\n",
       "              3.9058e-10, 3.1213e-10, 1.1760e-10, 2.3839e-10],\n",
       "             [1.1471e-09, 1.1914e-10, 9.1867e-10, 1.3418e-10, 1.9326e-10, 6.4100e-10,\n",
       "              1.1186e-09, 7.3352e-11, 4.2747e-10, 1.8127e-10],\n",
       "             [1.0704e-11, 2.6782e-11, 4.8109e-11, 4.0048e-12, 1.6564e-11, 2.0074e-11,\n",
       "              2.9421e-11, 3.0692e-11, 1.2322e-11, 3.3215e-11],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "              0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "             [4.1074e-11, 2.1859e-11, 1.7546e-11, 3.8892e-11, 3.5260e-11, 1.3145e-11,\n",
       "              1.1995e-11, 3.4478e-11, 1.2661e-11, 4.4917e-11],\n",
       "             [2.2076e-10, 1.1926e-10, 1.4917e-10, 1.2847e-10, 2.5828e-10, 8.2296e-11,\n",
       "              6.1854e-11, 1.0252e-10, 7.7287e-11, 7.5786e-11],\n",
       "             [5.3614e-12, 3.7485e-12, 6.3701e-12, 3.2649e-12, 2.0349e-12, 4.5053e-12,\n",
       "              2.7977e-12, 3.4771e-12, 3.3333e-12, 6.2178e-12],\n",
       "             [2.2560e-11, 6.1884e-11, 1.5333e-11, 1.8968e-11, 3.0171e-11, 2.0064e-11,\n",
       "              2.4415e-11, 1.2880e-11, 3.3135e-11, 3.3541e-11],\n",
       "             [1.6934e-11, 5.7946e-11, 1.6081e-11, 3.3381e-11, 1.5747e-11, 1.3076e-11,\n",
       "              1.7417e-11, 1.0234e-11, 2.5970e-12, 2.8496e-11],\n",
       "             [2.3018e-11, 3.8240e-11, 5.0725e-11, 1.7541e-11, 2.3975e-11, 2.7305e-11,\n",
       "              4.2293e-11, 1.8956e-11, 4.2624e-11, 2.7826e-11],\n",
       "             [1.7740e-09, 1.8682e-09, 1.2243e-09, 3.4586e-09, 1.9080e-09, 8.4095e-10,\n",
       "              1.0717e-09, 1.6717e-09, 2.2624e-09, 1.6663e-09]])},\n",
       "    2: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[ 3.1488e-06, -5.9072e-05, -6.5820e-06, -3.6477e-05, -6.9512e-05,\n",
       "              -3.3320e-05,  5.4783e-05, -5.3738e-05,  2.8489e-05,  9.4979e-05],\n",
       "             [ 2.4474e-06, -6.2660e-06,  2.2058e-06, -4.0224e-06, -9.5499e-07,\n",
       "              -1.5994e-06,  3.5385e-06, -2.4745e-07,  2.5740e-06,  7.7487e-07],\n",
       "             [-3.6231e-06, -5.2142e-06,  2.1748e-07, -1.8220e-06, -3.1283e-06,\n",
       "               1.7807e-06, -5.7222e-06, -2.9659e-06,  9.8836e-07,  7.2467e-07],\n",
       "             [-3.3947e-05,  5.7924e-05, -3.2527e-06,  8.1220e-06,  4.7626e-05,\n",
       "               1.7440e-05, -1.0387e-05, -5.2970e-05,  6.2810e-05,  3.9518e-05],\n",
       "             [ 8.9656e-06, -1.5792e-06,  3.9321e-06,  2.9401e-06,  7.2338e-06,\n",
       "               5.6249e-06,  3.5873e-06,  6.7634e-06,  1.6991e-06,  3.8094e-06],\n",
       "             [-2.0268e-06, -9.6759e-06, -1.9978e-05, -8.8651e-06,  1.5232e-05,\n",
       "              -1.3226e-05,  2.6287e-05,  1.5216e-05,  3.5218e-06, -1.6912e-05],\n",
       "             [-2.8151e-06,  3.4105e-06, -4.0766e-06,  1.2783e-06, -9.9465e-07,\n",
       "              -2.2833e-06,  5.4471e-06, -3.6836e-06,  3.9656e-06,  6.9416e-08],\n",
       "             [ 1.3573e-05,  1.9978e-07,  3.5699e-06, -1.4146e-05,  1.4248e-05,\n",
       "              -1.5003e-06,  6.1583e-06,  9.2702e-06, -1.4469e-05, -6.9934e-06],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "               0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "             [ 7.3600e-06,  2.7708e-06, -1.2756e-06, -1.0110e-05,  8.6882e-06,\n",
       "              -8.6700e-06,  1.2966e-06,  1.1467e-06, -3.2737e-06, -4.0262e-06],\n",
       "             [-1.2311e-05, -1.6226e-06,  8.8397e-06, -1.6820e-05, -9.1565e-06,\n",
       "               1.4884e-05,  6.9720e-06, -1.6756e-05, -2.7758e-05,  1.3759e-05],\n",
       "             [-7.6581e-05, -1.0613e-04,  5.7156e-05, -2.8524e-05, -5.5260e-05,\n",
       "               2.2535e-05,  8.8713e-05, -4.3434e-05,  6.1560e-06,  8.4521e-05],\n",
       "             [ 1.2720e-06,  6.0849e-06,  6.3939e-06, -1.0201e-05,  2.3002e-06,\n",
       "              -3.8465e-06,  8.8216e-06,  1.0091e-06, -9.6778e-06,  7.4560e-08],\n",
       "             [-4.6399e-04, -3.0168e-04,  5.8089e-05,  2.4583e-04,  2.8919e-05,\n",
       "              -3.4383e-05,  2.3764e-04,  4.5794e-05, -1.6924e-04,  1.0610e-04],\n",
       "             [ 4.6674e-06, -7.4792e-06,  2.0641e-06,  1.9740e-06,  6.1532e-06,\n",
       "              -1.0322e-06,  7.5994e-08, -2.1149e-07, -5.6344e-06,  9.4620e-06],\n",
       "             [-1.0152e-05, -1.3343e-05, -2.6309e-06, -2.2717e-06, -8.7329e-06,\n",
       "              -3.3687e-06, -1.2524e-05, -1.1812e-05, -8.1723e-07,  8.5208e-06],\n",
       "             [ 6.6210e-07, -9.3027e-07,  7.2137e-07,  1.0420e-06,  2.6173e-06,\n",
       "              -5.4441e-06,  4.7701e-06,  5.6400e-06,  3.1080e-06, -2.4478e-06],\n",
       "             [ 4.1291e-06,  1.0574e-05, -7.5713e-06, -1.8502e-05, -1.6244e-05,\n",
       "              -4.8813e-06, -4.4107e-06, -7.4479e-07,  2.0056e-06,  1.4018e-06],\n",
       "             [ 4.6760e-06, -8.9365e-06,  2.7798e-06, -7.1238e-06, -9.2669e-07,\n",
       "              -3.2464e-06,  4.9883e-06, -2.4806e-07,  3.4011e-06,  8.7379e-07],\n",
       "             [ 2.9231e-04, -7.2043e-04,  2.6312e-04, -5.1563e-04, -1.1490e-04,\n",
       "              -1.9790e-04,  4.3978e-04, -5.1106e-05,  2.8869e-04,  6.3874e-05],\n",
       "             [-1.3358e-06, -2.1950e-06,  3.8590e-06,  4.9929e-06, -5.7749e-06,\n",
       "              -1.9226e-07, -9.1074e-07, -1.4905e-06,  3.1340e-07,  4.3344e-06],\n",
       "             [-4.5501e-06,  1.0830e-05,  2.3734e-06, -2.5791e-06, -2.7528e-06,\n",
       "              -3.3686e-06,  1.0468e-05,  8.4987e-06,  2.9061e-06,  9.0844e-06],\n",
       "             [-3.3927e-06,  1.6043e-07, -3.0835e-06, -1.1171e-06, -4.3341e-06,\n",
       "              -4.9924e-06,  5.0223e-06,  5.2373e-06, -5.6240e-06,  6.3355e-06],\n",
       "             [ 5.0086e-05, -2.2560e-05,  1.0822e-05, -6.9497e-05,  3.5035e-05,\n",
       "              -4.8279e-05,  3.1439e-05,  1.5649e-05, -7.6184e-06, -5.5100e-06],\n",
       "             [ 3.2025e-05, -1.4357e-05, -3.6489e-05,  3.0496e-05, -1.1585e-05,\n",
       "               6.4172e-05,  2.0890e-05,  2.8226e-05,  1.5974e-05,  4.8058e-05],\n",
       "             [ 2.4060e-06, -5.2279e-07, -2.6998e-06,  2.6993e-06, -4.4111e-07,\n",
       "               3.5588e-06,  6.8630e-07,  3.7989e-06, -1.3466e-08,  2.3740e-06],\n",
       "             [-2.1776e-03, -1.4937e-03,  1.1450e-03,  6.2074e-04,  4.5157e-04,\n",
       "               1.2279e-03,  1.5988e-03, -3.2672e-04,  1.3953e-04,  1.2286e-03]]),\n",
       "     'exp_avg_sq': tensor([[3.2645e-11, 1.5234e-10, 1.3972e-10, 5.6698e-11, 1.4659e-10, 8.7406e-11,\n",
       "              9.7782e-11, 1.9832e-10, 8.2636e-11, 2.6280e-10],\n",
       "             [1.1237e-12, 7.3659e-12, 9.1281e-13, 3.0353e-12, 1.7110e-13, 4.7993e-13,\n",
       "              2.3490e-12, 1.1487e-14, 1.2430e-12, 1.1264e-13],\n",
       "             [4.4766e-12, 6.8734e-12, 2.4131e-12, 1.7352e-12, 1.8917e-12, 7.7224e-12,\n",
       "              3.9499e-12, 1.1451e-12, 3.4950e-12, 1.0979e-11],\n",
       "             [1.3452e-10, 1.2270e-10, 6.0938e-11, 7.9732e-11, 7.6740e-11, 4.0973e-11,\n",
       "              1.3507e-11, 9.6755e-11, 1.2596e-10, 8.2173e-11],\n",
       "             [8.0382e-12, 2.4939e-13, 1.5462e-12, 8.6442e-13, 5.2328e-12, 3.1639e-12,\n",
       "              1.2869e-12, 4.5744e-12, 2.8869e-13, 1.4511e-12],\n",
       "             [4.4392e-11, 4.2940e-11, 3.8617e-11, 4.7747e-11, 3.4148e-11, 6.9545e-11,\n",
       "              3.0515e-11, 4.2339e-11, 3.2638e-11, 3.1942e-11],\n",
       "             [1.8336e-12, 2.6913e-12, 3.8452e-12, 3.7806e-13, 2.2891e-13, 1.2063e-12,\n",
       "              6.8652e-12, 3.1395e-12, 3.6387e-12, 1.1149e-15],\n",
       "             [3.0066e-11, 2.0763e-11, 9.5619e-12, 2.1431e-11, 1.2147e-11, 1.2883e-11,\n",
       "              1.9969e-11, 1.3584e-11, 3.3624e-11, 9.9598e-12],\n",
       "             [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "              0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "             [5.2935e-12, 9.7494e-13, 3.6974e-13, 1.0631e-11, 7.1260e-12, 7.4996e-12,\n",
       "              1.7518e-13, 2.5105e-13, 1.3418e-12, 1.9140e-12],\n",
       "             [1.3624e-11, 1.5850e-11, 7.5698e-12, 2.7913e-11, 9.9031e-12, 2.0030e-11,\n",
       "              4.3864e-12, 2.9297e-11, 7.1538e-11, 1.7164e-11],\n",
       "             [5.1850e-10, 6.9745e-10, 3.7217e-10, 4.6824e-10, 4.5583e-10, 3.7373e-10,\n",
       "              6.1395e-10, 5.4538e-10, 2.1787e-10, 9.5542e-10],\n",
       "             [1.6180e-13, 3.7026e-12, 4.0882e-12, 1.0406e-11, 5.2907e-13, 1.4796e-12,\n",
       "              7.7820e-12, 1.0184e-13, 9.3659e-12, 5.5592e-16],\n",
       "             [6.8196e-09, 3.0783e-09, 8.7660e-10, 1.7914e-09, 6.7088e-10, 6.3719e-10,\n",
       "              1.4048e-09, 1.0604e-09, 1.9547e-09, 1.8050e-09],\n",
       "             [3.2615e-12, 9.7131e-12, 1.2268e-11, 9.1510e-12, 6.2795e-12, 2.2184e-13,\n",
       "              5.1395e-12, 7.4426e-12, 6.8334e-12, 1.2378e-11],\n",
       "             [6.6986e-12, 1.4501e-11, 9.1742e-12, 6.7971e-12, 6.9610e-12, 2.4303e-12,\n",
       "              9.2320e-12, 1.9255e-11, 1.1315e-11, 1.1890e-11],\n",
       "             [5.9514e-12, 6.8838e-12, 6.7955e-12, 4.6152e-12, 3.3503e-12, 3.7747e-12,\n",
       "              6.1616e-12, 4.4779e-12, 5.3280e-12, 8.6369e-13],\n",
       "             [4.9011e-12, 1.1277e-11, 1.3345e-11, 3.1112e-11, 2.2779e-11, 4.1136e-12,\n",
       "              9.2239e-12, 1.0396e-11, 7.6599e-12, 8.4899e-12],\n",
       "             [3.7884e-12, 1.1371e-11, 1.0896e-12, 8.1580e-12, 1.2889e-13, 1.9114e-12,\n",
       "              3.4648e-12, 2.9512e-14, 1.6070e-12, 1.7041e-13],\n",
       "             [2.4322e-09, 1.1919e-08, 1.5575e-09, 6.5992e-09, 3.0815e-10, 1.1526e-09,\n",
       "              4.3396e-09, 1.0484e-10, 1.9196e-09, 1.5407e-10],\n",
       "             [3.3475e-13, 9.0392e-13, 2.7938e-12, 4.6767e-12, 6.2564e-12, 6.9345e-15,\n",
       "              1.5561e-13, 4.1678e-13, 1.8427e-14, 3.5245e-12],\n",
       "             [1.3999e-11, 2.3777e-11, 2.4296e-11, 1.0433e-11, 2.6895e-11, 9.6162e-12,\n",
       "              1.5217e-11, 2.7275e-11, 4.3867e-12, 1.2657e-11],\n",
       "             [1.7509e-12, 3.9150e-15, 1.4463e-12, 1.8983e-13, 2.8574e-12, 3.7912e-12,\n",
       "              3.8368e-12, 4.1724e-12, 4.8112e-12, 6.1055e-12],\n",
       "             [7.1949e-11, 6.1506e-11, 1.6913e-11, 2.3916e-10, 3.7394e-11, 7.1477e-11,\n",
       "              4.2088e-11, 2.7702e-11, 1.4508e-11, 4.3541e-11],\n",
       "             [8.3053e-11, 3.0381e-11, 5.6092e-11, 3.6579e-11, 2.4241e-11, 1.8314e-10,\n",
       "              3.4650e-11, 3.7439e-11, 4.2464e-11, 7.9289e-11],\n",
       "             [2.5128e-12, 1.1864e-13, 3.1640e-12, 3.1627e-12, 8.4461e-14, 5.4976e-12,\n",
       "              2.0445e-13, 6.2642e-12, 7.8715e-17, 2.4464e-12],\n",
       "             [1.0716e-07, 5.2065e-08, 3.2175e-08, 1.6521e-08, 9.8484e-09, 3.7348e-08,\n",
       "              5.7845e-08, 1.2701e-08, 1.4539e-08, 3.8058e-08]])},\n",
       "    3: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[-8.8004e-05,  4.9668e-05, -1.8736e-05,  5.7898e-06,  4.4699e-06,\n",
       "              -1.1357e-05,  4.0956e-05, -1.1731e-04, -3.4468e-05,  1.5070e-05],\n",
       "             [ 5.4587e-06,  9.1146e-06,  2.6991e-06,  5.6820e-06, -2.1956e-05,\n",
       "              -9.1949e-06, -1.4320e-05,  2.2394e-05, -1.4041e-05,  7.0438e-06],\n",
       "             [ 7.4304e-06,  2.0605e-05, -2.2678e-05,  2.0183e-05, -1.7458e-05,\n",
       "              -2.2985e-05,  9.1208e-05,  2.1744e-05, -6.9436e-05,  7.2429e-05],\n",
       "             [ 5.6650e-05, -7.6661e-05, -3.1090e-05,  1.6767e-04, -3.1655e-06,\n",
       "               1.3062e-04, -4.0783e-05, -1.5397e-05,  6.8566e-05,  1.1227e-04],\n",
       "             [-9.5174e-05,  5.7991e-05, -7.5043e-05,  4.9600e-05, -1.6543e-05,\n",
       "              -6.0221e-05,  4.8531e-05,  5.2355e-05,  2.7717e-05, -5.3238e-05],\n",
       "             [-5.8817e-05,  1.3937e-05,  1.0924e-04, -3.1889e-05, -4.3041e-05,\n",
       "               5.2901e-05,  8.9095e-05,  4.4491e-05, -2.4853e-05,  4.8744e-05],\n",
       "             [-1.0760e-03, -1.2125e-03,  4.3961e-04,  6.7201e-04,  8.3518e-04,\n",
       "               1.9749e-04,  3.7825e-04, -4.2435e-04,  4.2972e-04,  6.4411e-04],\n",
       "             [-1.1161e-03, -1.5459e-03,  1.0795e-03, -7.1665e-04, -4.2256e-04,\n",
       "               7.0910e-04,  1.9333e-03, -3.8081e-06, -6.1197e-05,  8.4491e-04]]),\n",
       "     'exp_avg_sq': tensor([[3.2770e-10, 1.3272e-10, 1.0724e-10, 7.8649e-11, 2.5951e-10, 4.0128e-10,\n",
       "              1.0234e-10, 5.1412e-10, 8.2098e-11, 1.6579e-10],\n",
       "             [1.3954e-10, 5.8941e-11, 1.1484e-10, 1.1597e-10, 3.1355e-11, 3.4647e-11,\n",
       "              3.9155e-11, 6.2062e-11, 9.4346e-11, 2.2105e-10],\n",
       "             [2.4248e-10, 1.2593e-10, 1.4523e-10, 1.3552e-10, 8.7770e-11, 6.6886e-11,\n",
       "              3.4470e-10, 4.9931e-11, 2.8903e-10, 1.2939e-10],\n",
       "             [1.9818e-10, 1.9573e-10, 4.0458e-10, 9.5600e-10, 3.3452e-10, 6.2411e-10,\n",
       "              1.6454e-10, 2.0047e-10, 2.3855e-10, 5.9043e-10],\n",
       "             [5.5808e-10, 4.2096e-10, 5.9197e-10, 2.1244e-10, 7.0232e-11, 3.3629e-10,\n",
       "              2.6313e-10, 2.2020e-10, 5.4888e-11, 2.1683e-10],\n",
       "             [9.7283e-11, 2.3046e-11, 3.2834e-10, 3.7362e-11, 4.8603e-11, 8.3152e-11,\n",
       "              2.2185e-10, 6.5259e-11, 1.7895e-11, 6.2806e-11],\n",
       "             [2.8177e-08, 3.4651e-08, 8.0668e-09, 1.1113e-08, 1.8347e-08, 1.9831e-09,\n",
       "              4.2912e-09, 7.5051e-09, 7.0755e-09, 1.3955e-08],\n",
       "             [3.1009e-08, 5.5492e-08, 2.8272e-08, 1.9850e-08, 8.8397e-09, 1.4882e-08,\n",
       "              8.4297e-08, 7.5619e-09, 1.2084e-08, 2.0517e-08]])},\n",
       "    4: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[ 1.5286e-05, -7.8742e-06,  2.0294e-05, -2.8432e-05, -2.2735e-05,\n",
       "               3.6112e-05,  2.7368e-05,  4.3363e-05, -2.8184e-05,  1.4562e-05],\n",
       "             [-1.1184e-03, -1.0003e-03,  6.1337e-04,  6.0899e-04,  5.1081e-04,\n",
       "              -3.6286e-05,  6.7445e-04, -2.5583e-04,  3.7711e-04,  8.4307e-04],\n",
       "             [ 8.9656e-06, -1.5792e-06,  3.9321e-06,  2.9401e-06,  7.2338e-06,\n",
       "               5.6249e-06,  3.5873e-06,  6.7634e-06,  1.6991e-06,  3.8094e-06],\n",
       "             [ 3.1019e-04, -7.0793e-04,  2.6408e-04, -6.0521e-04, -1.1830e-04,\n",
       "              -2.3801e-04,  4.7331e-04, -3.9384e-05,  2.3964e-04,  9.6646e-05],\n",
       "             [-3.3088e-04,  1.1284e-05,  4.8950e-04, -2.5568e-04, -3.5352e-04,\n",
       "               1.5528e-04,  4.4132e-04,  9.2233e-05, -1.0760e-04,  4.7484e-04],\n",
       "             [-1.7159e-04, -9.3730e-05,  2.3461e-05,  1.2592e-04, -1.2319e-04,\n",
       "               1.5771e-04,  1.4329e-04, -1.1030e-04, -1.2031e-04,  2.5512e-05],\n",
       "             [-2.6935e-04, -2.3891e-04, -8.1607e-05,  1.2472e-04,  2.3815e-04,\n",
       "               3.2154e-04,  3.7566e-05, -2.5573e-04,  1.8760e-04,  1.9415e-04],\n",
       "             [-4.9221e-06, -1.7763e-04, -5.3547e-05, -1.6315e-04, -1.9006e-04,\n",
       "              -9.6233e-05,  1.6527e-04,  1.2822e-04, -1.6229e-04,  1.7470e-04],\n",
       "             [ 8.2587e-05,  2.2859e-05,  1.5611e-04,  8.4474e-05,  6.7807e-05,\n",
       "               1.5061e-05,  2.6875e-05,  6.7522e-05, -4.3415e-05, -1.2575e-04],\n",
       "             [ 3.3957e-05, -1.6196e-05, -3.6634e-05,  3.6496e-05, -1.6475e-05,\n",
       "               6.7277e-05,  2.0570e-05,  3.1389e-05,  1.5684e-05,  5.3492e-05],\n",
       "             [-9.2040e-04, -4.7377e-04,  8.4576e-05,  2.4133e-04,  3.1522e-04,\n",
       "               5.9829e-04,  5.1262e-04, -1.2813e-04, -3.7923e-05, -6.3684e-05]]),\n",
       "     'exp_avg_sq': tensor([[6.0577e-11, 6.1753e-11, 9.2368e-11, 4.4630e-11, 1.4669e-10, 4.6727e-11,\n",
       "              6.0971e-11, 1.4839e-10, 9.3425e-11, 6.9835e-11],\n",
       "             [3.0442e-08, 2.4105e-08, 1.0732e-08, 9.6801e-09, 7.1524e-09, 1.2680e-09,\n",
       "              1.0963e-08, 4.3724e-09, 8.4115e-09, 2.1048e-08],\n",
       "             [8.0382e-12, 2.4939e-13, 1.5462e-12, 8.6442e-13, 5.2328e-12, 3.1639e-12,\n",
       "              1.2869e-12, 4.5744e-12, 2.8869e-13, 1.4511e-12],\n",
       "             [2.6652e-09, 1.1932e-08, 1.6157e-09, 9.1996e-09, 3.7625e-10, 1.5296e-09,\n",
       "              5.0715e-09, 2.0596e-10, 1.4243e-09, 3.9796e-10],\n",
       "             [2.5984e-09, 1.5185e-10, 5.7428e-09, 2.4767e-09, 3.7973e-09, 6.6743e-10,\n",
       "              4.6643e-09, 5.2172e-10, 8.8686e-10, 5.3008e-09],\n",
       "             [7.5007e-10, 4.3343e-10, 5.8793e-10, 6.1465e-10, 6.5546e-10, 6.0617e-10,\n",
       "              5.9994e-10, 4.4345e-10, 6.4026e-10, 2.0735e-10],\n",
       "             [2.1929e-09, 1.9265e-09, 1.7123e-09, 4.4313e-10, 2.2271e-09, 2.5649e-09,\n",
       "              1.2541e-09, 2.8370e-09, 1.7717e-09, 3.3082e-09],\n",
       "             [1.7740e-09, 1.8682e-09, 1.2243e-09, 3.4586e-09, 1.9080e-09, 8.4095e-10,\n",
       "              1.0717e-09, 1.6717e-09, 2.2624e-09, 1.6663e-09],\n",
       "             [4.1546e-10, 6.1186e-11, 7.6249e-10, 3.6782e-10, 1.9293e-10, 3.7879e-11,\n",
       "              1.5306e-10, 2.6289e-10, 1.5721e-10, 5.4055e-10],\n",
       "             [8.9858e-11, 3.2069e-11, 6.2775e-11, 5.6366e-11, 3.3246e-11, 1.9855e-10,\n",
       "              3.5111e-11, 5.4627e-11, 4.2464e-11, 9.5712e-11],\n",
       "             [1.9963e-08, 6.0372e-09, 1.9485e-09, 1.7846e-09, 2.3954e-09, 8.7766e-09,\n",
       "              7.7286e-09, 9.7392e-10, 1.6941e-09, 1.3935e-09]])},\n",
       "    5: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[ 4.0023e-04, -8.6571e-04,  3.4102e-04, -6.8960e-04, -3.1572e-04,\n",
       "              -3.0687e-04,  7.0299e-04,  1.3225e-04,  5.8038e-05,  2.4582e-04],\n",
       "             [-2.7292e-03, -1.8168e-03,  1.1201e-03,  8.5198e-04,  6.4748e-04,\n",
       "               1.2496e-03,  1.7747e-03, -5.8550e-04,  2.7998e-04,  1.4280e-03],\n",
       "             [-3.5621e-05, -1.2695e-06,  2.2404e-05,  1.0012e-05, -1.6835e-05,\n",
       "               4.3661e-05,  4.8579e-05,  3.3366e-05, -1.6008e-05,  1.7476e-05]]),\n",
       "     'exp_avg_sq': tensor([[7.0912e-09, 1.6607e-08, 3.1023e-09, 1.2832e-08, 3.4837e-09, 3.6028e-09,\n",
       "              1.1215e-08, 2.9930e-09, 3.5504e-09, 2.6685e-09],\n",
       "             [1.6714e-07, 7.8191e-08, 3.7020e-08, 2.0863e-08, 1.4535e-08, 3.8451e-08,\n",
       "              7.3910e-08, 1.8724e-08, 1.7497e-08, 5.6886e-08],\n",
       "             [5.9087e-11, 8.2619e-11, 8.9237e-11, 3.7479e-11, 5.9081e-11, 1.1303e-10,\n",
       "              1.1978e-10, 9.0428e-11, 3.5719e-11, 3.2169e-11]])},\n",
       "    6: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[-2.3476e-03, -2.7074e-03,  1.5069e-03,  1.3597e-04,  3.3877e-04,\n",
       "               9.9913e-04,  2.4511e-03, -4.0920e-04,  3.7870e-04,  1.6366e-03],\n",
       "             [-1.6112e-05,  3.9686e-05, -2.5287e-06,  3.0644e-05, -7.1644e-06,\n",
       "              -2.0152e-05,  7.6241e-05, -7.9830e-06, -3.9278e-05,  5.7189e-05],\n",
       "             [ 2.5722e-06, -1.1509e-05, -1.7188e-05,  1.9273e-06, -1.5608e-05,\n",
       "               9.0919e-06, -7.7895e-07, -4.1342e-06, -1.5916e-05, -1.8746e-06],\n",
       "             [-3.3888e-06, -4.5406e-06, -3.6129e-06,  3.8536e-06, -1.0651e-06,\n",
       "              -1.7100e-06, -3.7903e-07,  1.4347e-06, -1.4973e-06, -5.9033e-07]]),\n",
       "     'exp_avg_sq': tensor([[1.2772e-07, 1.6380e-07, 5.8699e-08, 1.5388e-08, 1.0204e-08, 2.7910e-08,\n",
       "              1.3536e-07, 2.4528e-08, 2.8281e-08, 7.2379e-08],\n",
       "             [1.1074e-10, 1.4687e-10, 1.0414e-10, 1.0610e-10, 3.9172e-11, 2.7194e-11,\n",
       "              2.1634e-10, 2.7857e-11, 1.4971e-10, 8.5799e-11],\n",
       "             [3.5656e-12, 7.0485e-12, 2.0417e-11, 8.2776e-12, 2.1275e-11, 6.7180e-12,\n",
       "              2.0240e-11, 1.2750e-11, 1.1557e-11, 9.4163e-12],\n",
       "             [2.1545e-12, 3.8679e-12, 2.4488e-12, 2.7860e-12, 2.1281e-13, 5.4858e-13,\n",
       "              2.6952e-14, 3.8616e-13, 4.2061e-13, 6.5378e-14]])},\n",
       "    7: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([[ 7.9522e-05,  1.2990e-04,  2.0129e-04,  ..., -6.5162e-04,\n",
       "               5.2281e-04, -2.9096e-04],\n",
       "             [-6.7885e-06,  4.1864e-05,  1.7882e-05,  ...,  3.2173e-04,\n",
       "              -3.5638e-04, -1.5825e-04],\n",
       "             [-1.2932e-05,  1.4152e-05,  1.6105e-05,  ...,  2.9104e-04,\n",
       "              -3.0321e-04, -1.4676e-04],\n",
       "             ...,\n",
       "             [-2.0713e-05, -2.0166e-05, -1.2824e-05,  ...,  3.8517e-04,\n",
       "              -4.1141e-04, -1.8712e-04],\n",
       "             [-1.9595e-05, -1.2944e-05, -1.1372e-05,  ...,  3.0274e-04,\n",
       "              -3.2132e-04, -1.6217e-04],\n",
       "             [-1.9061e-05,  1.6911e-05,  5.8593e-06,  ...,  3.1442e-04,\n",
       "              -3.4450e-04, -1.5413e-04]]),\n",
       "     'exp_avg_sq': tensor([[3.1824e-10, 6.9612e-10, 1.7736e-09,  ..., 2.8988e-08, 2.6676e-08,\n",
       "              6.0689e-09],\n",
       "             [7.1814e-12, 8.7379e-10, 2.2243e-10,  ..., 3.2484e-09, 7.0599e-09,\n",
       "              1.1789e-09],\n",
       "             [4.2713e-11, 5.4183e-11, 1.6237e-10,  ..., 2.8557e-09, 6.4889e-09,\n",
       "              1.1309e-09],\n",
       "             ...,\n",
       "             [1.0799e-10, 5.4322e-10, 1.5342e-10,  ..., 4.2468e-09, 8.1753e-09,\n",
       "              1.6499e-09],\n",
       "             [1.2111e-10, 3.1162e-10, 1.3197e-10,  ..., 3.0109e-09, 7.0680e-09,\n",
       "              1.4394e-09],\n",
       "             [1.1939e-10, 9.8796e-11, 4.2325e-11,  ..., 3.1285e-09, 7.2000e-09,\n",
       "              1.3707e-09]])},\n",
       "    8: {'step': tensor(11.),\n",
       "     'exp_avg': tensor([-6.0607e-04,  7.7659e-04,  6.6861e-04,  8.2475e-04,  8.2119e-04,\n",
       "              7.2314e-04,  4.4702e-04, -2.1392e-05,  6.5135e-04,  9.9721e-04,\n",
       "              2.4017e-04,  7.9042e-04,  9.2788e-04,  8.8490e-04,  3.6380e-05,\n",
       "              7.9819e-04,  4.0479e-04,  2.4265e-04, -1.7584e-04,  3.6458e-04,\n",
       "              9.4565e-04,  3.4399e-04,  1.0136e-03,  3.6983e-04, -2.9186e-04,\n",
       "             -1.3679e-04,  3.1683e-04,  7.6200e-04,  9.4565e-04,  8.2374e-04,\n",
       "              8.1329e-04,  3.2683e-04,  8.8838e-04,  8.6103e-04,  8.0873e-04,\n",
       "              8.1810e-04,  8.1459e-04,  6.7762e-04,  7.8479e-04,  9.0672e-04,\n",
       "              8.3388e-04,  8.0425e-04,  7.8427e-04,  7.6780e-04,  8.9277e-04,\n",
       "              6.1029e-04,  9.9800e-04,  9.4173e-04,  7.6644e-04,  7.2735e-04,\n",
       "              8.5047e-04,  7.2945e-04,  8.7941e-04,  6.9384e-04,  2.8434e-04,\n",
       "              6.9752e-04,  7.8285e-04,  7.6002e-04,  9.1179e-04,  7.3955e-04,\n",
       "              6.5495e-04,  8.7100e-04,  1.0163e-04,  7.8276e-04,  9.6410e-04,\n",
       "              6.9938e-04,  8.4327e-04,  8.1629e-04,  8.1036e-04,  8.0354e-04,\n",
       "              7.5742e-04,  8.5675e-04,  9.5992e-04,  8.7494e-04,  8.2538e-04,\n",
       "              7.8599e-04, -1.7105e-04,  1.3881e-04,  9.5923e-04,  8.4430e-04,\n",
       "              7.4205e-04,  8.2104e-04,  9.6923e-04,  8.1926e-04,  3.8964e-04,\n",
       "              8.5384e-04,  7.8201e-04,  9.6114e-04,  7.2487e-04,  8.0420e-04,\n",
       "              8.3760e-04,  9.2138e-04,  5.9961e-04,  6.8106e-04,  2.9737e-04,\n",
       "              8.6604e-04,  7.2074e-04,  8.6733e-04,  7.2749e-04,  7.0724e-04,\n",
       "              7.8724e-04,  3.2138e-04,  7.8099e-04, -1.7872e-04,  8.1377e-04,\n",
       "              7.6333e-04,  6.4390e-04,  8.3539e-04,  8.9599e-04,  1.6242e-04,\n",
       "              7.7362e-04,  6.7527e-04,  6.4166e-04, -3.6899e-04,  8.7516e-04,\n",
       "              7.4651e-04,  1.4397e-04,  8.5404e-04,  5.8964e-04,  8.3114e-04,\n",
       "              6.4726e-04,  6.6788e-04,  7.9651e-04,  8.4785e-04,  3.9325e-04,\n",
       "              9.0190e-04, -3.6537e-04,  7.5290e-04,  6.6293e-04,  7.1222e-04,\n",
       "              7.6900e-04,  7.7206e-04,  5.7685e-04,  7.3329e-04,  6.6009e-04,\n",
       "              3.6072e-04,  7.9804e-04,  3.3313e-04, -3.5379e-04,  8.9562e-04,\n",
       "              7.8458e-04,  8.1036e-04,  8.1136e-04,  6.1420e-04,  7.0761e-04,\n",
       "              7.3306e-04,  8.2369e-04,  7.9630e-04,  6.2862e-04,  7.2249e-04,\n",
       "              9.3494e-04,  7.8014e-04,  7.9675e-04,  7.9344e-04,  2.0074e-04,\n",
       "              8.1160e-04,  7.4711e-04,  8.2001e-04,  9.6835e-04,  8.4126e-04,\n",
       "              7.3425e-04,  7.7819e-04,  7.2518e-04,  8.8250e-04,  9.7998e-04,\n",
       "              7.7417e-04,  7.1838e-04,  3.1151e-04,  3.5463e-04,  8.9286e-04,\n",
       "              7.9046e-04,  9.5244e-04,  6.8049e-04,  8.3912e-04,  8.7509e-04,\n",
       "              7.4583e-04,  9.5698e-04,  1.1957e-05,  7.9799e-04,  8.6396e-04,\n",
       "              6.9289e-04,  7.4913e-04,  7.8074e-04,  7.5526e-04,  6.1342e-04,\n",
       "              7.6676e-04,  8.0001e-04,  8.0390e-04,  1.0348e-03,  8.8749e-04,\n",
       "              9.2308e-04,  9.2253e-04,  8.6913e-04,  7.9987e-04,  1.0506e-03,\n",
       "              8.3064e-04,  8.7421e-04,  8.1351e-04,  6.9908e-04,  7.2888e-04,\n",
       "              1.0350e-03,  8.1824e-04,  7.2713e-04,  9.4239e-04,  7.2439e-04,\n",
       "              6.4339e-04,  7.8389e-04,  7.5591e-04,  7.7254e-04,  5.2492e-04,\n",
       "              8.0590e-04,  7.7691e-04,  6.5282e-04,  6.8594e-04,  9.3415e-04,\n",
       "              3.6085e-04,  6.8598e-04,  6.6243e-04,  1.0068e-03,  2.6186e-04,\n",
       "              7.4707e-04,  8.1985e-04,  8.5111e-04,  7.9714e-04,  9.5951e-04,\n",
       "              8.0446e-04,  3.5742e-04,  8.6194e-04,  8.3343e-04,  3.2993e-04,\n",
       "              7.7711e-04,  7.5135e-04,  8.9406e-04,  7.5213e-04,  8.3544e-04,\n",
       "              8.0329e-04,  7.2689e-04,  9.4860e-04,  9.7659e-04,  7.6307e-04,\n",
       "              8.7555e-04,  8.6156e-04,  7.9103e-04,  9.3359e-04,  6.5792e-04,\n",
       "              9.5491e-04,  7.6641e-04,  3.6151e-04,  7.2414e-04,  8.3764e-04,\n",
       "              8.0215e-04,  8.0076e-04,  8.4951e-04,  8.8448e-04,  8.1948e-04,\n",
       "              7.9206e-04,  9.5969e-04,  8.3287e-04,  9.5290e-04,  8.2475e-04,\n",
       "              7.0416e-04,  7.0484e-04,  6.9129e-04,  7.8532e-04,  9.0555e-04,\n",
       "              7.7670e-04,  7.3188e-04,  7.7336e-04,  6.5468e-04,  8.9445e-04,\n",
       "              9.2017e-04,  9.5616e-04,  8.5563e-04,  4.4517e-04,  8.4035e-04,\n",
       "              1.1557e-04,  1.0346e-03,  6.4269e-04,  8.2026e-04,  7.9633e-04,\n",
       "              8.7593e-04,  9.2648e-04,  8.1155e-04,  9.8996e-04,  7.0093e-04,\n",
       "              8.6509e-04,  7.4370e-04,  9.8260e-04,  8.6326e-04,  8.3274e-04,\n",
       "              8.0083e-04,  8.8109e-04,  8.6085e-04, -9.9463e-04,  2.5199e-04,\n",
       "              7.9842e-04,  6.2987e-04,  3.3976e-04,  4.0291e-04,  8.4334e-04,\n",
       "             -1.9647e-03,  1.9599e-04,  2.1549e-04,  8.2657e-04,  3.5721e-04,\n",
       "              3.0129e-04,  8.2174e-04,  6.9013e-04,  8.7404e-04,  7.1161e-04,\n",
       "              6.3713e-04,  7.7881e-04, -1.0167e-04, -7.7801e-03,  9.2165e-04,\n",
       "              8.2518e-04,  9.2390e-04,  8.9858e-04,  7.5095e-04,  7.5692e-04,\n",
       "              7.9926e-04,  6.9125e-04,  8.6890e-04,  3.2905e-04,  8.2826e-04,\n",
       "              2.5705e-04,  6.5176e-04,  8.4537e-04,  2.4221e-04,  8.6989e-04,\n",
       "              5.0043e-04,  8.0150e-04,  2.5380e-04,  5.8306e-04, -7.3024e-04,\n",
       "              3.9945e-04,  7.5308e-04,  8.5577e-04,  8.9004e-04,  8.9950e-04,\n",
       "              9.4040e-04,  6.5565e-04,  6.7551e-04,  5.9192e-04,  6.0393e-04,\n",
       "             -9.0846e-04,  8.7193e-04,  6.9125e-04, -4.9112e-03,  9.2031e-04,\n",
       "             -2.3860e-03, -1.3825e-03,  3.1506e-04,  3.9475e-04,  1.0126e-04,\n",
       "             -1.7074e-03,  2.4319e-04,  4.9947e-04, -1.5246e-03, -3.9555e-03,\n",
       "             -4.5794e-04, -4.0428e-04, -4.1328e-04,  3.3867e-04,  3.7897e-04,\n",
       "             -5.6361e-03, -2.0672e-04,  1.4699e-04, -7.6091e-03,  1.7046e-05,\n",
       "             -5.6873e-03,  7.6635e-05,  7.8106e-04,  2.6818e-04, -7.6247e-04,\n",
       "              8.7332e-04,  7.5180e-04,  7.3270e-04,  6.8864e-04,  9.5217e-04,\n",
       "              7.1799e-04,  6.5773e-04,  8.7530e-04,  3.4223e-04,  8.9016e-04,\n",
       "              9.1337e-04, -2.3484e-03,  8.5156e-04,  1.0066e-03,  2.2487e-04,\n",
       "              9.1438e-04,  3.5520e-04,  7.0870e-04,  8.0593e-04,  7.8554e-04,\n",
       "              7.5873e-04,  5.4676e-04,  8.4649e-04,  7.7200e-04,  8.4306e-04,\n",
       "              8.3676e-04,  2.1196e-04,  9.2426e-04,  1.7246e-05,  7.9741e-04,\n",
       "             -5.4098e-04,  8.5280e-04,  7.7406e-04,  4.4790e-04,  4.8845e-04,\n",
       "              1.1026e-03,  8.2285e-04,  9.2848e-04, -1.2913e-04,  7.1168e-04,\n",
       "              6.8617e-04,  6.5525e-04,  7.0878e-04,  7.2803e-04,  5.7707e-04,\n",
       "              7.8768e-04,  6.8806e-04,  1.0300e-03,  4.6935e-04,  6.3572e-04,\n",
       "              3.3829e-04,  2.4288e-04,  6.7738e-04,  6.8739e-04,  8.7431e-04,\n",
       "              8.1404e-04,  4.9254e-04,  7.6136e-04, -5.4646e-04,  6.6111e-04,\n",
       "             -1.3648e-03,  8.2584e-05,  8.7025e-05,  6.7307e-04,  8.5563e-04,\n",
       "              8.4382e-04,  1.0093e-04,  7.7135e-04,  2.6524e-04,  8.6308e-04,\n",
       "              7.8450e-04,  3.7674e-04,  5.2238e-04,  4.7569e-04,  7.0751e-04,\n",
       "              6.3987e-04,  3.4523e-04,  2.9029e-04,  5.2965e-04,  3.3157e-04,\n",
       "              7.5271e-04,  6.9851e-04,  8.3048e-04,  4.9345e-04,  3.9622e-04,\n",
       "              8.2810e-04,  3.0362e-04,  8.5249e-04,  6.9061e-04,  4.0744e-04,\n",
       "              2.0838e-04,  6.8335e-04,  8.5469e-04,  4.7060e-04,  6.5280e-04,\n",
       "              2.7371e-04, -2.9826e-04,  7.7810e-04,  1.5036e-04,  7.6334e-04,\n",
       "             -5.3435e-04,  3.7561e-04, -5.6960e-04, -2.6139e-04, -2.7260e-03,\n",
       "             -1.0280e-02, -8.4575e-03, -2.1300e-03,  1.5576e-04,  8.7173e-04,\n",
       "              7.9023e-04,  7.1241e-04, -7.0046e-03,  7.8891e-04,  4.3990e-04,\n",
       "              6.7424e-04, -1.5469e-03, -1.6880e-03,  7.1083e-04,  3.3885e-04,\n",
       "              7.7441e-04,  6.3027e-04,  6.2150e-04,  2.2936e-04,  7.6054e-04,\n",
       "              8.4197e-04,  8.6113e-04,  7.2541e-04,  6.6973e-04,  8.9291e-04,\n",
       "              7.2823e-04,  6.2920e-04,  8.2850e-04,  8.0366e-04,  2.3473e-04,\n",
       "              8.9138e-04,  2.8430e-04,  7.4653e-04, -3.0271e-02, -5.1392e-04,\n",
       "             -1.0852e-02,  6.1987e-04,  8.2412e-04, -3.7749e-03,  8.8329e-04,\n",
       "             -1.0061e-02, -2.2095e-03,  8.3383e-04,  8.2893e-04, -5.5283e-04,\n",
       "              8.1524e-04,  8.2019e-04,  4.6490e-04, -2.3916e-03,  7.2462e-04,\n",
       "              6.6490e-04,  6.5729e-04,  7.0427e-04, -5.6262e-04, -3.2925e-04,\n",
       "             -1.4608e-03,  3.9019e-04,  7.6762e-05,  8.5716e-04,  7.3489e-04,\n",
       "              5.2328e-04,  3.9131e-04,  3.2280e-04,  8.0937e-04,  9.6670e-04,\n",
       "              6.3427e-04,  7.5078e-04,  7.0258e-04,  7.4783e-04, -7.7294e-03,\n",
       "             -5.3253e-03,  1.3089e-05,  3.2832e-04,  5.4735e-04,  5.6381e-04,\n",
       "             -7.7524e-05,  8.2261e-04,  3.2975e-04,  8.3750e-04,  2.8750e-04,\n",
       "             -3.7907e-03,  6.7305e-04,  8.1503e-04,  6.0191e-04,  3.6280e-04,\n",
       "              7.3921e-04,  8.2417e-04,  7.4921e-04,  7.6356e-04,  6.3943e-04,\n",
       "              6.2623e-04,  9.7661e-04, -6.9418e-04,  9.0116e-04, -1.2489e-03,\n",
       "              7.5012e-04, -8.6569e-03, -8.8868e-04, -3.2519e-02, -4.0842e-02,\n",
       "             -7.8143e-03,  3.2455e-04,  7.5916e-04, -2.6075e-03, -1.1542e-03,\n",
       "             -3.6458e-03, -5.1072e-03, -2.0544e-02, -3.7009e-04,  6.8725e-04,\n",
       "             -1.8693e-03,  4.1009e-04,  6.7043e-04,  4.3076e-04,  8.9005e-05,\n",
       "              8.2789e-04, -1.7503e-03,  7.3717e-04, -5.6412e-04, -4.9908e-03,\n",
       "             -2.9079e-03, -4.8316e-04, -6.3699e-04, -3.4384e-03,  6.7612e-05,\n",
       "             -9.7241e-04,  4.2160e-04,  9.3517e-04, -6.8109e-04,  7.9978e-04,\n",
       "              4.7805e-04,  7.0562e-04,  2.8283e-05,  7.2720e-04,  8.4296e-04,\n",
       "              9.1851e-04, -4.2691e-04, -7.6036e-05,  2.5314e-04,  5.7452e-04,\n",
       "              3.4688e-04,  8.2174e-04,  6.3264e-04,  2.8480e-04, -6.6002e-04,\n",
       "             -7.0204e-05,  6.9036e-04,  3.1425e-04, -1.5099e-02, -1.0256e-03,\n",
       "              3.7811e-04,  6.1371e-04, -5.9057e-04, -5.6294e-03, -1.1008e-03,\n",
       "              2.9734e-04, -9.2609e-04,  7.9201e-04,  7.3144e-04, -5.6034e-03,\n",
       "              7.4529e-04,  7.3512e-04,  8.7267e-04,  7.8551e-04,  6.8562e-04,\n",
       "              7.5376e-04,  7.5724e-04,  8.5142e-04,  6.4362e-04,  8.2226e-04,\n",
       "              8.2729e-04,  7.5422e-04,  9.1153e-04,  9.1473e-04,  7.0815e-04,\n",
       "              8.4257e-04,  4.0831e-04, -5.4516e-03, -1.1641e-03,  8.2240e-04,\n",
       "             -3.1579e-03, -6.2971e-03,  4.7636e-04,  1.7872e-04, -9.6951e-04,\n",
       "              2.5530e-04,  3.2108e-04, -2.0488e-03, -1.0781e-03, -1.1296e-04,\n",
       "              7.9347e-04,  6.2492e-04, -3.2170e-03, -3.0953e-03, -4.8692e-03,\n",
       "              9.1420e-04,  8.2450e-04,  9.2023e-04,  9.3524e-04,  7.8388e-04,\n",
       "              8.5214e-04,  6.1861e-04,  7.5256e-04,  6.4587e-04, -4.3317e-04,\n",
       "              8.2572e-04,  8.0559e-04, -5.7486e-04,  7.2628e-04,  7.5048e-04,\n",
       "             -6.1054e-04, -1.1046e-03,  2.4795e-04, -1.5685e-03, -2.0270e-03,\n",
       "              7.1895e-04,  6.2269e-04,  6.8653e-04,  7.0379e-04,  8.3398e-04,\n",
       "              6.7744e-04,  8.5028e-04,  6.9167e-04,  7.0490e-04, -2.3400e-04,\n",
       "              4.5197e-04, -2.7564e-03,  8.5738e-04,  4.1183e-04,  7.9379e-04,\n",
       "              9.0219e-04,  6.9889e-04, -1.6968e-03, -4.4775e-05,  5.1867e-04,\n",
       "              8.3977e-04,  8.7809e-04, -6.9249e-05,  3.8337e-04,  1.0162e-03,\n",
       "             -1.5033e-04,  5.3242e-04,  5.5175e-04, -4.2001e-03, -5.6896e-03,\n",
       "              5.6105e-04, -2.1680e-03, -1.7034e-02,  7.2279e-04,  8.9002e-04,\n",
       "              6.9333e-04,  7.3515e-04]),\n",
       "     'exp_avg_sq': tensor([4.6109e-08, 2.4205e-08, 2.1136e-08, 2.5018e-08, 2.5385e-08, 2.2252e-08,\n",
       "             2.8460e-08, 3.3709e-08, 2.1081e-08, 3.2540e-08, 3.3309e-08, 2.4425e-08,\n",
       "             2.9787e-08, 2.7530e-08, 3.6328e-08, 2.4927e-08, 2.9827e-08, 2.6011e-08,\n",
       "             7.5095e-08, 2.8142e-08, 3.0875e-08, 3.0562e-08, 3.4043e-08, 2.9909e-08,\n",
       "             7.9080e-08, 7.4572e-08, 3.4418e-08, 2.3389e-08, 3.0523e-08, 2.6752e-08,\n",
       "             2.5844e-08, 2.8575e-08, 2.7381e-08, 2.7379e-08, 2.5373e-08, 2.5165e-08,\n",
       "             2.5326e-08, 2.2576e-08, 2.4302e-08, 2.8170e-08, 2.6152e-08, 2.4892e-08,\n",
       "             2.4435e-08, 2.4101e-08, 2.8016e-08, 2.0366e-08, 3.2224e-08, 3.0452e-08,\n",
       "             2.3536e-08, 2.3458e-08, 2.7217e-08, 2.3120e-08, 2.7898e-08, 2.2170e-08,\n",
       "             2.8081e-08, 2.1787e-08, 2.5075e-08, 2.4008e-08, 2.9223e-08, 2.2755e-08,\n",
       "             2.0448e-08, 2.7054e-08, 2.6710e-08, 2.4751e-08, 3.0687e-08, 2.1980e-08,\n",
       "             2.6582e-08, 2.6589e-08, 2.5464e-08, 2.5662e-08, 2.3131e-08, 2.7552e-08,\n",
       "             3.0692e-08, 2.8186e-08, 2.6114e-08, 2.3608e-08, 4.0693e-08, 3.5942e-08,\n",
       "             2.9725e-08, 2.5248e-08, 2.4506e-08, 2.5574e-08, 3.0047e-08, 2.6073e-08,\n",
       "             2.7269e-08, 2.7979e-08, 2.4212e-08, 3.2375e-08, 2.1881e-08, 2.5190e-08,\n",
       "             2.6649e-08, 3.0036e-08, 2.0257e-08, 2.1620e-08, 2.8626e-08, 2.6718e-08,\n",
       "             2.3085e-08, 2.7567e-08, 2.2937e-08, 2.1985e-08, 2.4856e-08, 3.2957e-08,\n",
       "             2.5304e-08, 6.9881e-08, 2.6387e-08, 2.4754e-08, 2.0578e-08, 2.6662e-08,\n",
       "             2.8901e-08, 2.9848e-08, 2.3689e-08, 6.2539e-08, 1.9830e-08, 4.3275e-08,\n",
       "             2.7930e-08, 2.3902e-08, 3.2343e-08, 2.6811e-08, 3.2508e-08, 2.6768e-08,\n",
       "             3.3764e-08, 3.2266e-08, 2.5189e-08, 2.7183e-08, 2.8449e-08, 2.8684e-08,\n",
       "             4.2500e-08, 2.4155e-08, 2.1636e-08, 2.2954e-08, 2.3738e-08, 2.4123e-08,\n",
       "             1.8910e-08, 2.2691e-08, 2.1284e-08, 3.0985e-08, 2.4178e-08, 6.4780e-08,\n",
       "             7.2518e-08, 2.7960e-08, 2.4439e-08, 2.4754e-08, 2.5314e-08, 2.0032e-08,\n",
       "             2.2783e-08, 2.3654e-08, 2.5770e-08, 2.4964e-08, 1.9578e-08, 2.2524e-08,\n",
       "             2.9682e-08, 2.5043e-08, 2.4960e-08, 2.5017e-08, 3.6458e-08, 2.6277e-08,\n",
       "             2.4088e-08, 2.5490e-08, 3.1648e-08, 2.7071e-08, 2.3353e-08, 2.4698e-08,\n",
       "             2.2836e-08, 2.8312e-08, 3.2297e-08, 2.3486e-08, 2.2298e-08, 2.6510e-08,\n",
       "             3.4360e-08, 2.8598e-08, 2.5414e-08, 3.0628e-08, 2.1638e-08, 2.6020e-08,\n",
       "             2.7038e-08, 2.4046e-08, 3.0135e-08, 2.9692e-08, 2.5571e-08, 2.7759e-08,\n",
       "             2.2167e-08, 2.3218e-08, 2.5272e-08, 2.2938e-08, 1.9890e-08, 2.4265e-08,\n",
       "             2.5389e-08, 2.5859e-08, 3.4187e-08, 2.8436e-08, 2.9696e-08, 2.9277e-08,\n",
       "             2.7248e-08, 2.4680e-08, 3.5017e-08, 2.6910e-08, 2.8564e-08, 2.5044e-08,\n",
       "             2.2790e-08, 2.2997e-08, 3.4261e-08, 2.5530e-08, 2.3216e-08, 2.9617e-08,\n",
       "             2.2774e-08, 1.9972e-08, 2.4896e-08, 2.3939e-08, 2.4480e-08, 6.0554e-08,\n",
       "             2.5340e-08, 2.5288e-08, 2.0472e-08, 2.2092e-08, 3.1031e-08, 3.4829e-08,\n",
       "             2.1367e-08, 2.0914e-08, 3.2979e-08, 3.1650e-08, 2.3634e-08, 2.5979e-08,\n",
       "             2.7608e-08, 2.4583e-08, 3.0488e-08, 2.5558e-08, 2.7313e-08, 2.6461e-08,\n",
       "             2.6481e-08, 3.0573e-08, 2.4637e-08, 2.3337e-08, 2.8716e-08, 2.3847e-08,\n",
       "             2.6598e-08, 2.5401e-08, 2.3355e-08, 2.9656e-08, 3.1535e-08, 2.4295e-08,\n",
       "             2.7909e-08, 2.6714e-08, 2.4233e-08, 3.0630e-08, 2.0528e-08, 2.9593e-08,\n",
       "             2.4109e-08, 2.7490e-08, 2.2926e-08, 2.5714e-08, 2.5388e-08, 2.4785e-08,\n",
       "             2.6979e-08, 2.8510e-08, 2.5983e-08, 2.4469e-08, 3.0526e-08, 2.5508e-08,\n",
       "             3.1385e-08, 2.5643e-08, 2.2658e-08, 2.2407e-08, 2.2545e-08, 2.4672e-08,\n",
       "             2.9612e-08, 2.4363e-08, 2.2848e-08, 2.3870e-08, 2.1069e-08, 2.7618e-08,\n",
       "             2.9572e-08, 3.0551e-08, 2.7155e-08, 3.1804e-08, 2.6495e-08, 2.7422e-08,\n",
       "             3.3467e-08, 2.0917e-08, 2.6422e-08, 2.5104e-08, 2.7017e-08, 2.9215e-08,\n",
       "             2.5183e-08, 3.2012e-08, 2.2198e-08, 2.7912e-08, 2.3831e-08, 3.1338e-08,\n",
       "             2.7669e-08, 2.6599e-08, 2.5394e-08, 2.8102e-08, 2.7600e-08, 1.4369e-07,\n",
       "             2.7338e-08, 2.4641e-08, 2.0710e-08, 3.0031e-08, 2.6639e-08, 2.6272e-08,\n",
       "             1.8941e-07, 2.9547e-08, 2.6635e-08, 2.5793e-08, 2.8680e-08, 2.7236e-08,\n",
       "             2.5585e-08, 2.0640e-08, 2.7561e-08, 2.2352e-08, 2.0764e-08, 2.4131e-08,\n",
       "             3.5306e-08, 3.0583e-06, 3.0029e-08, 2.6471e-08, 3.0054e-08, 2.8952e-08,\n",
       "             2.3453e-08, 2.4577e-08, 2.4862e-08, 2.2382e-08, 2.7313e-08, 2.8005e-08,\n",
       "             2.6441e-08, 2.6942e-08, 2.0818e-08, 2.7462e-08, 3.2096e-08, 2.8188e-08,\n",
       "             6.1242e-08, 2.5979e-08, 3.1553e-08, 2.9540e-08, 1.1282e-07, 2.8968e-08,\n",
       "             2.4433e-08, 2.8423e-08, 2.8665e-08, 2.8832e-08, 3.0740e-08, 2.1468e-08,\n",
       "             2.1859e-08, 2.0316e-08, 3.0988e-08, 1.4363e-07, 2.7862e-08, 2.1957e-08,\n",
       "             1.0614e-06, 2.9859e-08, 3.0594e-07, 1.5354e-07, 3.5069e-08, 6.0067e-08,\n",
       "             3.4244e-08, 2.7844e-07, 6.5684e-08, 3.2368e-08, 2.1336e-07, 5.2370e-07,\n",
       "             7.5897e-08, 7.1656e-08, 7.3779e-08, 2.8670e-08, 3.1066e-08, 1.0232e-06,\n",
       "             4.1310e-08, 2.7891e-08, 1.7806e-06, 7.0081e-08, 1.3019e-06, 2.6911e-08,\n",
       "             2.5101e-08, 3.1913e-08, 1.1044e-07, 2.7762e-08, 2.4054e-08, 2.3669e-08,\n",
       "             2.2359e-08, 3.0750e-08, 2.2646e-08, 2.0950e-08, 2.8147e-08, 2.7803e-08,\n",
       "             2.8544e-08, 2.9686e-08, 3.6551e-07, 2.6987e-08, 3.3227e-08, 3.1435e-08,\n",
       "             2.9960e-08, 3.5511e-08, 2.3055e-08, 2.5880e-08, 2.4532e-08, 2.4883e-08,\n",
       "             3.3414e-08, 2.7892e-08, 2.4623e-08, 2.7130e-08, 2.6954e-08, 2.9793e-08,\n",
       "             2.9494e-08, 3.4405e-08, 2.5206e-08, 1.0629e-07, 2.7486e-08, 2.4976e-08,\n",
       "             2.7933e-08, 2.9520e-08, 3.7483e-08, 2.6950e-08, 2.9966e-08, 7.0594e-08,\n",
       "             2.2962e-08, 2.1827e-08, 2.1240e-08, 2.2892e-08, 2.2806e-08, 1.9430e-08,\n",
       "             2.4784e-08, 2.2057e-08, 3.3996e-08, 3.4729e-08, 2.0521e-08, 3.3136e-08,\n",
       "             2.9407e-08, 2.1960e-08, 2.2079e-08, 2.8433e-08, 2.5840e-08, 2.8930e-08,\n",
       "             2.4324e-08, 7.8793e-08, 2.1205e-08, 1.2997e-07, 3.0036e-08, 3.3371e-08,\n",
       "             2.1153e-08, 2.7589e-08, 2.6978e-08, 5.8457e-08, 2.4324e-08, 5.8121e-08,\n",
       "             2.7407e-08, 2.4810e-08, 2.9348e-08, 3.1718e-08, 3.1693e-08, 2.2603e-08,\n",
       "             3.4769e-08, 3.0939e-08, 2.8670e-08, 2.8362e-08, 2.7045e-08, 2.3480e-08,\n",
       "             2.3226e-08, 2.6782e-08, 2.8830e-08, 3.0157e-08, 2.6287e-08, 2.9413e-08,\n",
       "             2.7471e-08, 2.2098e-08, 3.6075e-08, 3.4404e-08, 2.1356e-08, 2.7414e-08,\n",
       "             2.8765e-08, 2.1304e-08, 3.4238e-08, 7.8947e-08, 2.5128e-08, 3.4698e-08,\n",
       "             2.5115e-08, 1.1531e-07, 3.0386e-08, 2.0185e-07, 3.6946e-08, 8.4221e-07,\n",
       "             2.7665e-06, 2.6015e-06, 1.9569e-07, 3.4562e-08, 2.7905e-08, 2.5618e-08,\n",
       "             2.2662e-08, 1.8983e-06, 2.6110e-08, 5.8820e-08, 2.1796e-08, 1.2750e-07,\n",
       "             2.2160e-07, 2.2621e-08, 3.3549e-08, 2.4712e-08, 6.4230e-08, 2.0876e-08,\n",
       "             2.6453e-08, 2.4301e-08, 2.7093e-08, 2.7632e-08, 2.2611e-08, 2.2055e-08,\n",
       "             2.8868e-08, 2.3424e-08, 2.0796e-08, 2.6955e-08, 2.4829e-08, 2.8446e-08,\n",
       "             2.8958e-08, 2.7541e-08, 2.4051e-08, 2.2577e-05, 8.1031e-08, 3.0782e-06,\n",
       "             6.1515e-08, 2.6948e-08, 6.1586e-07, 2.8401e-08, 2.8221e-06, 3.3540e-07,\n",
       "             2.6621e-08, 2.6549e-08, 4.7064e-08, 2.5759e-08, 2.6242e-08, 3.0575e-08,\n",
       "             2.3559e-07, 2.3325e-08, 2.1771e-08, 2.1449e-08, 2.3006e-08, 7.6536e-08,\n",
       "             4.1885e-08, 1.7848e-07, 2.8274e-08, 3.6278e-08, 2.7565e-08, 2.3607e-08,\n",
       "             3.0542e-08, 2.8393e-08, 3.1294e-08, 2.5621e-08, 3.1774e-08, 2.0907e-08,\n",
       "             2.3772e-08, 2.3140e-08, 2.4280e-08, 1.8587e-06, 1.0866e-06, 2.8982e-08,\n",
       "             2.7885e-08, 3.1698e-08, 2.9658e-08, 1.3335e-07, 2.6183e-08, 2.8291e-08,\n",
       "             2.6903e-08, 3.4892e-08, 7.4081e-07, 3.3403e-08, 2.6495e-08, 2.0209e-08,\n",
       "             2.7989e-08, 2.4176e-08, 2.6797e-08, 2.3830e-08, 2.4332e-08, 2.0829e-08,\n",
       "             2.0529e-08, 3.1823e-08, 7.9535e-08, 2.9071e-08, 1.8864e-07, 2.4022e-08,\n",
       "             2.5801e-06, 1.2334e-07, 2.4680e-05, 4.0685e-05, 2.7277e-06, 2.7971e-08,\n",
       "             2.4148e-08, 5.0071e-07, 1.1447e-07, 5.5622e-07, 7.6390e-07, 1.2228e-05,\n",
       "             6.9665e-08, 2.2710e-08, 3.0339e-07, 3.2999e-08, 6.1604e-08, 2.9818e-08,\n",
       "             3.0960e-08, 2.5883e-08, 2.2855e-07, 2.3987e-08, 7.5198e-08, 8.8354e-07,\n",
       "             4.5642e-07, 7.9138e-08, 1.3518e-07, 5.7119e-07, 2.9038e-08, 1.1626e-07,\n",
       "             2.9625e-08, 2.9506e-08, 2.0422e-07, 2.5513e-08, 3.0374e-08, 2.2540e-08,\n",
       "             2.9278e-08, 2.3295e-08, 2.7132e-08, 2.9535e-08, 1.0831e-07, 7.5947e-08,\n",
       "             2.8519e-08, 2.9103e-08, 3.1498e-08, 2.6066e-08, 3.2852e-08, 3.4632e-08,\n",
       "             1.1055e-07, 3.5867e-08, 2.1918e-08, 3.1854e-08, 6.7118e-06, 1.2100e-07,\n",
       "             2.8497e-08, 3.2552e-08, 1.1155e-07, 1.2020e-06, 1.2021e-07, 3.0145e-08,\n",
       "             1.8132e-07, 2.5182e-08, 2.3717e-08, 9.6160e-07, 2.3942e-08, 2.3463e-08,\n",
       "             2.8869e-08, 2.4724e-08, 2.2259e-08, 2.4294e-08, 2.4350e-08, 2.6627e-08,\n",
       "             2.1421e-08, 2.6499e-08, 2.6665e-08, 2.4046e-08, 2.9515e-08, 2.9727e-08,\n",
       "             2.2664e-08, 2.7341e-08, 2.9318e-08, 1.1407e-06, 1.4920e-07, 2.6506e-08,\n",
       "             4.1922e-07, 1.2430e-06, 3.0425e-08, 3.0744e-08, 1.8019e-07, 5.9442e-08,\n",
       "             2.9795e-08, 2.6446e-07, 1.7378e-07, 3.9008e-08, 2.4826e-08, 2.0303e-08,\n",
       "             3.6984e-07, 4.3409e-07, 8.4021e-07, 2.9493e-08, 2.6276e-08, 2.9668e-08,\n",
       "             3.0462e-08, 2.4988e-08, 2.7388e-08, 2.0207e-08, 2.4121e-08, 2.0996e-08,\n",
       "             4.5849e-08, 2.6215e-08, 2.5634e-08, 4.7182e-08, 2.2777e-08, 2.3540e-08,\n",
       "             1.1320e-07, 1.1501e-07, 6.5375e-08, 1.8466e-07, 2.7927e-07, 2.3207e-08,\n",
       "             2.0659e-08, 2.1927e-08, 2.2401e-08, 2.6657e-08, 2.1541e-08, 2.7821e-08,\n",
       "             2.1943e-08, 2.2562e-08, 1.3553e-07, 2.9604e-08, 3.3010e-07, 2.7578e-08,\n",
       "             2.9646e-08, 2.5391e-08, 2.9061e-08, 2.2071e-08, 1.5637e-07, 3.3628e-08,\n",
       "             2.9093e-08, 2.6437e-08, 2.8051e-08, 3.7287e-08, 3.2347e-08, 3.3937e-08,\n",
       "             6.9981e-08, 3.1298e-08, 5.9967e-08, 5.7713e-07, 9.2072e-07, 2.8645e-08,\n",
       "             3.3720e-07, 8.7581e-06, 2.3138e-08, 2.8712e-08, 2.2597e-08, 2.3391e-08])}},\n",
       "   'param_groups': [{'lr': 0.004,\n",
       "     'betas': (0.9, 0.999),\n",
       "     'eps': 1e-08,\n",
       "     'weight_decay': 0,\n",
       "     'amsgrad': False,\n",
       "     'maximize': False,\n",
       "     'foreach': None,\n",
       "     'capturable': False,\n",
       "     'differentiable': False,\n",
       "     'fused': None,\n",
       "     'params': [0, 1, 2, 3, 4, 5, 6, 7, 8]}]}],\n",
       " 'lr_schedulers': [{'factor': 0.1,\n",
       "   'default_min_lr': 0,\n",
       "   'min_lrs': [0],\n",
       "   'patience': 3,\n",
       "   'verbose': False,\n",
       "   'cooldown': 0,\n",
       "   'cooldown_counter': 0,\n",
       "   'mode': 'min',\n",
       "   'threshold': 0.0001,\n",
       "   'threshold_mode': 'rel',\n",
       "   'eps': 1e-08,\n",
       "   'last_epoch': 0,\n",
       "   '_last_lr': [0.004],\n",
       "   'mode_worse': inf,\n",
       "   'best': inf,\n",
       "   'num_bad_epochs': 0}],\n",
       " 'hparams_name': 'kwargs',\n",
       " 'hyper_parameters': {'optimizer': torch.optim.adam.Adam,\n",
       "  'optimizer_params': {'lr': 0.004},\n",
       "  'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
       "  'scheduler_params': {'mode': 'min', 'patience': 3},\n",
       "  'scheduler_interval': 'epoch'}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.load(path, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m torchFastText\u001b[38;5;241m.\u001b[39mfrom_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchFastText_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightning_logs/version_0/checkpoints/epoch=0-step=11.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchFastText_config.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/codif-ape-train/src/models/torch-fastText/torchFastText/torchFastText.py:682\u001b[0m, in \u001b[0;36mtorchFastText.load_from_checkpoint_test\u001b[0;34m(self, checkpoint_path, config_path)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint_test\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint_path, config_path):\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;241m=\u001b[39m \u001b[43mFastTextModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpytorch_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpytorch_model\u001b[38;5;241m.\u001b[39mtokenizer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:165\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 165\u001b[0m obj \u001b[38;5;241m=\u001b[39m instantiator(\u001b[38;5;28mcls\u001b[39m, _cls_kwargs) \u001b[38;5;28;01mif\u001b[39;00m instantiator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n",
      "File \u001b[0;32m~/work/codif-ape-train/src/models/torch-fastText/torchFastText/model/lightning_module.py:41\u001b[0m, in \u001b[0;36mFastTextModule.__init__\u001b[0;34m(self, model, loss, optimizer, optimizer_params, scheduler, scheduler_params, scheduler_interval, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy_fn \u001b[38;5;241m=\u001b[39m Accuracy(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_params \u001b[38;5;241m=\u001b[39m optimizer_params\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'num_classes'"
     ]
    }
   ],
   "source": [
    "model = torchFastText.from_json(\"torchFastText_config.json\")\n",
    "path = \"lightning_logs/version_0/checkpoints/epoch=0-step=11.ckpt\"\n",
    "model.load_from_checkpoint_test(path, \"torchFastText_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"coiffeur, boulangerie, pâtisserie\"]\n",
    "X = np.array([[text[0], 0, 0, 0, 0, 0, 0]])  # our new entry\n",
    "TOP_K = 5\n",
    "\n",
    "pred, conf = model.predict(X, top_k=TOP_K)\n",
    "pred_naf = encoder.inverse_transform(pred.reshape(-1))\n",
    "subset = naf2008.set_index(\"code\").loc[np.flip(pred_naf)]\n",
    "\n",
    "for i in range(TOP_K - 1, -1, -1):\n",
    "    print(\n",
    "        f\"Prediction: {pred_naf[i]}, confidence:  {conf[0, i]}, description: {subset['libelle'][pred_naf[i]]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchFastText.explainability.visualisation import (\n",
    "    visualize_letter_scores,\n",
    "    visualize_word_scores,\n",
    ")\n",
    "\n",
    "pred, conf, all_scores, all_scores_letters = model.predict_and_explain(X)\n",
    "visualize_word_scores(all_scores, text, pred_naf.reshape(1, -1))\n",
    "visualize_letter_scores(all_scores_letters, text, pred_naf.reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

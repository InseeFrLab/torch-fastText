{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from torchFastText import torchFastText\n",
    "from torchFastText.preprocess import clean_text_feature\n",
    "from torchFastText.datasets import NGramTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some utils functions that will help us format our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_surface(\n",
    "    df: pd.DataFrame, surface_feature_name: int, like_sirene_3: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Categorize the surface of the activity.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to categorize.\n",
    "        surface_feature_name (str): Name of the surface feature.\n",
    "        like_sirene_3 (bool): If True, categorize like Sirene 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a new column \"surf_cat\".\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].replace(\"nan\", np.nan)\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(float)\n",
    "    # Check surface feature exists\n",
    "    if surface_feature_name not in df.columns:\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} not found in DataFrame.\")\n",
    "    # Check surface feature is a float variable\n",
    "    if not (pd.api.types.is_float_dtype(df_copy[surface_feature_name])):\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} must be a float variable.\")\n",
    "\n",
    "    if like_sirene_3:\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy[surface_feature_name],\n",
    "            bins=[0, 120, 400, 2500, np.inf],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "    else:\n",
    "        # Log transform the surface\n",
    "        df_copy[\"surf_log\"] = np.log(df[surface_feature_name])\n",
    "\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy.surf_log,\n",
    "            bins=[0, 3, 4, 5, 12],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "\n",
    "    df_copy[surface_feature_name] = df_copy[\"surf_cat\"].replace(\"nan\", \"0\")\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(int)\n",
    "    df_copy = df_copy.drop(columns=[\"surf_log\", \"surf_cat\"], errors=\"ignore\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def clean_and_tokenize_df(\n",
    "    df,\n",
    "    categorical_features=[\"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\"],\n",
    "    text_feature=\"libelle_processed\",\n",
    "    label_col=\"apet_finale\",\n",
    "):\n",
    "    df.fillna(\"nan\", inplace=True)\n",
    "\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"evenement_type\": \"EVT\",\n",
    "            \"cj\": \"CJ\",\n",
    "            \"activ_nat_et\": \"NAT\",\n",
    "            \"liasse_type\": \"TYP\",\n",
    "            \"activ_surf_et\": \"SRF\",\n",
    "            \"activ_perm_et\": \"CRT\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    les = []\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        les.append(le)\n",
    "\n",
    "    df = categorize_surface(df, \"SRF\", like_sirene_3=True)\n",
    "    df = df[[text_feature, \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"SRF\", \"CRT\", label_col]]\n",
    "\n",
    "    return df, les\n",
    "\n",
    "\n",
    "def stratified_split_rare_labels(X, y, test_size=0.2, min_train_samples=1):\n",
    "    # Get unique labels and their frequencies\n",
    "    unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Separate rare and common labels\n",
    "    rare_labels = unique_labels[label_counts == 1]\n",
    "\n",
    "    # Create initial mask for rare labels to go into training set\n",
    "    rare_label_mask = np.isin(y, rare_labels)\n",
    "\n",
    "    # Separate data into rare and common label datasets\n",
    "    X_rare = X[rare_label_mask]\n",
    "    y_rare = y[rare_label_mask]\n",
    "    X_common = X[~rare_label_mask]\n",
    "    y_common = y[~rare_label_mask]\n",
    "\n",
    "    # Split common labels stratified\n",
    "    X_common_train, X_common_test, y_common_train, y_common_test = train_test_split(\n",
    "        X_common, y_common, test_size=test_size, stratify=y_common\n",
    "    )\n",
    "\n",
    "    # Combine rare labels with common labels split\n",
    "    X_train = np.concatenate([X_rare, X_common_train])\n",
    "    y_train = np.concatenate([y_rare, y_common_train])\n",
    "    X_test = X_common_test\n",
    "    y_test = y_common_test\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:24:44 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-01-27 15:24:44 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-01-27 15:24:44 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-01-27 15:24:45 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-01-27 15:24:45 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    }
   ],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "    key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    secret=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    ")\n",
    "df = (\n",
    "    pq.ParquetDataset(\n",
    "        \"projet-ape/extractions/20241027_sirene4.parquet\",\n",
    "        filesystem=fs,\n",
    "    )\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ").sample(frac=0.001).fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:24:49 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>libelle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0111Z</td>\n",
       "      <td>Culture de céréales (à l'exception du riz), de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112Z</td>\n",
       "      <td>Culture du riz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0113Z</td>\n",
       "      <td>Culture de légumes, de melons, de racines et d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0114Z</td>\n",
       "      <td>Culture de la canne à sucre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0115Z</td>\n",
       "      <td>Culture du tabac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>9609Z</td>\n",
       "      <td>Autres services personnels n.c.a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>9700Z</td>\n",
       "      <td>Activités des ménages en tant qu'employeurs de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>9810Z</td>\n",
       "      <td>Activités indifférenciées des ménages en tant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>9820Z</td>\n",
       "      <td>Activités indifférenciées des ménages en tant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>9900Z</td>\n",
       "      <td>Activités des organisations et organismes extr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      code                                            libelle\n",
       "0    0111Z  Culture de céréales (à l'exception du riz), de...\n",
       "1    0112Z                                     Culture du riz\n",
       "2    0113Z  Culture de légumes, de melons, de racines et d...\n",
       "3    0114Z                        Culture de la canne à sucre\n",
       "4    0115Z                                   Culture du tabac\n",
       "..     ...                                                ...\n",
       "727  9609Z                  Autres services personnels n.c.a.\n",
       "728  9700Z  Activités des ménages en tant qu'employeurs de...\n",
       "729  9810Z  Activités indifférenciées des ménages en tant ...\n",
       "730  9820Z  Activités indifférenciées des ménages en tant ...\n",
       "731  9900Z  Activités des organisations et organismes extr...\n",
       "\n",
       "[732 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with fs.open(\"projet-ape/data/naf2008.csv\") as file:\n",
    "    naf2008 = pd.read_csv(file, sep=\";\")\n",
    "naf2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_libelles(\n",
    "    df: pd.DataFrame,\n",
    "    df_naf: pd.DataFrame,\n",
    "    y: str,\n",
    "    text_feature: str,\n",
    "    textual_features: list,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    missing_codes = set(df_naf[\"code\"])\n",
    "    fake_obs = df_naf[df_naf[\"code\"].isin(missing_codes)]\n",
    "    fake_obs[y] = fake_obs[\"code\"]\n",
    "    fake_obs[text_feature] = fake_obs[[text_feature]].apply(\n",
    "        lambda row: \" \".join(f\"[{col}] {val}\" for col, val in row.items() if val != \"\"), axis=1\n",
    "    )\n",
    "    df = pd.concat([df, fake_obs[[col for col in fake_obs.columns if col in df.columns]]])\n",
    "\n",
    "    if textual_features is not None:\n",
    "        for feature in textual_features:\n",
    "            df[feature] = df[feature].fillna(value=\"\")\n",
    "    if categorical_features is not None:\n",
    "        for feature in categorical_features:\n",
    "            df[feature] = df[feature].fillna(value=\"NaN\")\n",
    "\n",
    "    print(f\"\\t*** {len(missing_codes)} codes have been added in the database...\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** 732 codes have been added in the database...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\"evenement_type\", \"cj\",  \"activ_nat_et\", \"liasse_type\", \"activ_surf_et\", \"activ_perm_et\"]\n",
    "text_feature = \"libelle\"\n",
    "y = \"apet_finale\"\n",
    "textual_features = None\n",
    "\n",
    "df = add_libelles(df, naf2008, y, text_feature, textual_features, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make available our processing function clean_text_feature for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"libelle_processed\"] = clean_text_feature(df[\"libelle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the columns in the right format:\n",
    " - First column contains the processed text (str)\n",
    " - Next ones contain the \"tokenized\" categorical (discrete) variables in int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['activit conseil gestion organis entrepris' 5 25 ... 10 1 0]\n",
      " ['servic concierger' 2 25 ... 10 1 0]\n",
      " ['mission back offic administratif, juridique, comptable, sais' 2 25 ...\n",
      "  6 1 0]\n",
      " ...\n",
      " ['[libelle] activit indifferencie menag tant producteur bien usag propr'\n",
      "  20 25 ... 7 0 0]\n",
      " ['[libelle] activit indifferencie menag tant producteur servic usag propr'\n",
      "  20 25 ... 7 0 0]\n",
      " ['[libelle] activit organis organ extraterritorial' 20 25 ... 7 0 0]]\n",
      "[587 727 633 ... 729 730 731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110660/1529226646.py:55: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"nan\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle_processed\")\n",
    "X = df[[\"libelle_processed\", \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
    "y = df[\"apet_finale\"].values\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test sets. We especially take care that:  \n",
    "- classes with only one instance appear in the train set (instead of the test set)\n",
    "- all classes are represented in the train set\n",
    "\n",
    "The `stratified_split_rare_labels` function from the `preprocess` subpackage is used to carefully split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = stratified_split_rare_labels(X, y)\n",
    "assert set(range(len(naf2008[\"code\"]))) == set(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the torch-fastText model (without training it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model (without building it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model building\n",
    "NUM_TOKENS= int(1e5) # Number of rows in the embedding matrix\n",
    "EMBED_DIM = 50 # Dimension of the embedding = number of columns in the embedding matrix\n",
    "SPARSE = False # Whether to use sparse Embedding layer for fast computation (see PyTorch documentation)\n",
    "CAT_EMBED_DIM = 10 # Dimension of the embedding for categorical features\n",
    "\n",
    "# Parameters for tokenizer\n",
    "MIN_COUNT = 1 # Minimum number of occurrences of a word in the corpus to be included in the vocabulary\n",
    "MIN_N = 3 # Minimum length of char n-grams\n",
    "MAX_N = 6 # Maximum length of char n-grams\n",
    "LEN_WORD_NGRAMS = 3 # Length of word n-grams\n",
    "\n",
    "# Parameters for training - not useful immediately\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:25:00 - torchFastText.torchFastText - categorical_embedding_dims provided but not categorical_vocabulary_sizes. It will be inferred later\n",
      "2025-01-27 15:25:00 - torchFastText.torchFastText - categorical_embedding_dims provided as int but not num_categorical_features. It will be inferred later\n"
     ]
    }
   ],
   "source": [
    "model = torchFastText(\n",
    "    num_tokens=NUM_TOKENS,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    categorical_embedding_dims=CAT_EMBED_DIM,\n",
    "    min_count=MIN_COUNT,\n",
    "    min_n=MIN_N,\n",
    "    max_n=MAX_N,\n",
    "    len_word_ngrams=LEN_WORD_NGRAMS,\n",
    "    sparse = SPARSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save these parameters to a JSON file. Initialization can also be done providing a JSON file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_json('torchFastText_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:25:05 - torchFastText.torchFastText - categorical_embedding_dims provided but not categorical_vocabulary_sizes. It will be inferred later\n",
      "2025-01-27 15:25:05 - torchFastText.torchFastText - categorical_embedding_dims provided as int but not num_categorical_features. It will be inferred later\n"
     ]
    }
   ],
   "source": [
    "model = torchFastText.from_json('torchFastText_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model using the training data. We have now access to the tokenizer, the PyTorch model as well as a PyTorch Lightning module ready to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:47:12 - torchFastText.torchFastText - num_categorical_features: old value is 2. New value is 6.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Categorical vocabulary sizes and their embedding dimensions must have the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4e-3\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlightning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/codif-ape-train/torch-fastText/notebooks/../torchFastText/torchFastText.py:334\u001b[0m, in \u001b[0;36mtorchFastText.build\u001b[0;34m(self, X_train, y_train, lightning, optimizer, optimizer_params, lr, scheduler, scheduler_params, patience_scheduler, loss)\u001b[0m\n\u001b[1;32m    329\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_categorical_features was provided at initialization but no categorical variables are provided in X_train. Updating to None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_categorical_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_categorical_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tokenizer(training_text)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_pytorch_model()\n",
      "File \u001b[0;32m~/work/codif-ape-train/torch-fastText/notebooks/../torchFastText/torchFastText.py:76\u001b[0m, in \u001b[0;36mtorchFastText._validate_categorical_inputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_embedding_dims, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_vocabulary_sizes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_embedding_dims):\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical vocabulary sizes and their embedding dimensions must have the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_categorical_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_vocabulary_sizes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_categorical_features:\n",
      "\u001b[0;31mValueError\u001b[0m: Categorical vocabulary sizes and their embedding dimensions must have the same length"
     ]
    }
   ],
   "source": [
    "LR = 4e-3\n",
    "model.build(X_train, y_train, lightning=True, lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextModel(\n",
      "  (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "  (emb_0): Embedding(21, 10)\n",
      "  (emb_1): Embedding(26, 10)\n",
      "  (emb_2): Embedding(8, 10)\n",
      "  (emb_3): Embedding(12, 10)\n",
      "  (emb_4): Embedding(3, 10)\n",
      "  (emb_5): Embedding(4, 10)\n",
      "  (fc): Linear(in_features=50, out_features=732, bias=True)\n",
      ")\n",
      "<NGramTokenizer(min_n=3, max_n=6, num_buckets=100000, word_ngrams=3, nwords=3991)>\n",
      "FastTextModule(\n",
      "  (model): FastTextModel(\n",
      "    (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "    (emb_0): Embedding(21, 10)\n",
      "    (emb_1): Embedding(26, 10)\n",
      "    (emb_2): Embedding(8, 10)\n",
      "    (emb_3): Embedding(12, 10)\n",
      "    (emb_4): Embedding(3, 10)\n",
      "    (emb_5): Embedding(4, 10)\n",
      "    (fc): Linear(in_features=50, out_features=732, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (accuracy_fn): MulticlassAccuracy()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)\n",
    "print(model.lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is useful to initialize the full torchFastText model without training it, if needed for some reason. But if it is not necessary, and we could have directly launched the training (building is then handled automatically if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{102213: '<lo', 72593: 'lor', 56522: 'ore', 68158: 'rem', 63426: 'em>', 17355: '<lor', 19844: 'lore', 63985: 'orem', 78418: 'rem>', 36410: '<lore', 46327: 'lorem', 44931: 'orem>', 33409: '<lorem', 11121: 'lorem>', 88809: '<ip', 18388: 'ips', 44778: 'psu', 95551: 'sum', 49682: 'um>', 40394: '<ips', 41871: 'ipsu', 64529: 'psum', 58809: 'sum>', 93065: '<ipsu', 74022: 'ipsum', 88771: 'psum>', 33380: '<ipsum', 69818: 'ipsum>', 98333: '<do', 15903: 'dol', 99090: 'olo', 29381: 'or>', 4949: '<dol', 22780: 'dolo', 63778: 'olor', 56515: 'lor>', 73594: '<dolo', 98636: 'dolor', 53990: 'olor>', 45786: '<dolor', 39464: 'dolor>', 23738: '<si', 38838: 'sit', 53345: 'it>', 4172: '<sit', 41738: 'sit>', 65320: '<sit>', 14928: '<am', 13369: 'ame', 61472: 'met', 41021: 'et>', 83627: '<ame', 27065: 'amet', 27980: 'met>', 61559: '<amet', 16651: 'amet>', 5297: '<amet>', 0: '</s>', 66049: 'lorem ipsum', 65182: 'ipsum dolor', 75764: 'dolor sit', 88112: 'sit amet', 74161: 'amet </s>', 38837: 'lorem ipsum dolor', 47788: 'ipsum dolor sit', 67172: 'dolor sit amet', 87183: 'sit amet </s>'}]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"lorem ipsum dolor sit amet\"]\n",
    "print(model.tokenizer.tokenize(sentence)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving parameters to JSON can also be done after building, but the model needs to be rebuilt after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:25:13 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau).\n"
     ]
    }
   ],
   "source": [
    "model.to_json('torchFastText_config.json')\n",
    "model = torchFastText.from_json('torchFastText_config.json')\n",
    "model.build(X_train, y_train, lightning=True, lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way to build torchFastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is only useful to initialize the tokenizer, but X_train and y_train are not needed to initialize the PyTorch model, provided we give the right parameters to construct layer. \n",
    "\n",
    "To highlight this, we provide a lower-level process to build the model where one can first build the tokenizer, and then build the model with custom architecture parameters. \n",
    "\n",
    "The tokenizer can be loaded **from the same JSON file** as the model parameters, or initialized using the right arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 16:49:17 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau).\n"
     ]
    }
   ],
   "source": [
    "training_text = X_train[:, 0].tolist()\n",
    "categorical_variables = X_train[:, 1:]\n",
    "\n",
    "# Before: this was inferred during the build method ; now required\n",
    "CAT_VOCAB_SIZE = (np.max(categorical_variables, axis=0) + 1).astype(int).tolist()\n",
    "\n",
    "# Tokenizer needs training text to build the vocabulary\n",
    "tokenizer = NGramTokenizer.from_json('torchFastText_config.json', training_text) # alternative 1 - see that it is the same JSON file as before\n",
    "tokenizer = NGramTokenizer(min_n=MIN_N, max_n=MAX_N, num_tokens= NUM_TOKENS,len_word_ngrams=LEN_WORD_NGRAMS, min_count=MIN_COUNT, training_text=training_text) # alternative 2\n",
    "\n",
    "# This model constructor is now independent from training data\n",
    "model = torchFastText.build_from_tokenizer(tokenizer, embedding_dim=EMBED_DIM, categorical_embedding_dims=CAT_EMBED_DIM, sparse=SPARSE, lr = LR, num_classes=NUM_CLASSES, num_categorical_features=NUM_CAT_VAR, categorical_vocabulary_sizes=CAT_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the PyTorch model and the Lightning module are now directly built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextModel(\n",
      "  (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "  (emb_0): Embedding(21, 10)\n",
      "  (emb_1): Embedding(26, 10)\n",
      "  (emb_2): Embedding(8, 10)\n",
      "  (emb_3): Embedding(12, 10)\n",
      "  (emb_4): Embedding(3, 10)\n",
      "  (emb_5): Embedding(4, 10)\n",
      "  (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      ")\n",
      "<NGramTokenizer(min_n=3, max_n=6, num_buckets=100000, word_ngrams=3, nwords=3991)>\n",
      "FastTextModule(\n",
      "  (model): FastTextModel(\n",
      "    (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "    (emb_0): Embedding(21, 10)\n",
      "    (emb_1): Embedding(26, 10)\n",
      "    (emb_2): Embedding(8, 10)\n",
      "    (emb_3): Embedding(12, 10)\n",
      "    (emb_4): Embedding(3, 10)\n",
      "    (emb_5): Embedding(4, 10)\n",
      "    (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (accuracy_fn): MulticlassAccuracy()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)\n",
    "print(model.lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the PyTorch model building did not use the training data, please keep in mind that its architecture (that you customize here) should match the vocabulary size of the categorical variables and the total number of class, otherwise the model will raise an error during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a torchFastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Checking inputs...\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Inputs successfully checked. Starting the training process..\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Running on: cpu\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Lightning module successfully created.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Launching training...\n",
      "\n",
      "  | Name        | Type               | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model       | FastTextModel      | 5.2 M  | train\n",
      "1 | loss        | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy_fn | MulticlassAccuracy | 0      | train\n",
      "-----------------------------------------------------------\n",
      "5.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 M     Total params\n",
      "20.980    Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a93857109e4890a9b777abcfb53944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010f4d4041c44d4b89a0fff3bb0b15f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93fb0700b06433b95bf1e63a9f21238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "2025-01-27 16:49:26 - torchFastText.torchFastText - Training done in 3.69 seconds.\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience_scheduler=PATIENCE,\n",
    "    patience_train=PATIENCE,\n",
    "    lr=LR,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained model from a Lightning checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model.best_model_path) # or any other checkpoint path (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"coiffeur, boulangerie, pâtisserie\"]\n",
    "X= np.array([[text[0], 0, 0, 0, 0, 0, 0]]) # our new entry\n",
    "TOP_K = 5\n",
    "\n",
    "pred, conf = model.predict(X, top_k=TOP_K)\n",
    "pred_naf = encoder.inverse_transform(pred.reshape(-1))\n",
    "subset = naf2008.set_index(\"code\").loc[np.flip(pred_naf)]\n",
    "\n",
    "for i in range(TOP_K-1, -1, -1):\n",
    "    print(f\"Prediction: {pred_naf[i]}, confidence:  {conf[0, i]}, description: {subset['libelle'][pred_naf[i]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchFastText.explainability.visualisation import (\n",
    "    visualize_letter_scores,\n",
    "    visualize_word_scores,\n",
    ")\n",
    "\n",
    "pred, conf, all_scores, all_scores_letters = model.predict_and_explain(X)\n",
    "visualize_word_scores(all_scores, text, pred_naf.reshape(1, -1))\n",
    "visualize_letter_scores(all_scores_letters, text, pred_naf.reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

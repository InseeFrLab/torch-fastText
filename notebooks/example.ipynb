{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (2.6.0+cpu)\n",
      "Requirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (2.5.0.post0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (1.6.1)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (19.0.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (3.9.1)\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (1.3.8)\n",
      "Requirement already satisfied: captum in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 10)) (8.1.5)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: ruff>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 12)) (0.9.7)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 13)) (4.1.0)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.12/site-packages (from -r ../requirements.txt (line 14)) (8.3.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch->-r ../requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch->-r ../requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch->-r ../requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch->-r ../requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch->-r ../requirements.txt (line 1)) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch->-r ../requirements.txt (line 1)) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch->-r ../requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.12/site-packages (from pytorch_lightning->-r ../requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.12/site-packages (from pytorch_lightning->-r ../requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from pytorch_lightning->-r ../requirements.txt (line 2)) (1.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from pytorch_lightning->-r ../requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.12/site-packages (from pytorch_lightning->-r ../requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->-r ../requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->-r ../requirements.txt (line 4)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->-r ../requirements.txt (line 4)) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->-r ../requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk->-r ../requirements.txt (line 7)) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk->-r ../requirements.txt (line 7)) (2024.11.6)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (from captum->-r ../requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r ../requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r ../requirements.txt (line 10)) (8.32.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r ../requirements.txt (line 10)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r ../requirements.txt (line 10)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r ../requirements.txt (line 10)) (3.0.13)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from pre-commit->-r ../requirements.txt (line 13)) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from pre-commit->-r ../requirements.txt (line 13)) (2.6.8)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.12/site-packages (from pre-commit->-r ../requirements.txt (line 13)) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.12/site-packages (from pre-commit->-r ../requirements.txt (line 13)) (20.29.2)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.12/site-packages (from pytest->-r ../requirements.txt (line 14)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.12/site-packages (from pytest->-r ../requirements.txt (line 14)) (1.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (3.11.12)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (0.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->captum->-r ../requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib->captum->-r ../requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib->captum->-r ../requirements.txt (line 9)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->captum->-r ../requirements.txt (line 9)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib->captum->-r ../requirements.txt (line 9)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->captum->-r ../requirements.txt (line 9)) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/conda/lib/python3.12/site-packages (from virtualenv>=20.10.0->pre-commit->-r ../requirements.txt (line 13)) (0.3.9)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/lib/python3.12/site-packages (from virtualenv>=20.10.0->pre-commit->-r ../requirements.txt (line 13)) (4.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch->-r ../requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (1.18.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 10)) (0.2.3)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r ../requirements.txt (line 2)) (3.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary (torchFastText in active development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../\")\n",
    "from torchFastText import torchFastText\n",
    "from torchFastText.preprocess import clean_text_feature\n",
    "from torchFastText.datasets import NGramTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchFastText\n",
      "  Downloading torchfasttext-0.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /opt/conda/lib/python3.12/site-packages (from torchFastText) (2.1.2)\n",
      "Requirement already satisfied: pytorch-lightning>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from torchFastText) (2.5.0.post0)\n",
      "Requirement already satisfied: torch>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from pytorch-lightning>=2.4.0->torchFastText) (2.6.0+cpu)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.12/site-packages (from pytorch-lightning>=2.4.0->torchFastText) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.12/site-packages (from pytorch-lightning>=2.4.0->torchFastText) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (2025.2.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from pytorch-lightning>=2.4.0->torchFastText) (1.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from pytorch-lightning>=2.4.0->torchFastText) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.12/site-packages (from pytorch-lightning>=2.4.0->torchFastText) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.12/site-packages (from pytorch-lightning>=2.4.0->torchFastText) (0.12.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (3.11.12)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=2.4.0->torchFastText) (75.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning>=2.4.0->torchFastText) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning>=2.4.0->torchFastText) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning>=2.4.0->torchFastText) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.1.0->pytorch-lightning>=2.4.0->torchFastText) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning>=2.4.0->torchFastText) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning>=2.4.0->torchFastText) (2.1.5)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.4.0->torchFastText) (3.10)\n",
      "Downloading torchfasttext-0.0.2-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: torchFastText\n",
      "Successfully installed torchFastText-0.0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchFastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions that will help us format our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"notebooks/\")\n",
    "from utils import categorize_surface, clean_and_tokenize_df, stratified_split_rare_labels, add_libelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 16:09:05 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-25 16:09:05 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-25 16:09:07 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-25 16:09:09 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n",
      "2025-02-25 16:09:09 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    }
   ],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"},\n",
    "    anon=True,\n",
    ")\n",
    "df = (\n",
    "    pq.ParquetDataset(\n",
    "        \"projet-ape/extractions/20241027_sirene4.parquet\",\n",
    "        filesystem=fs,\n",
    "    )\n",
    "    .read_pandas()\n",
    "    .to_pandas()\n",
    ").sample(frac=0.001).fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 16:09:22 - botocore.httpchecksum - Skipping checksum validation. Response did not contain one of the following algorithms: ['crc32', 'sha1', 'sha256'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>libelle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0111Z</td>\n",
       "      <td>Culture de céréales (à l'exception du riz), de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0112Z</td>\n",
       "      <td>Culture du riz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0113Z</td>\n",
       "      <td>Culture de légumes, de melons, de racines et d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0114Z</td>\n",
       "      <td>Culture de la canne à sucre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0115Z</td>\n",
       "      <td>Culture du tabac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>9609Z</td>\n",
       "      <td>Autres services personnels n.c.a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>9700Z</td>\n",
       "      <td>Activités des ménages en tant qu'employeurs de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>9810Z</td>\n",
       "      <td>Activités indifférenciées des ménages en tant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>9820Z</td>\n",
       "      <td>Activités indifférenciées des ménages en tant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>9900Z</td>\n",
       "      <td>Activités des organisations et organismes extr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      code                                            libelle\n",
       "0    0111Z  Culture de céréales (à l'exception du riz), de...\n",
       "1    0112Z                                     Culture du riz\n",
       "2    0113Z  Culture de légumes, de melons, de racines et d...\n",
       "3    0114Z                        Culture de la canne à sucre\n",
       "4    0115Z                                   Culture du tabac\n",
       "..     ...                                                ...\n",
       "727  9609Z                  Autres services personnels n.c.a.\n",
       "728  9700Z  Activités des ménages en tant qu'employeurs de...\n",
       "729  9810Z  Activités indifférenciées des ménages en tant ...\n",
       "730  9820Z  Activités indifférenciées des ménages en tant ...\n",
       "731  9900Z  Activités des organisations et organismes extr...\n",
       "\n",
       "[732 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with fs.open(\"projet-ape/data/naf2008.csv\") as file:\n",
    "    naf2008 = pd.read_csv(file, sep=\";\")\n",
    "naf2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t*** 732 codes have been added in the database...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_features = [\"evenement_type\", \"cj\",  \"activ_nat_et\", \"liasse_type\", \"activ_surf_et\", \"activ_perm_et\"]\n",
    "text_feature = \"libelle\"\n",
    "y = \"apet_finale\"\n",
    "textual_features = None\n",
    "\n",
    "df = add_libelles(df, naf2008, y, text_feature, textual_features, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make available our processing function clean_text_feature for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"libelle_processed\"] = clean_text_feature(df[\"libelle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the columns in the right format:\n",
    " - First column contains the processed text (str)\n",
    " - Next ones contain the \"tokenized\" categorical (discrete) variables in int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['loueur meubl non professionnel' 2 28 ... 5 1 0]\n",
      " ['realis artist' 2 28 ... 6 1 0]\n",
      " ['locat echafaudag montag demontag' 2 28 ... 7 1 0]\n",
      " ...\n",
      " ['[libelle] activit indifferencie menag tant producteur bien usag propr'\n",
      "  22 28 ... 8 0 0]\n",
      " ['[libelle] activit indifferencie menag tant producteur servic usag propr'\n",
      "  22 28 ... 8 0 0]\n",
      " ['[libelle] activit organis organ extraterritorial' 22 28 ... 8 0 0]]\n",
      "[578 693 612 ... 729 730 731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/torch-fastText/notebooks/utils.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  \"cj\": \"CJ\",\n"
     ]
    }
   ],
   "source": [
    "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle_processed\")\n",
    "X = df[[\"libelle_processed\", \"EVT\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
    "y = df[\"apet_finale\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for the 3 first obs\n",
      "\n",
      "[['loueur meubl non professionnel' 2 28 7 5 1 0]\n",
      " ['realis artist' 2 28 7 6 1 0]\n",
      " ['locat echafaudag montag demontag' 2 28 3 7 1 0]]\n",
      "\n",
      "\n",
      "NAF codes (labels) for the 3 first obs\n",
      "\n",
      "[578 693 612]\n"
     ]
    }
   ],
   "source": [
    "print(\"Features for the 3 first obs\\n\")\n",
    "print(X[:3])\n",
    "print(\"\\n\")\n",
    "print(\"NAF codes (labels) for the 3 first obs\\n\")\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test sets. We especially take care that:  \n",
    "- classes with only one instance appear in the train set (instead of the test set)\n",
    "- all classes are represented in the train set\n",
    "\n",
    "The `stratified_split_rare_labels` function from the `preprocess` subpackage is used to carefully split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = stratified_split_rare_labels(X, y)\n",
    "assert set(range(len(naf2008[\"code\"]))) == set(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the torch-fastText model (without training it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model (without building it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model building\n",
    "NUM_TOKENS= int(1e5) # Number of rows in the embedding matrix\n",
    "EMBED_DIM = 50 # Dimension of the embedding = number of columns in the embedding matrix\n",
    "SPARSE = False # Whether to use sparse Embedding layer for fast computation (see PyTorch documentation)\n",
    "CAT_EMBED_DIM = 10 # Dimension of the embedding for categorical features\n",
    "\n",
    "# Parameters for tokenizer\n",
    "MIN_COUNT = 1 # Minimum number of occurrences of a word in the corpus to be included in the vocabulary\n",
    "MIN_N = 3 # Minimum length of char n-grams\n",
    "MAX_N = 6 # Maximum length of char n-grams\n",
    "LEN_WORD_NGRAMS = 3 # Length of word n-grams\n",
    "\n",
    "# Parameters for training - not useful immediately\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:25:00 - torchFastText.torchFastText - categorical_embedding_dims provided but not categorical_vocabulary_sizes. It will be inferred later\n",
      "2025-01-27 15:25:00 - torchFastText.torchFastText - categorical_embedding_dims provided as int but not num_categorical_features. It will be inferred later\n"
     ]
    }
   ],
   "source": [
    "model = torchFastText(\n",
    "    num_tokens=NUM_TOKENS,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    categorical_embedding_dims=CAT_EMBED_DIM,\n",
    "    min_count=MIN_COUNT,\n",
    "    min_n=MIN_N,\n",
    "    max_n=MAX_N,\n",
    "    len_word_ngrams=LEN_WORD_NGRAMS,\n",
    "    sparse = SPARSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save these parameters to a JSON file. Initialization can also be done providing a JSON file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_json('torchFastText_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:25:05 - torchFastText.torchFastText - categorical_embedding_dims provided but not categorical_vocabulary_sizes. It will be inferred later\n",
      "2025-01-27 15:25:05 - torchFastText.torchFastText - categorical_embedding_dims provided as int but not num_categorical_features. It will be inferred later\n"
     ]
    }
   ],
   "source": [
    "model = torchFastText.from_json('torchFastText_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model using the training data. We have now access to the tokenizer, the PyTorch model as well as a PyTorch Lightning module ready to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:47:12 - torchFastText.torchFastText - num_categorical_features: old value is 2. New value is 6.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Categorical vocabulary sizes and their embedding dimensions must have the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4e-3\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlightning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/codif-ape-train/torch-fastText/notebooks/../torchFastText/torchFastText.py:334\u001b[0m, in \u001b[0;36mtorchFastText.build\u001b[0;34m(self, X_train, y_train, lightning, optimizer, optimizer_params, lr, scheduler, scheduler_params, patience_scheduler, loss)\u001b[0m\n\u001b[1;32m    329\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_categorical_features was provided at initialization but no categorical variables are provided in X_train. Updating to None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_categorical_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_categorical_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_tokenizer(training_text)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_pytorch_model()\n",
      "File \u001b[0;32m~/work/codif-ape-train/torch-fastText/notebooks/../torchFastText/torchFastText.py:76\u001b[0m, in \u001b[0;36mtorchFastText._validate_categorical_inputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_embedding_dims, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_vocabulary_sizes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_embedding_dims):\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical vocabulary sizes and their embedding dimensions must have the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_categorical_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_vocabulary_sizes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_categorical_features:\n",
      "\u001b[0;31mValueError\u001b[0m: Categorical vocabulary sizes and their embedding dimensions must have the same length"
     ]
    }
   ],
   "source": [
    "LR = 4e-3\n",
    "model.build(X_train, y_train, lightning=True, lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextModel(\n",
      "  (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "  (emb_0): Embedding(21, 10)\n",
      "  (emb_1): Embedding(26, 10)\n",
      "  (emb_2): Embedding(8, 10)\n",
      "  (emb_3): Embedding(12, 10)\n",
      "  (emb_4): Embedding(3, 10)\n",
      "  (emb_5): Embedding(4, 10)\n",
      "  (fc): Linear(in_features=50, out_features=732, bias=True)\n",
      ")\n",
      "<NGramTokenizer(min_n=3, max_n=6, num_buckets=100000, word_ngrams=3, nwords=3991)>\n",
      "FastTextModule(\n",
      "  (model): FastTextModel(\n",
      "    (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "    (emb_0): Embedding(21, 10)\n",
      "    (emb_1): Embedding(26, 10)\n",
      "    (emb_2): Embedding(8, 10)\n",
      "    (emb_3): Embedding(12, 10)\n",
      "    (emb_4): Embedding(3, 10)\n",
      "    (emb_5): Embedding(4, 10)\n",
      "    (fc): Linear(in_features=50, out_features=732, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (accuracy_fn): MulticlassAccuracy()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)\n",
    "print(model.lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is useful to initialize the full torchFastText model without training it, if needed for some reason. But if it is not necessary, and we could have directly launched the training (building is then handled automatically if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{102213: '<lo', 72593: 'lor', 56522: 'ore', 68158: 'rem', 63426: 'em>', 17355: '<lor', 19844: 'lore', 63985: 'orem', 78418: 'rem>', 36410: '<lore', 46327: 'lorem', 44931: 'orem>', 33409: '<lorem', 11121: 'lorem>', 88809: '<ip', 18388: 'ips', 44778: 'psu', 95551: 'sum', 49682: 'um>', 40394: '<ips', 41871: 'ipsu', 64529: 'psum', 58809: 'sum>', 93065: '<ipsu', 74022: 'ipsum', 88771: 'psum>', 33380: '<ipsum', 69818: 'ipsum>', 98333: '<do', 15903: 'dol', 99090: 'olo', 29381: 'or>', 4949: '<dol', 22780: 'dolo', 63778: 'olor', 56515: 'lor>', 73594: '<dolo', 98636: 'dolor', 53990: 'olor>', 45786: '<dolor', 39464: 'dolor>', 23738: '<si', 38838: 'sit', 53345: 'it>', 4172: '<sit', 41738: 'sit>', 65320: '<sit>', 14928: '<am', 13369: 'ame', 61472: 'met', 41021: 'et>', 83627: '<ame', 27065: 'amet', 27980: 'met>', 61559: '<amet', 16651: 'amet>', 5297: '<amet>', 0: '</s>', 66049: 'lorem ipsum', 65182: 'ipsum dolor', 75764: 'dolor sit', 88112: 'sit amet', 74161: 'amet </s>', 38837: 'lorem ipsum dolor', 47788: 'ipsum dolor sit', 67172: 'dolor sit amet', 87183: 'sit amet </s>'}]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"lorem ipsum dolor sit amet\"]\n",
    "print(model.tokenizer.tokenize(sentence)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving parameters to JSON can also be done after building, but the model needs to be rebuilt after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 15:25:13 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau).\n"
     ]
    }
   ],
   "source": [
    "model.to_json('torchFastText_config.json')\n",
    "model = torchFastText.from_json('torchFastText_config.json')\n",
    "model.build(X_train, y_train, lightning=True, lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way to build torchFastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is only useful to initialize the tokenizer, but X_train and y_train are not needed to initialize the PyTorch model, provided we give the right parameters to construct layer. \n",
    "\n",
    "To highlight this, we provide a lower-level process to build the model where one can first build the tokenizer, and then build the model with custom architecture parameters. \n",
    "\n",
    "The tokenizer can be loaded **from the same JSON file** as the model parameters, or initialized using the right arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 16:49:17 - torchFastText.torchFastText - No scheduler parameters provided. Using default parameters (suited for ReduceLROnPlateau).\n"
     ]
    }
   ],
   "source": [
    "training_text = X_train[:, 0].tolist()\n",
    "categorical_variables = X_train[:, 1:]\n",
    "\n",
    "# Before: this was inferred during the build method ; now required\n",
    "CAT_VOCAB_SIZE = (np.max(categorical_variables, axis=0) + 1).astype(int).tolist()\n",
    "\n",
    "# Tokenizer needs training text to build the vocabulary\n",
    "tokenizer = NGramTokenizer.from_json('torchFastText_config.json', training_text) # alternative 1 - see that it is the same JSON file as before\n",
    "tokenizer = NGramTokenizer(min_n=MIN_N, max_n=MAX_N, num_tokens= NUM_TOKENS,len_word_ngrams=LEN_WORD_NGRAMS, min_count=MIN_COUNT, training_text=training_text) # alternative 2\n",
    "\n",
    "# This model constructor is now independent from training data\n",
    "model = torchFastText.build_from_tokenizer(tokenizer, embedding_dim=EMBED_DIM, categorical_embedding_dims=CAT_EMBED_DIM, sparse=SPARSE, lr = LR, num_classes=NUM_CLASSES, num_categorical_features=NUM_CAT_VAR, categorical_vocabulary_sizes=CAT_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the PyTorch model and the Lightning module are now directly built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTextModel(\n",
      "  (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "  (emb_0): Embedding(21, 10)\n",
      "  (emb_1): Embedding(26, 10)\n",
      "  (emb_2): Embedding(8, 10)\n",
      "  (emb_3): Embedding(12, 10)\n",
      "  (emb_4): Embedding(3, 10)\n",
      "  (emb_5): Embedding(4, 10)\n",
      "  (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      ")\n",
      "<NGramTokenizer(min_n=3, max_n=6, num_buckets=100000, word_ngrams=3, nwords=3991)>\n",
      "FastTextModule(\n",
      "  (model): FastTextModel(\n",
      "    (embeddings): EmbeddingBag(103992, 50, mode='mean')\n",
      "    (emb_0): Embedding(21, 10)\n",
      "    (emb_1): Embedding(26, 10)\n",
      "    (emb_2): Embedding(8, 10)\n",
      "    (emb_3): Embedding(12, 10)\n",
      "    (emb_4): Embedding(3, 10)\n",
      "    (emb_5): Embedding(4, 10)\n",
      "    (fc): Linear(in_features=60, out_features=732, bias=True)\n",
      "  )\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (accuracy_fn): MulticlassAccuracy()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.pytorch_model)\n",
    "print(model.tokenizer)\n",
    "print(model.lightning_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the PyTorch model building did not use the training data, please keep in mind that its architecture (that you customize here) should match the vocabulary size of the categorical variables and the total number of class, otherwise the model will raise an error during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a torchFastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Checking inputs...\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Inputs successfully checked. Starting the training process..\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Running on: cpu\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Lightning module successfully created.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "2025-01-27 16:49:23 - torchFastText.torchFastText - Launching training...\n",
      "\n",
      "  | Name        | Type               | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model       | FastTextModel      | 5.2 M  | train\n",
      "1 | loss        | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy_fn | MulticlassAccuracy | 0      | train\n",
      "-----------------------------------------------------------\n",
      "5.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 M     Total params\n",
      "20.980    Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a93857109e4890a9b777abcfb53944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010f4d4041c44d4b89a0fff3bb0b15f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93fb0700b06433b95bf1e63a9f21238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "2025-01-27 16:49:26 - torchFastText.torchFastText - Training done in 3.69 seconds.\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience_scheduler=PATIENCE,\n",
    "    patience_train=PATIENCE,\n",
    "    lr=LR,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "## The library uses lightning library to train the model. It is possible to add some specific parameters to the training method to use it :\n",
    "##\n",
    "## trainer_params = {'profiler': 'simple', 'enable_progress_bar': False}\n",
    "##\n",
    "## model.train(\n",
    "##    X_train,\n",
    "##    y_train,\n",
    "##    X_test,\n",
    "##    y_test,\n",
    "##    num_epochs=NUM_EPOCHS,\n",
    "##    batch_size=BATCH_SIZE,\n",
    "##    patience_scheduler=PATIENCE,\n",
    "##    patience_train=PATIENCE,\n",
    "##    lr=LR,\n",
    "##    verbose = True,\n",
    "##    trainer_params = trainer_params\n",
    "##)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained model from a Lightning checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from_checkpoint(model.best_model_path) # or any other checkpoint path (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"coiffeur, boulangerie, pâtisserie\"]\n",
    "X= np.array([[text[0], 0, 0, 0, 0, 0, 0]]) # our new entry\n",
    "TOP_K = 5\n",
    "\n",
    "pred, conf = model.predict(X, top_k=TOP_K)\n",
    "pred_naf = encoder.inverse_transform(pred.reshape(-1))\n",
    "subset = naf2008.set_index(\"code\").loc[np.flip(pred_naf)]\n",
    "\n",
    "for i in range(TOP_K-1, -1, -1):\n",
    "    print(f\"Prediction: {pred_naf[i]}, confidence:  {conf[0, i]}, description: {subset['libelle'][pred_naf[i]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchFastText.explainability.visualisation import (\n",
    "    visualize_letter_scores,\n",
    "    visualize_word_scores,\n",
    ")\n",
    "\n",
    "pred, conf, all_scores, all_scores_letters = model.predict_and_explain(X)\n",
    "visualize_word_scores(all_scores, text, pred_naf.reshape(1, -1))\n",
    "visualize_letter_scores(all_scores_letters, text, pred_naf.reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
